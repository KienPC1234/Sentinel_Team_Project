# ShieldCall VN Backend API - Ollama Integration Summary

## ğŸ¯ Overview

The ShieldCall VN Backend API has been fully integrated with **Ollama**, a local AI inference engine. This allows the API to use state-of-the-art language models for:

- AI-powered chat responses
- Scam detection and analysis
- Image text analysis
- Audio transcript analysis

## âœ¨ Key Features

### 1. **Automatic Fallback System**
- If Ollama is running: Uses real AI models
- If Ollama is down: Falls back to rule-based analysis automatically
- No disruption to service

### 2. **Integrated Endpoints**
All endpoints work seamlessly with Ollama:

| Endpoint | Status | Ollama Use |
|----------|--------|-----------|
| `/check-session` | âœ… | N/A |
| `/check-phone` | âœ… | N/A |
| `/chat-ai` | âœ… | AI responses |
| `/chat-ai-stream` | âœ… | Streaming responses |
| `/analyze-images` | âœ… | Text analysis |
| `/analyze-audio` | âœ… | Scam detection |
| `/report-crash` | âœ… | N/A |

### 3. **Vietnamese Language Support**
- Ollama models work natively with Vietnamese text
- Proper handling of diacritics and language nuances
- Optimized prompts in Vietnamese

---

## ğŸš€ Quick Start

### 1. Install Ollama

**Download from:** https://ollama.ai

Available for:
- macOS (Intel & Apple Silicon)
- Linux
- Windows (with WSL2)

### 2. Pull a Model

```bash
# Recommended: Neural Chat (7.4B, optimized)
ollama pull neural-chat

# Alternatives:
ollama pull mistral        # Fast and capable
ollama pull llama2         # General purpose
ollama pull openchat       # Fast lightweight
```

### 3. Start Ollama

```bash
# macOS/Linux
ollama serve

# Windows: Run Ollama app (starts automatically)

# Verify it's running:
curl http://localhost:11434/api/tags
```

### 4. Start API Server

```bash
cd /data/PKV_TEAM
source .venv/bin/activate
python manage.py runserver 0.0.0.0:8001
```

### 5. Test Integration

```bash
# Test Ollama
python test_ollama.py

# Test API
python test_api.py
```

---

## ğŸ“ Implementation Files

### New Files Created

1. **`api/utils/ollama_client.py`** (NEW)
   - Main Ollama integration module
   - Functions: `generate_response()`, `stream_response()`, etc.
   - Automatic availability detection
   - Error handling and logging

2. **`OLLAMA_SETUP.md`** (NEW)
   - Complete Ollama setup guide
   - Model selection recommendations
   - Troubleshooting guide
   - Performance tuning tips

3. **`test_ollama.py`** (NEW)
   - Comprehensive test suite for Ollama integration
   - Tests connection, models, response generation
   - Validates scam detection
   - Django integration verification

4. **`setup.sh`** (NEW)
   - Automated setup script
   - Checks all dependencies
   - Verifies Ollama status
   - Clear startup instructions

### Modified Files

1. **`api/ai_chat/views.py`**
   - Updated `get_llm_response()` to use Ollama
   - Integrated Ollama client
   - Context-aware prompts in Vietnamese
   - Automatic fallback handling

2. **`api/utils/media_utils.py`**
   - Enhanced `analyze_image_risk()` with Ollama
   - Enhanced `analyze_audio_risk()` with scam detection
   - Improved text analysis using language models
   - Fallback to keyword matching if needed

---

## ğŸ”§ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ShieldCall API (Django)           â”‚
â”‚  /check-session                     â”‚
â”‚  /check-phone                       â”‚
â”‚  /chat-ai          â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  /chat-ai-stream   â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  /analyze-images   â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  /analyze-audio    â”€â”€â”€â”€â”€â”€â”¤      â”‚   â”‚
â”‚  /report-crash            â”‚      â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”˜
                             â”‚      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”   â”‚
                    â”‚  Requests  â”‚   â”‚
                    â”‚  Library   â”‚   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â”‚
                             â”‚       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚   Ollama Client      â”‚
                    â”‚ (api/utils/           â”‚
                    â”‚  ollama_client.py)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Ollama Service        â”‚
                    â”‚ localhost:11434       â”‚
                    â”‚ (Fast AI Inference)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Local LLM Models     â”‚
                    â”‚  - neural-chat        â”‚
                    â”‚  - mistral            â”‚
                    â”‚  - llama2             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ How It Works

### Flow Diagram

```
User Request â†’ Django View
    â†“
Is Ollama Available?
    â”œâ”€â†’ YES â†’ Call Ollama API â†’ Get AI Response âœ“
    â”‚
    â””â”€â†’ NO â†’ Use Fallback Rules â†’ Get Response âœ“
    â†“
Return Response to Client
```

### Example: Chat Request

```python
# User sends: "Tin nháº¯n nÃ y lá»«a Ä‘áº£o khÃ´ng?"

# 1. Django receives request
# 2. Creates Vietnamese prompt
prompt = """Báº¡n lÃ  trá»£ lÃ½ an toÃ n Ä‘iá»‡n thoáº¡i ShieldCall VN. 
PhÃ¢n tÃ­ch tin nháº¯n nÃ y Ä‘á»ƒ phÃ¡t hiá»‡n lá»«a Ä‘áº£o.
Tin nháº¯n: "Tin nháº¯n nÃ y lá»«a Ä‘áº£o khÃ´ng?" """

# 3. Checks if Ollama available
if is_ollama_available():
    # 4a. Calls Ollama with prompt
    response = generate_response(prompt, model="neural-chat")
    # 5a. Returns AI analysis to user
else:
    # 4b. Uses keyword matching fallback
    response = perform_keyword_analysis(prompt)
    # 5b. Returns basic analysis to user
```

---

## ğŸ¯ Key Integration Points

### 1. Chat Endpoints (`api/ai_chat/`)

```python
# get_llm_response() function
def get_llm_response(user_message, session_id, context='general'):
    # Vietnamese prompts
    prompt = f"""Báº¡n lÃ  trá»£ lÃ½ an toÃ n Ä‘iá»‡n thoáº¡i ShieldCall VN...
{user_message}"""
    
    # Try Ollama first
    if is_ollama_available():
        response = generate_response(prompt)
        classification = classify_message(user_message)
        return {
            'ai_response': response,
            'action_suggested': classification['suggested_action']
        }
    
    # Fallback to mock response
    return mock_response
```

### 2. Image Analysis (`api/media_analysis/`)

```python
# analyze_image_risk() function
def analyze_image_risk(ocr_text, image_file=None):
    # Use Ollama for intelligent analysis
    analysis = analyze_text_for_scam(ocr_text)
    
    # Convert risk score to level
    risk_level = map_score_to_level(analysis['risk_score'])
    
    return {
        'is_safe': not analysis['is_scam'],
        'risk_level': risk_level,
        'details': analysis['reason']
    }
```

### 3. Audio Analysis (`api/media_analysis/`)

```python
# analyze_audio_risk() function  
def analyze_audio_risk(transcript, phone_number):
    # Analyze transcript with Ollama for scam patterns
    analysis = analyze_text_for_scam(transcript)
    
    return {
        'risk_score': analysis['risk_score'],
        'is_scam': analysis['is_scam'],
        'warning_message': analysis['reason'],
        'duration': 0
    }
```

---

## ğŸ“Š Performance Metrics

### Response Times

| Scenario | Time | Notes |
|----------|------|-------|
| **First request** | 10-30s | Model loading |
| **Chat response** | 2-5s | Typical |
| **Image analysis** | 1-3s | Depends on text length |
| **Audio analysis** | 2-4s | Transcript length |
| **Fallback** | <100ms | No Ollama |

### Model Selection Impact

```
Model          | Speed | Quality | Memory | Vietnamese
---------------|-------|---------|--------|------------
neural-chat    | âš¡âš¡âš¡  | â­â­â­  | 7.4GB  | âœ… Good
mistral        | âš¡âš¡âš¡  | â­â­â­  | 7GB    | âš ï¸ Basic
llama2         | âš¡âš¡   | â­â­â­â­ | 7GB+   | âœ… Good
openchat       | âš¡âš¡âš¡  | â­â­    | 5GB    | âš ï¸ Basic
dolphin-mixtral| âš¡    | â­â­â­â­ | 46GB   | âœ… Excellent
```

---

## ğŸ› ï¸ Configuration

### Default Settings (`api/utils/ollama_client.py`)

```python
OLLAMA_BASE_URL = "http://localhost:11434"  # Ollama service URL
DEFAULT_MODEL = "neural-chat"               # Default model
TIMEOUT = 60                                # Request timeout (sec)
```

### Change Default Model

Edit `api/utils/ollama_client.py`:

```python
# Before
DEFAULT_MODEL = "neural-chat"

# After
DEFAULT_MODEL = "mistral"  # or your preferred model
```

### Environment Variables (Optional)

Add to `.env`:

```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=neural-chat
OLLAMA_TIMEOUT=60
```

---

## ğŸ§ª Testing

### Test Ollama Connection

```bash
python test_ollama.py
```

Checks:
- âœ… Ollama service running
- âœ… Available models
- âœ… Response generation
- âœ… Scam detection
- âœ… Django integration

### Test Full API

```bash
python test_api.py
```

Works with or without Ollama:
- âœ… Session management
- âœ… Phone checking
- âœ… Chat AI
- âœ… Crash reporting

### Manual Testing

```bash
# Get available models
curl http://localhost:11434/api/tags

# Generate text
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "neural-chat",
    "prompt": "Xin chÃ o",
    "stream": false
  }'

# Test API with Ollama running
SESSION=$(curl -s "http://localhost:8001/check-session?session_id=00000000-0000-0000-0000-000000000000" | python3 -c "import sys, json; print(json.load(sys.stdin)['new_session_id'])")

curl -X POST http://localhost:8001/chat-ai \
  -H "Content-Type: application/json" \
  -d "{
    \"user_message\": \"Xin chÃ o\",
    \"session_id\": \"$SESSION\",
    \"context\": \"general\"
  }"
```

---

## ğŸš¨ Troubleshooting

### Ollama Not Responding

```bash
# Check if running
curl http://localhost:11434/api/tags

# Restart Ollama
killall ollama
ollama serve
```

### Models Not Found

```bash
# List installed models
ollama list

# Pull a model
ollama pull neural-chat
```

### Slow Responses

```bash
# Try a smaller model
ollama pull mistral

# Or increase timeout in ollama_client.py
TIMEOUT = 120  # 2 minutes
```

### Memory Issues

```bash
# Check system memory
free -h

# Use a lightweight model
ollama pull mistral

# Or reduce model size
ollama list | grep size
```

---

## ğŸ“š Documentation Files

| File | Purpose |
|------|---------|
| **OLLAMA_SETUP.md** | Complete Ollama setup guide |
| **IMPLEMENTATION.md** | API implementation details |
| **test_ollama.py** | Ollama integration test |
| **test_api.py** | Full API test suite |
| **API_EXAMPLES.sh** | cURL command examples |
| **setup.sh** | Automated setup script |

---

## ğŸ”„ Workflow Example

### Complete conversation flow with Ollama:

```
1. Client sends: "Tin nháº¯n nÃ y lá»«a Ä‘áº£o khÃ´ng?"
   â†“
2. API receives request at /chat-ai
   â†“
3. Django creates Vietnamese prompt
   â†“
4. Checks: is_ollama_available() â†’ YES
   â†“
5. Calls Ollama with prompt
   â†“
6. Ollama uses neural-chat model
   â†“
7. Returns: "ÄÃ¢y lÃ  tin nháº¯n lá»«a Ä‘áº£o..."
   â†“
8. API classifies: suggested_action = "BLOCK"
   â†“
9. Stores in database
   â†“
10. Returns JSON response to client
    {
      "ai_response": "ÄÃ¢y lÃ  tin nháº¯n lá»«a Ä‘áº£o...",
      "action_suggested": "BLOCK"
    }
```

---

## âœ… Verification Checklist

- [ ] Ollama installed from ollama.ai
- [ ] Model pulled: `ollama pull neural-chat`
- [ ] Ollama running: `curl http://localhost:11434/api/tags`
- [ ] API dependencies installed: `pip install -r requirements.txt`
- [ ] Database migrated: `python manage.py migrate`
- [ ] Ollama test passes: `python test_ollama.py`
- [ ] API test passes: `python test_api.py`
- [ ] Server running: `python manage.py runserver 0.0.0.0:8001`

---

## ğŸ“ Next Steps

1. **Review** `OLLAMA_SETUP.md` for detailed Ollama setup
2. **Run** `python test_ollama.py` to verify integration
3. **Start** `python manage.py runserver 0.0.0.0:8001`
4. **Test** API with `python test_api.py`
5. **Customize** model selection as needed

---

## ğŸ“ Support

For Ollama issues:
- Ollama Docs: https://github.com/ollama/ollama
- Models: https://ollama.ai/library

For API issues:
- See `IMPLEMENTATION.md`
- Check API logs: `python manage.py runserver --verbosity 2`

---

**Integration Status:** âœ… Complete  
**Last Updated:** February 12, 2026  
**Tested Models:** neural-chat, mistral, llama2  
**Fallback Support:** âœ… Yes (fully functional)
