"""
ShieldCall VN ‚Äì Celery Tasks
MVP spec Section 9: Async tasks
"""
import logging
import re
import json
from celery import shared_task
from django.utils import timezone
from datetime import timedelta
from django.db.models import Count, F

import logging
from celery import shared_task
from django.utils import timezone
from datetime import timedelta
from django.db.models import Count, F

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    fh = logging.FileHandler('llm_access.log')
    fh.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
    logger.addHandler(fh)

@shared_task(name='core.rebuild_scam_vector_index')
def rebuild_scam_vector_index():
    """
    Rebuilds the FAISS vector index for RAG from Articles and LearnLessons.
    """
    try:
        from api.utils.vector_db import vector_db
        vector_db.rebuild_index(force_cpu=True)
        logger.info("Successfully rebuilt scam vector index.")
    except Exception as e:
        logger.error(f"Error rebuilding vector index: {e}")


@shared_task(name='core.recompute_phone_risk')
def recompute_phone_risk(phone_number: str):
    """
    Recompute risk score using the Hybrid Scoring Engine.
    Updates PhoneNumber model with new risk level, score, and trust metrics.
    """
    try:
        from api.phone_security.models import PhoneNumber, PhoneRiskLevel
        from api.core.views.scan_views import _phone_risk_score
    except ImportError:
        logger.warning('Dependencies not found for recompute_phone_risk')
        return

    # Calculate Score using the unified engine
    result = _phone_risk_score(phone_number)
    
    # Map API level string directly to Enum
    level_map = {
        'SAFE': PhoneRiskLevel.SAFE,
        'GREEN': PhoneRiskLevel.GREEN,
        'YELLOW': PhoneRiskLevel.YELLOW,
        'RED': PhoneRiskLevel.RED
    }
    risk_enum = level_map.get(result['risk_level'], PhoneRiskLevel.SAFE)

    # Update DB
    PhoneNumber.objects.update_or_create(
        phone_number=phone_number,
        defaults={
            'risk_level': risk_enum,
            'reports_count': result['report_count'],
            'trust_score': result.get('trust_score', 0),
            'risk_label': f"Score: {result['risk_score']}",
            # If carrier or other enriched data is available in result, save it here
        }
    )
    logger.info(f'Recomputed risk for {phone_number}: score={result["risk_score"]}')


@shared_task(name='core.daily_trend_aggregation')
def daily_trend_aggregation(date_str: str = None):
    """
    Aggregate daily scam report counts by type.
    Run daily via celery beat.
    """
    from api.core.models import Report, TrendDaily

    if date_str:
        from datetime import date as dt
        target_date = dt.fromisoformat(date_str)
    else:
        target_date = (timezone.now() - timedelta(days=1)).date()

    # Aggregate reports by scam_type for the given date
    aggregated = (
        Report.objects.filter(created_at__date=target_date)
        .values('scam_type')
        .annotate(count=Count('id'))
    )

    for entry in aggregated:
        TrendDaily.objects.update_or_create(
            date=target_date,
            region='VN',
            scam_type=entry['scam_type'],
            defaults={'count': entry['count']},
        )

    logger.info(f'Trend aggregation for {target_date}: {len(aggregated)} types')


@shared_task(name='core.scan_domain_async')
def scan_domain_async(url: str, user_id: int = None):
    """
    Async domain scan for heavy analysis.
    """
    from api.core.views.scan_views import _analyze_domain
    from api.core.models import ScanEvent

    result = _analyze_domain(url)

    ScanEvent.objects.create(
        user_id=user_id,
        scan_type='domain',
        raw_input=url,
        normalized_input=result['domain'],
        result_json=result,
        risk_score=result['risk_score'],
        risk_level=result['risk_level'],
    )

    return result


@shared_task(name='core.deduplicate_reports')
def deduplicate_reports():
    """
    Find and flag duplicate reports.
    """
    from api.core.models import Report

    pending = Report.objects.filter(status='pending').order_by('created_at')
    seen = {}
    duplicates = 0

    for report in pending:
        key = f"{report.target_type}:{report.target_value}"
        if key in seen:
            # Mark as duplicate in moderation note
            report.moderation_note = f'Possible duplicate of Report #{seen[key]}'
            report.save(update_fields=['moderation_note'])
            duplicates += 1
        else:
            seen[key] = report.pk

    logger.info(f'Found {duplicates} potential duplicate reports')


def _notify_scan_complete(scan_event, title_suffix=''):
    """
    Send push notification (OneSignal + WebSocket) when a scan completes.
    Only sends if the scan was initiated by an authenticated user.
    """
    try:
        if not scan_event.user_id:
            return  # Anonymous scan, no push target

        from api.utils.push_service import push_service

        risk_level = getattr(scan_event, 'risk_level', 'SAFE')
        risk_score = getattr(scan_event, 'risk_score', 0)

        level_labels = {
            'RED': 'üî¥ Nguy hi·ªÉm',
            'YELLOW': 'üü° C·∫©n th·∫≠n',
            'GREEN': 'üü¢ An to√†n',
            'SAFE': 'üü¢ An to√†n',
        }
        level_text = level_labels.get(risk_level, 'üü¢ An to√†n')

        title = f"K·∫øt qu·∫£ qu√©t: {level_text}"
        message = f"{title_suffix} ‚Äî ƒêi·ªÉm r·ªßi ro: {risk_score}/100"
        url = f"/scan/status/{scan_event.id}/" if hasattr(scan_event, 'id') else None

        push_service.send_push(
            user_id=scan_event.user_id,
            title=title,
            message=message,
            url=url,
            notification_type='scan_result'
        )
        logger.info(f"[Push] Sent scan notification to user {scan_event.user_id} for event {scan_event.id}")
    except Exception as e:
        logger.warning(f"[Push] Failed to send scan notification: {e}")


@shared_task(name='core.perform_scan_task', bind=True)
def perform_scan_task(self, scan_event_id):
    """
    Unified background task to perform scanning logic (phone, message, domain, email).
    """
    from api.core.models import ScanEvent, ScanStatus, RiskLevel
    from api.utils.vt_client import VTClient
    from api.utils.ollama_client import analyze_text_for_scam
    from api.core.views.scan_views import _phone_risk_score, _analyze_message_text, _analyze_domain
    
    try:
        scan_event = ScanEvent.objects.get(id=scan_event_id)
        scan_event.status = ScanStatus.PROCESSING
        scan_event.job_id = self.request.id
        scan_event.save()

        scan_type = scan_event.scan_type
        raw_input = scan_event.raw_input
        result = {}

        if scan_type == 'phone':
            result = _phone_risk_score(raw_input)
        elif scan_type == 'message':
            result = _analyze_message_text(raw_input)
        elif scan_type == 'domain':
            result = _analyze_domain(raw_input)
        elif scan_type == 'email':
            # Basic email reputation check logic
            from api.utils.normalization import normalize_domain
            email = raw_input.lower().strip()
            ai_res = None
            try:
                ai_res = analyze_text_for_scam(email)
            except Exception as _ai_exc:
                logger.warning(f"perform_scan_task: AI analysis failed for email ({_ai_exc}), using fallback.")
            if not ai_res:
                ai_res = {'risk_score': 0, 'risk_level': 'SAFE', 'explanation': 'AI kh√¥ng kh·∫£ d·ª•ng.'}
            ai_notice = ai_res.get('ai_notice')
            details = [ai_res.get('explanation', 'Ph√¢n t√≠ch ho√†n t·∫•t.')]
            if ai_notice and ai_notice not in details:
                details.insert(0, ai_notice)
            result = {
                'email': email,
                'risk_score': ai_res.get('risk_score', 0),
                'risk_level': ai_res.get('risk_level', 'SAFE'),
                'details': details,
                'ai_available': ai_res.get('explanation') != 'AI kh√¥ng kh·∫£ d·ª•ng.',
                'ai_retry_used': ai_res.get('ai_retry_used', False),
                'ai_retry_count': ai_res.get('ai_retry_count', 0),
                'ai_notice': ai_notice,
            }

        scan_event.result_json = result
        scan_event.risk_score = result.get('risk_score', 0)
        scan_event.risk_level = result.get('risk_level', RiskLevel.SAFE)
        scan_event.status = ScanStatus.COMPLETED
        scan_event.save()

        type_labels = {'phone': 's·ªë ƒëi·ªán tho·∫°i', 'message': 'tin nh·∫Øn', 'domain': 'website', 'email': 'email'}
        _notify_scan_complete(scan_event, f"Qu√©t {type_labels.get(scan_type, scan_type)} ho√†n t·∫•t")
        return result

    except Exception as e:
        logger.error(f"Scan task {scan_event_id} failed: {e}")
        ScanEvent.objects.filter(id=scan_event_id).update(
            status=ScanStatus.FAILED,
            result_json={'error': str(e)}
        )
        return {'error': str(e)}


@shared_task(name='core.perform_image_scan_task', bind=True)
def perform_image_scan_task(self, scan_event_id, images_data):
    """
    Background task for multi-image / OCR scans.
    Provides real-time progress updates via Channels.
    Uses extract_ocr_with_boxes for annotated bounding-box images.
    """
    from api.core.models import ScanEvent, ScanStatus, RiskLevel
    from api.utils.media_utils import extract_ocr_with_boxes
    from api.utils.ollama_client import analyze_text_for_scam
    import base64
    import io
    import re
    from asgiref.sync import async_to_sync
    from channels.layers import get_channel_layer

    channel_layer = get_channel_layer()
    group_name = f'scan_{scan_event_id}'
    
    logger.info(f"Task started for event {scan_event_id}. Group: {group_name}")

    def send_progress(message, step='processing', data=None):
        logger.info(f"Progress [{scan_event_id}]: {message} ({step})")
        if channel_layer:
            try:
                async_to_sync(channel_layer.group_send)(
                    group_name,
                    {
                        'type': 'scan_progress',
                        'message': message,
                        'status': 'processing',
                        'step': step,
                        'data': data
                    }
                )
            except Exception as e:
                logger.error(f"WS send error for event {scan_event_id}: {e}")
        else:
            logger.warning(f"No channel layer available for event {scan_event_id}")

    try:
        scan_event = ScanEvent.objects.get(id=scan_event_id)
        scan_event.status = ScanStatus.PROCESSING
        scan_event.job_id = self.request.id
        scan_event.save()

        num_images = len(images_data)
        send_progress(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {num_images} h√¨nh ·∫£nh...", step="init")

        full_ocr = ""
        all_qr_contents = []
        annotated_images = []
        all_phones = set()
        all_urls = set()
        all_emails = set()
        all_bank_accounts = set()

        for i, img_b64 in enumerate(images_data):
            try:
                send_progress(f"ƒêang ph√¢n t√≠ch ·∫£nh {i+1}/{num_images} (OCR + QR)...", step="ocr")
                if ',' in img_b64: img_b64 = img_b64.split(',')[1]
                img_bytes = base64.b64decode(img_b64)
                img_file = io.BytesIO(img_bytes)
                img_file.name = f"image_{i}.png"
                img_file.size = len(img_bytes)
                
                ocr_result = extract_ocr_with_boxes(img_file)
                text = ocr_result.get("text", "")
                qr_contents = ocr_result.get("qr_contents", [])
                annotated_b64 = ocr_result.get("annotated_image_b64", "")
                
                if text:
                    full_ocr += text + "\n---\n"
                    send_progress(f"·∫¢nh {i+1}: Tr√≠ch xu·∫•t {len(text)} k√Ω t·ª±", step="ocr_ok")
                else:
                    send_progress(f"·∫¢nh {i+1}: Kh√¥ng ph√°t hi·ªán text", step="ocr_ok")
                
                if qr_contents:
                    all_qr_contents.extend(qr_contents)
                    send_progress(f"·∫¢nh {i+1}: Ph√°t hi·ªán {len(qr_contents)} m√£ QR", step="qr_ok")
                
                if annotated_b64:
                    annotated_images.append(annotated_b64)
                
                # Extract entities from OCR text
                if text:
                    phones = re.findall(r'(?:\+84|0)\d[\d\s\-\.]{7,12}\d', text)
                    all_phones.update(p.replace(' ', '').replace('-', '').replace('.', '') for p in phones)
                    urls = re.findall(r'https?://[^\s<>"]+|www\.[^\s<>"]+', text)
                    all_urls.update(urls)
                    emails = re.findall(r'[a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,}', text)
                    all_emails.update(emails)
                    bank_accs = re.findall(r'\b\d{8,19}\b', text)
                    # Filter out likely non-bank numbers (too short or phone-like)
                    for acc in bank_accs:
                        if len(acc) >= 8 and not any(acc == p for p in all_phones):
                            all_bank_accounts.add(acc)
                
            except Exception as e:
                logger.error(f"OCR Error in task for event {scan_event_id}: {e}")
                send_progress(f"C·∫£nh b√°o: L·ªói x·ª≠ l√Ω ·∫£nh {i+1}.", step="ocr_warning")

        # Also extract entities from QR contents
        for qr in all_qr_contents:
            urls_in_qr = re.findall(r'https?://[^\s<>"]+', qr)
            all_urls.update(urls_in_qr)

        if full_ocr or all_qr_contents:
            # Build analysis text including QR contents
            analysis_text = full_ocr
            if all_qr_contents:
                analysis_text += "\n[QR CODE CONTENTS]:\n" + "\n".join(all_qr_contents) + "\n"
            
            send_progress("ƒêang ph√¢n t√≠ch n·ªôi dung b·∫±ng Tr√≠ tu·ªá nh√¢n t·∫°o (AI)...", step="analyzing")
            
            # Retry mechanism for AI analysis
            ai_res = None
            for attempt in range(3):
                try:
                    ai_res = analyze_text_for_scam(analysis_text, use_web_search=True)
                    if ai_res and (ai_res.get('risk_score') is not None):
                        break
                except:
                    logger.warning(f"AI analysis attempt {attempt+1} failed for event {scan_event_id}")
            
            if not ai_res:
                 ai_res = {'risk_score': 0, 'risk_level': 'SAFE', 'explanation': 'L·ªói ph√¢n t√≠ch AI sau 3 l·∫ßn th·ª≠.'}

            ai_notice = ai_res.get('ai_notice')
            if ai_notice:
                send_progress(ai_notice, step="ai_retry_notice")
            
            send_progress("AI ƒë√£ ho√†n t·∫•t ph√¢n t√≠ch logic.", step="analyzing")
        else:
            send_progress("Kh√¥ng t√¨m th·∫•y k√Ω t·ª± kh·∫£ d·ª•ng trong ·∫£nh.", step="analyzing")
            ai_res = {'risk_score': 0, 'risk_level': 'SAFE', 'explanation': 'Kh√¥ng t√¨m th·∫•y ch·ªØ ho·∫∑c m√£ QR trong ·∫£nh ƒë·ªÉ ph√¢n t√≠ch.'}

        # Determine level correctly
        score = ai_res.get('risk_score', 0)
        level = RiskLevel.SAFE
        if score >= 80: level = RiskLevel.RED
        elif score >= 50: level = RiskLevel.YELLOW
        elif score >= 20: level = RiskLevel.GREEN

        # Map reason to explanation if needed
        explanation = ai_res.get('explanation') or ai_res.get('reason') or ''

        # Build entities dict
        entities = {}
        if all_phones:
            entities['phones'] = list(all_phones)[:20]
        if all_urls:
            entities['urls'] = list(all_urls)[:20]
        if all_emails:
            entities['emails'] = list(all_emails)[:10]
        if all_bank_accounts:
            entities['bank_accounts'] = list(all_bank_accounts)[:10]

        result = {
            'ocr_text': full_ocr,
            'risk_score': score,
            'risk_level': level,
            'explanation': explanation,
            'ai_notice': ai_res.get('ai_notice', ''),
            'ai_retry_used': ai_res.get('ai_retry_used', False),
            'ai_retry_count': ai_res.get('ai_retry_count', 0),
            'web_sources': ai_res.get('web_sources', []),
            'web_context': ai_res.get('web_context', ''),
            'qr_contents': list(set(all_qr_contents)),
            'entities': entities if entities else None,
            'annotated_images': annotated_images,
            'annotated_image': annotated_images[0] if annotated_images else '',
        }

        # Save to DB without large annotated images (they bloat the JSON field)
        scan_event.result_json = {k: v for k, v in result.items() if k not in ('annotated_images', 'annotated_image')}
        scan_event.risk_score = score
        scan_event.risk_level = level
        scan_event.status = ScanStatus.COMPLETED
        scan_event.save()
        
        send_progress("Ho√†n t·∫•t qu√©t h√¨nh ·∫£nh!", step="completed", data=result)
        _notify_scan_complete(scan_event, 'Qu√©t h√¨nh ·∫£nh/QR ho√†n t·∫•t')
        return result

    except Exception as e:
        logger.error(f"Image scan task {scan_event_id} failed: {e}")
        send_progress(f"L·ªói h·ªá th·ªëng: {str(e)}", step="error")
        ScanEvent.objects.filter(id=scan_event_id).update(
            status=ScanStatus.FAILED,
            result_json={'error': str(e)}
        )
        return {'error': str(e)}


@shared_task(name='core.perform_audio_scan_task', bind=True)
def perform_audio_scan_task(self, scan_event_id, audio_file_path):
    """
    Background task for audio transcription + AI scam analysis.
    Uses Faster-Whisper for speech-to-text, then Ollama for risk assessment.
    Provides real-time progress updates via Channels.
    """
    from api.core.models import ScanEvent, ScanStatus, RiskLevel
    from api.utils.media_utils import transcribe_audio, analyze_audio_risk
    from api.utils.ollama_client import analyze_text_for_scam
    from asgiref.sync import async_to_sync
    from channels.layers import get_channel_layer
    import os

    channel_layer = get_channel_layer()
    group_name = f'scan_{scan_event_id}'

    def send_progress(message, step='processing', data=None):
        logger.info(f"Audio [{scan_event_id}]: {message} ({step})")
        if channel_layer:
            try:
                async_to_sync(channel_layer.group_send)(
                    group_name,
                    {
                        'type': 'scan_progress',
                        'message': message,
                        'status': 'processing',
                        'step': step,
                        'data': data,
                    }
                )
            except Exception as e:
                logger.error(f"WS send error for audio event {scan_event_id}: {e}")

    try:
        scan_event = ScanEvent.objects.get(id=scan_event_id)
        scan_event.status = ScanStatus.PROCESSING
        scan_event.job_id = self.request.id
        scan_event.save()

        send_progress("ƒêang t·∫£i m√¥ h√¨nh nh·∫≠n di·ªán gi·ªçng n√≥i...", step="init")

        # ‚îÄ‚îÄ Step 1: Transcribe audio ‚îÄ‚îÄ
        send_progress("ƒêang chuy·ªÉn ƒë·ªïi gi·ªçng n√≥i th√†nh vƒÉn b·∫£n...", step="transcribing")

        # Open the saved temp file for transcription
        with open(audio_file_path, 'rb') as audio_fp:
            from django.core.files.uploadedfile import InMemoryUploadedFile
            import io
            audio_bytes = audio_fp.read()
            audio_file_obj = io.BytesIO(audio_bytes)
            audio_file_obj.name = os.path.basename(audio_file_path)
            transcription_result = transcribe_audio(audio_file_obj)

        transcript = transcription_result.get('transcript', '')
        language = transcription_result.get('language', 'unknown')
        duration = transcription_result.get('duration', 0)
        segments = transcription_result.get('segments', [])

        if transcript:
            send_progress(
                f"ƒê√£ nh·∫≠n di·ªán {len(segments)} ƒëo·∫°n ‚Ä¢ {duration:.1f}s ‚Ä¢ Ng√¥n ng·ªØ: {language}",
                step="transcribed",
                data={'transcript': transcript, 'language': language, 'duration': duration}
            )
        else:
            send_progress("Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c gi·ªçng n√≥i trong file √¢m thanh.", step="transcribed")

        # ‚îÄ‚îÄ Step 2: AI Scam Analysis on transcript ‚îÄ‚îÄ
        if transcript:
            send_progress("ƒêang ph√¢n t√≠ch d·∫•u hi·ªáu l·ª´a ƒë·∫£o trong ƒëo·∫°n h·ªôi tho·∫°i b·∫±ng AI...", step="analyzing")

            ai_res = None
            for attempt in range(3):
                try:
                    ai_res = analyze_text_for_scam(transcript, use_web_search=True)
                    if ai_res and ai_res.get('risk_score') is not None:
                        break
                except Exception:
                    logger.warning(f"AI analysis attempt {attempt+1} failed for audio event {scan_event_id}")

            if not ai_res:
                ai_res = {
                    'risk_score': 0,
                    'risk_level': 'SAFE',
                    'explanation': 'L·ªói ph√¢n t√≠ch AI sau 3 l·∫ßn th·ª≠.',
                }

            ai_notice = ai_res.get('ai_notice')
            if ai_notice:
                send_progress(ai_notice, step="ai_retry_notice")

            send_progress("AI ƒë√£ ho√†n t·∫•t ph√¢n t√≠ch d·∫•u hi·ªáu l·ª´a ƒë·∫£o.", step="analyzed")
        else:
            ai_res = {
                'risk_score': 0,
                'risk_level': 'SAFE',
                'explanation': 'Kh√¥ng t√¨m th·∫•y gi·ªçng n√≥i ƒë·ªÉ ph√¢n t√≠ch.',
            }

        # ‚îÄ‚îÄ Step 3: Determine risk level and save ‚îÄ‚îÄ
        score = ai_res.get('risk_score', 0)
        level = RiskLevel.SAFE
        if score >= 80:
            level = RiskLevel.RED
        elif score >= 50:
            level = RiskLevel.YELLOW
        elif score >= 20:
            level = RiskLevel.GREEN

        explanation = ai_res.get('explanation') or ai_res.get('reason') or ''

        result = {
            'transcript': transcript,
            'language': language,
            'duration': duration,
            'segments': segments,
            'risk_score': score,
            'risk_level': level,
            'explanation': explanation,
            'ai_notice': ai_res.get('ai_notice', ''),
            'ai_retry_used': ai_res.get('ai_retry_used', False),
            'ai_retry_count': ai_res.get('ai_retry_count', 0),
            'web_sources': ai_res.get('web_sources', []),
            'web_context': ai_res.get('web_context', ''),
        }

        scan_event.result_json = result
        scan_event.risk_score = score
        scan_event.risk_level = level
        scan_event.status = ScanStatus.COMPLETED
        scan_event.save()

        send_progress("Ho√†n t·∫•t qu√©t √¢m thanh!", step="completed", data=result)
        _notify_scan_complete(scan_event, 'Qu√©t √¢m thanh ho√†n t·∫•t')

        # Cleanup temp file
        try:
            if os.path.exists(audio_file_path):
                os.remove(audio_file_path)
        except Exception:
            pass

        return result

    except Exception as e:
        logger.error(f"Audio scan task {scan_event_id} failed: {e}")
        send_progress(f"L·ªói h·ªá th·ªëng: {str(e)}", step="error")
        ScanEvent.objects.filter(id=scan_event_id).update(
            status=ScanStatus.FAILED,
            result_json={'error': str(e)},
        )
        # Cleanup temp file
        try:
            if os.path.exists(audio_file_path):
                os.remove(audio_file_path)
        except Exception:
            pass
        return {'error': str(e)}


@shared_task(name='core.perform_web_scrapping_task', bind=True)
def perform_web_scrapping_task(self, scan_event_id, url):
    """
    Task to scrape web content and analyze with AI for scam/phishing signs.
    Provides real-time progress updates via Channels.
    """
    from api.core.models import ScanEvent, ScanStatus, RiskLevel
    from api.utils.ollama_client import analyze_text_for_scam
    import whois
    import dns.resolver
    from ipwhois import IPWhois, IPDefinedError
    from api.utils.normalization import normalize_domain
    from asgiref.sync import async_to_sync
    from channels.layers import get_channel_layer

    channel_layer = get_channel_layer()
    group_name = f'scan_{scan_event_id}'

    def send_progress(message, step='processing', data=None):
        if not channel_layer:
            logger.warning(f"[WebScan] Event {scan_event_id}: No channel layer; progress not sent")
            return
        try:
            async_to_sync(channel_layer.group_send)(
                group_name,
                {
                    'type': 'scan_progress',
                    'message': message,
                    'status': 'processing',
                    'step': step,
                    'data': data
                }
            )
        except Exception as e:
            logger.warning(f"[WebScan] Event {scan_event_id}: Progress send failed: {e}")

    def _decode_html_response(http_response):
        html_text = http_response.text or ''
        if any(marker in html_text for marker in ('√É', '√Ç', '√°¬ª', '\ufffd')):
            try:
                encoding = http_response.apparent_encoding or 'utf-8'
                html_text = http_response.content.decode(encoding, errors='replace')
            except Exception:
                pass
        return html_text

    def _repair_mojibake(text: str) -> str:
        if not text:
            return ''
        repaired = text.strip()
        if any(marker in repaired for marker in ('√É', '√Ç', '√°¬ª', '√Ñ')):
            try:
                repaired_candidate = repaired.encode('latin-1', errors='ignore').decode('utf-8', errors='ignore').strip()
                if repaired_candidate:
                    repaired = repaired_candidate
            except Exception:
                pass
        return re.sub(r'\s+', ' ', repaired)

    try:
        scan_event = ScanEvent.objects.get(id=scan_event_id)
        scan_event.status = ScanStatus.PROCESSING
        scan_event.job_id = self.request.id
        scan_event.save()

        send_progress(f"B·∫Øt ƒë·∫ßu ph√¢n t√≠ch website...", step="init")
        
        domain = normalize_domain(url)
        logger.info(f"[WebScan] Event {scan_event_id}: Starting analysis for URL: {url};  Normalized: {domain}")
        send_progress(f"T√™n mi·ªÅn: {domain}", step="init")

        content = ""
        network_risk_score = 0
        network_details = []

        # --- NETWORK ANALYSIS (Deep) ---
        send_progress("ƒêang ki·ªÉm tra th√¥ng tin t√™n mi·ªÅn (WHOIS, DNS)...", step="network_analysis")
        try:
            # 1. WHOIS Age
            try:
                send_progress("ƒêang tra c·ª©u WHOIS (tu·ªïi t√™n mi·ªÅn, ch·ªß s·ªü h·ªØu)...", step="whois")
                w = whois.whois(domain)
                creation_date = w.creation_date
                if isinstance(creation_date, list):
                    creation_date = creation_date[0]
                
                if creation_date:
                    # Robust date conversion
                    if hasattr(creation_date, 'date'):
                         creation_dt = creation_date.date()
                    else:
                         creation_dt = creation_date # Assume it's already date or compatible

                    age_days = (timezone.now().date() - creation_dt).days
                    
                    if age_days < 14:
                        network_risk_score = max(network_risk_score, 75) # Very high risk if new
                        network_details.append(f"T√™n mi·ªÅn qu√° m·ªõi (ƒëƒÉng k√Ω {age_days} ng√†y tr∆∞·ªõc)")
                        send_progress(f"‚ö†Ô∏è T√™n mi·ªÅn r·∫•t m·ªõi: ch·ªâ {age_days} ng√†y tu·ªïi - d·∫•u hi·ªáu nghi ng·ªù!", step="whois_warning")
                        logger.info(f"[WebScan] Event {scan_event_id}: Domain too new ({age_days} days)")
                    elif age_days < 30:
                         network_risk_score += 10
                         network_details.append(f"T√™n mi·ªÅn m·ªõi (ƒëƒÉng k√Ω {age_days} ng√†y tr∆∞·ªõc)")
                         send_progress(f"T√™n mi·ªÅn kh√° m·ªõi: {age_days} ng√†y tu·ªïi", step="whois_info")
                         logger.info(f"[WebScan] Event {scan_event_id}: Domain new ({age_days} days)")
                    else:
                         send_progress(f"WHOIS: T√™n mi·ªÅn {age_days} ng√†y tu·ªïi", step="whois_ok")
            except Exception as e:
                logger.warning(f"[WebScan] Event {scan_event_id}: WHOIS lookup failed: {e}")

            # 2. DNS Checks (Resolvers)
            try:
                send_progress("ƒêang ki·ªÉm tra DNS (MX records)...", step="dns")
                # Check for MX records (Phishing sites often lack email setup)
                try:
                    dns.resolver.resolve(domain, 'MX')
                    has_mx = True
                    logger.info(f"[WebScan] Event {scan_event_id}: MX records found")
                except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):
                    has_mx = False
                    # Only penalize if it pretends to be a bank/corp
                    network_risk_score += 5
                    logger.info(f"[WebScan] Event {scan_event_id}: No MX records found")
                    
                if not has_mx:
                    network_risk_score += 7
            except Exception:
                pass

            # 3. ASN / IP Reputation
            try:
                send_progress("ƒêang ki·ªÉm tra ASN / IP Reputation...", step="asn")
                import socket
                ip = socket.gethostbyname(domain)
                obj = IPWhois(ip)
                try:
                    res = obj.lookup_rdap(depth=1)
                    asn_desc = res.get('asn_description', '') or res.get('asn_registry', '')
                    
                    # Check for known bad ASNs or cheap hosting
                    suspicious_keywords = ['Hetzner', 'DigitalOcean', 'Choopa', 'Namecheap', 'Vultr', 'Linode', 'Hostinger']
                    if any(k.lower() in asn_desc.lower() for k in suspicious_keywords):
                        # Cheap hosting itself isn't bad, but phishing often uses it
                        network_risk_score += 7
                        network_details.append(f"Hosting provider: {asn_desc}")
                        send_progress(f"Hosting: {asn_desc} (th∆∞·ªùng d√πng cho phishing)", step="asn_warning")
                        logger.info(f"[WebScan] Event {scan_event_id}: Suspicious/Cheap hosting provider found: {asn_desc}")
                    else:
                        send_progress(f"IP/ASN: {asn_desc}", step="asn_ok")
                except IPDefinedError:
                     pass
            except Exception as e:
                logger.warning(f"[WebScan] Event {scan_event_id}: ASN lookup failed: {e}")

            send_progress(f"ƒê√£ ho√†n t·∫•t ki·ªÉm tra m·∫°ng. ƒêi·ªÉm r·ªßi ro m·∫°ng: {network_risk_score}", step="network_analysis")
            logger.info(f"[WebScan] Event {scan_event_id}: Network scan complete. Score: {network_risk_score}")

        except Exception as e:
            logger.error(f"[WebScan] Event {scan_event_id}: Deep network scan failed: {e}")
            send_progress(f"L·ªói ki·ªÉm tra m·∫°ng: {str(e)}", step="network_warning")
        
        # 1. Fetch Content using requests + BeautifulSoup (replacing Ollama web_fetch)
        send_progress(f"ƒêang thu th·∫≠p n·ªôi dung t·ª´ {domain}...", step="scraping")
        
        # Build full URL if scheme not provided
        if '://' not in url:
            target_url = f"https://{url}"
        else:
            target_url = url

        fetch_success = False
        page_title = ""
        
        try:
            import requests as http_requests
            from bs4 import BeautifulSoup

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            }
            resp = http_requests.get(target_url, headers=headers, timeout=15, verify=True, allow_redirects=True)
            resp.raise_for_status()

            html_text = _decode_html_response(resp)
            soup = BeautifulSoup(html_text, 'html.parser')

            # Extract title
            title_tag = soup.find('title')
            page_title = _repair_mojibake(title_tag.get_text(strip=True) if title_tag else '')

            # Remove script/style/nav/footer for cleaner content
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'noscript', 'iframe']):
                tag.decompose()

            content = soup.get_text(separator='\n', strip=True)
            if content:
                fetch_success = True
                send_progress(f"ƒê√£ t·∫£i n·ªôi dung website ({len(content)} k√Ω t·ª±)", step="scraping_ok")
                logger.info(f"[WebScan] Event {scan_event_id}: Fetched content via requests+BS4 ({len(content)} chars)")

            # Try HTTP fallback if HTTPS failed and no content
            if not fetch_success and target_url.startswith('https://'):
                http_url = target_url.replace('https://', 'http://', 1)
                resp = http_requests.get(http_url, headers=headers, timeout=15, verify=False, allow_redirects=True)
                resp.raise_for_status()
                html_text = _decode_html_response(resp)
                soup = BeautifulSoup(html_text, 'html.parser')
                title_tag = soup.find('title')
                page_title = _repair_mojibake(title_tag.get_text(strip=True) if title_tag else '')
                for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'noscript', 'iframe']):
                    tag.decompose()
                content = soup.get_text(separator='\n', strip=True)
                if content:
                    fetch_success = True
                    logger.info(f"[WebScan] Event {scan_event_id}: Fetched content via HTTP fallback ({len(content)} chars)")
        except Exception as e:
            logger.warning(f"[WebScan] Event {scan_event_id}: Web fetch failed: {e}")

        if not fetch_success:
            # ‚ùå Mark fetch failure but CONTINUE analysis
            send_progress(f"‚ùå Kh√¥ng th·ªÉ thu th·∫≠p n·ªôi dung website (ti·∫øp t·ª•c ph√¢n t√≠ch...)", step="scraping_warning")
            content = ""
            network_details.append("‚ùå Kh√¥ng th·ªÉ truy c·∫≠p n·ªôi dung website")
            logger.warning(f"[WebScan] Event {scan_event_id}: Failed to fetch content, continuing with network analysis only")

        # 2. AI Analysis
        send_progress("ƒêang ph√¢n t√≠ch n·ªôi dung b·∫±ng Tr√≠ tu·ªá nh√¢n t·∫°o (AI)...", step="analyzing")
        # Keep the content snapshot short so the prompt stays within the local
        # model's context window.  Web-search enrichment is skipped here because
        # this task already performs dedicated network/WHOIS intelligence above.
        content_section = f"Page Content Snapshot:\n{content[:2500]}" if content else "(Kh√¥ng c√≥ n·ªôi dung - kh√¥ng th·ªÉ truy c·∫≠p website)"
        ai_input = (
            f"Domain: {domain}\nURL: {url}\n"
            f"Page Title: {page_title}\n" if page_title else f"Domain: {domain}\nURL: {url}\n"
        ) + (
            f"Network findings: {', '.join(network_details) if network_details else 'none'}\n\n"
            f"{content_section}"
        )
        logger.info(f"[WebScan] Event {scan_event_id}: Sending to AI ({len(ai_input)} chars)")

        try:
            import time
            from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeout

            ai_timeout_seconds = 600
            keepalive_every_seconds = 10
            executor = ThreadPoolExecutor(max_workers=1)
            future = executor.submit(analyze_text_for_scam, ai_input, None, True)
            start_time = time.time()
            last_keepalive = 0
            ai_res = None

            while True:
                try:
                    ai_res = future.result(timeout=5)
                    break
                except FutureTimeout:
                    elapsed = time.time() - start_time
                    if elapsed - last_keepalive >= keepalive_every_seconds:
                        send_progress("AI ƒëang ph√¢n t√≠ch...", step="ping")
                        last_keepalive = elapsed
                    if elapsed >= ai_timeout_seconds:
                        ai_res = {
                            'is_scam': False,
                            'risk_score': 0,
                            'indicators': [],
                            'explanation': 'AI ph√¢n t√≠ch qu√° th·ªùi gian cho ph√©p.'
                        }
                        future.cancel()
                        break
            executor.shutdown(wait=False, cancel_futures=True)
        except Exception as _ai_exc:
            import traceback
            logger.warning(traceback.format_exc())
            logger.warning(f"[WebScan] Event {scan_event_id}: AI analysis failed ({_ai_exc}), using fallback.")
            ai_res = {'is_scam': False, 'risk_score': 0, 'indicators': [], 'explanation': 'AI kh√¥ng kh·∫£ d·ª•ng.'}

        logger.info(f"[WebScan] Event {scan_event_id}: AI Result: {ai_res}")
        ai_notice = ai_res.get('ai_notice')
        if ai_notice:
            send_progress(ai_notice, step="ai_retry_notice")
        send_progress("AI ƒë√£ ho√†n t·∫•t ph√¢n t√≠ch.", step="analyzing")

        final_risk_score = max(ai_res.get('risk_score', 0), network_risk_score)
        
        # Combine networks details
        explanation = ai_res.get('explanation') or ai_res.get('reason') or ''
        if network_details:
             explanation += f"\n\nPh√°t hi·ªán t·ª´ m·∫°ng l∆∞·ªõi: {', '.join(network_details)}"

        result = {
            'domain': domain,
            'url': url,
            'page_title': page_title,
            'fetch_success': fetch_success,
            'risk_score': final_risk_score,
            'risk_level': ai_res.get('risk_level', 'SAFE'),
            'explanation': explanation,
            'ai_notice': ai_notice,
            'ai_retry_used': ai_res.get('ai_retry_used', False),
            'ai_retry_count': ai_res.get('ai_retry_count', 0),
            'scam_type': ai_res.get('scam_type') or ai_res.get('type') or 'other',
            'content_length': len(content) if content else 0,
            'network_details': network_details,
            'web_sources': ai_res.get('web_sources', []),
            'web_context': ai_res.get('web_context', '')
        }
        
        # Adjust risk level based on network score
        if final_risk_score >= 80:
            result['risk_level'] = RiskLevel.RED
        elif final_risk_score >= 40:
             result['risk_level'] = RiskLevel.YELLOW
        elif final_risk_score >= 10:
             result['risk_level'] = RiskLevel.GREEN

        scan_event.result_json = result
        scan_event.risk_score = result['risk_score']
        scan_event.risk_level = result['risk_level']
        scan_event.status = ScanStatus.COMPLETED
        scan_event.save()
        
        send_progress("Ho√†n t·∫•t qu√©t website!", step="completed", data=result)
        _notify_scan_complete(scan_event, f"Qu√©t website '{url}' ho√†n t·∫•t")
        return result

    except Exception as e:
        logger.error(f"Web scan task {scan_event_id} failed: {e}")
        send_progress(f"L·ªói h·ªá th·ªëng: {str(e)}", step="error")
        ScanEvent.objects.filter(id=scan_event_id).update(
            status=ScanStatus.FAILED,
            result_json={'error': str(e)}
        )
        return {'error': str(e)}

@shared_task
def process_forum_report(report_id):
    """
    AI processes a forum report:
    1. Analyze the reported post content and reason using Ollama.
    2. If AI confirms a violation (confidence > 0.7), flag as AI_FLAGGED.
    3. If it's a clear violation, we can auto-approve it.
    """
    from api.core.models import ForumPostReport
    from api.utils.ollama_client import generate_response
    import json as _json
    try:
        report = ForumPostReport.objects.select_related('post', 'post__author').get(id=report_id)
        post = report.post
        
        # Analyze content using generate_response with structured JSON output
        prompt = f"""Ph√¢n t√≠ch b√†i vi·∫øt di·ªÖn ƒë√†n sau ƒë√¢y xem c√≥ vi ph·∫°m ti√™u chu·∫©n c·ªông ƒë·ªìng (l·ª´a ƒë·∫£o, qu·∫•y r·ªëi, n·ªôi dung ƒë·ªôc h·∫°i, spam, ƒëe d·ªça) hay kh√¥ng.

Ti√™u ƒë·ªÅ: {post.title}
N·ªôi dung: {post.content[:2000]}
L√Ω do b√°o c√°o t·ª´ ng∆∞·ªùi d√πng: {report.reason}

H√£y ph√¢n t√≠ch kh√°ch quan v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON v·ªõi c√°c tr∆∞·ªùng:
- violation: true n·∫øu vi ph·∫°m, false n·∫øu kh√¥ng
- confidence: s·ªë t·ª´ 0 ƒë·∫øn 1 th·ªÉ hi·ªán m·ª©c ƒë·ªô ch·∫Øc ch·∫Øn
- reason: gi·∫£i th√≠ch ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát"""

        format_schema = {
            "type": "object",
            "properties": {
                "violation": {"type": "boolean"},
                "confidence": {"type": "number"},
                "reason": {"type": "string"}
            },
            "required": ["violation", "confidence", "reason"]
        }
        
        raw_result = generate_response(prompt, format_schema=format_schema)
        
        # Parse the JSON result
        try:
            analysis_res = _json.loads(raw_result) if isinstance(raw_result, str) else {}
        except (_json.JSONDecodeError, TypeError):
            analysis_res = {'violation': False, 'confidence': 0, 'reason': 'Kh√¥ng th·ªÉ ph√¢n t√≠ch (l·ªói format)'}
        
        report.ai_analysis = analysis_res
        
        from api.utils.email_utils import send_report_outcome_email
        from api.utils.push_service import PushNotificationService
        
        violation = analysis_res.get('violation')
        confidence = analysis_res.get('confidence', 0)
        ai_reason = analysis_res.get('reason', '')
        
        if violation is True and confidence > 0.8:
            report.status = ForumPostReport.ReportStatus.APPROVED
            report.is_resolved = True
            
            # Lock the post if it's a clear violation
            post.is_locked = True
            post.save(update_fields=['is_locked'])
            
            logger.info(f"Report #{report_id} auto-approved by AI. Post #{post.id} locked.")
            
            # Notify reporter via email
            send_report_outcome_email(report.reporter, "b√†i vi·∫øt", post.title, 'approved', ai_reason)
            
            # Push notification to reporter
            PushNotificationService.send_push(
                report.reporter.id,
                'B√°o c√°o ƒë∆∞·ª£c ch·∫•p thu·∫≠n',
                f'B√°o c√°o b√†i vi·∫øt "{post.title[:50]}" ƒë√£ ƒë∆∞·ª£c AI x√°c nh·∫≠n vi ph·∫°m v√† b√†i vi·∫øt ƒë√£ b·ªã kh√≥a.',
                url=f'/forum/{post.id}/',
                notification_type='success'
            )
            # Notify admins
            PushNotificationService.broadcast_admin(
                'AI: Vi ph·∫°m ph√°t hi·ªán',
                f'B√†i vi·∫øt "{post.title[:50]}" b·ªã AI ph√°t hi·ªán vi ph·∫°m (conf: {confidence}). ƒê√£ t·ª± ƒë·ªông kh√≥a.',
                url='/admin-cp/forum/',
                notification_type='warning'
            )
        elif violation is True:
            report.status = ForumPostReport.ReportStatus.AI_FLAGGED
            logger.info(f"Report #{report_id} flagged by AI for manual review.")
            
            # Notify reporter 
            send_report_outcome_email(report.reporter, "b√†i vi·∫øt", post.title, 'reviewing', ai_reason)
            PushNotificationService.send_push(
                report.reporter.id,
                'B√°o c√°o ƒëang ƒë∆∞·ª£c xem x√©t',
                f'AI ƒë√£ g·∫Øn c·ªù b√†i vi·∫øt "{post.title[:50]}". Admin s·∫Ω xem x√©t th√™m.',
                url=f'/forum/{post.id}/',
                notification_type='info'
            )
            # Notify admins  
            PushNotificationService.broadcast_admin(
                'AI: C·∫ßn xem x√©t',
                f'B√†i vi·∫øt "{post.title[:50]}" b·ªã AI g·∫Øn c·ªù (conf: {confidence}). C·∫ßn admin xem x√©t.',
                url='/admin-cp/forum/',
                notification_type='warning'
            )
        else:
            # AI says safe
            send_report_outcome_email(report.reporter, "b√†i vi·∫øt", post.title, 'rejected', ai_reason)
            PushNotificationService.send_push(
                report.reporter.id,
                'K·∫øt qu·∫£ ph√¢n t√≠ch b√°o c√°o',
                f'AI kh√¥ng ph√°t hi·ªán vi ph·∫°m trong b√†i vi·∫øt "{post.title[:50]}". C·∫£m ∆°n b·∫°n ƒë√£ b√°o c√°o!',
                url=f'/forum/{post.id}/',
                notification_type='info'
            )
            
        report.save()
        
    except ForumPostReport.DoesNotExist:
        logger.error(f"Report #{report_id} not found in process_forum_report task.")
    except Exception as e:
        logger.error(f"Error in process_forum_report task: {e}")

@shared_task
def process_forum_comment_report(report_id):
    """
    AI processes a forum comment report.
    """
    from api.core.models import ForumCommentReport
    from api.utils.ollama_client import generate_response
    from api.utils.email_utils import send_report_outcome_email
    import json as _json
    
    try:
        report = ForumCommentReport.objects.select_related('comment', 'comment__author', 'reporter').get(id=report_id)
        comment = report.comment
        
        prompt = f"""Ph√¢n t√≠ch b√¨nh lu·∫≠n di·ªÖn ƒë√†n sau ƒë√¢y xem c√≥ vi ph·∫°m ti√™u chu·∫©n c·ªông ƒë·ªìng (l·ª´a ƒë·∫£o, qu·∫•y r·ªëi, n·ªôi dung ƒë·ªôc h·∫°i, spam, ƒëe d·ªça) hay kh√¥ng.

N·ªôi dung b√¨nh lu·∫≠n: {comment.content[:2000]}
L√Ω do b√°o c√°o t·ª´ ng∆∞·ªùi d√πng: {report.reason}

H√£y ph√¢n t√≠ch kh√°ch quan v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON v·ªõi c√°c tr∆∞·ªùng:
- violation: true n·∫øu vi ph·∫°m, false n·∫øu kh√¥ng
- confidence: s·ªë t·ª´ 0 ƒë·∫øn 1 th·ªÉ hi·ªán m·ª©c ƒë·ªô ch·∫Øc ch·∫Øn
- reason: gi·∫£i th√≠ch ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát"""

        format_schema = {
            "type": "object",
            "properties": {
                "violation": {"type": "boolean"},
                "confidence": {"type": "number"},
                "reason": {"type": "string"}
            },
            "required": ["violation", "confidence", "reason"]
        }
        
        raw_result = generate_response(prompt, format_schema=format_schema)
        
        try:
            analysis_res = _json.loads(raw_result) if isinstance(raw_result, str) else {}
        except (_json.JSONDecodeError, TypeError):
            analysis_res = {'violation': False, 'confidence': 0, 'reason': 'Kh√¥ng th·ªÉ ph√¢n t√≠ch (l·ªói format)'}
        
        report.ai_analysis = analysis_res
        
        from api.utils.email_utils import send_report_outcome_email
        from api.utils.push_service import PushNotificationService
        
        violation = analysis_res.get('violation')
        confidence = analysis_res.get('confidence', 0)
        ai_reason = analysis_res.get('reason', '')
        comment_preview = comment.content[:50] + '...' if len(comment.content) > 50 else comment.content
        
        if violation is True and confidence > 0.8:
            report.status = ForumCommentReport.ReportStatus.APPROVED
            report.is_resolved = True
            
            logger.info(f"Comment Report #{report_id} auto-approved by AI.")
            
            send_report_outcome_email(report.reporter, "b√¨nh lu·∫≠n", comment_preview, 'approved', ai_reason)
            PushNotificationService.send_push(
                report.reporter.id,
                'B√°o c√°o b√¨nh lu·∫≠n ƒë∆∞·ª£c ch·∫•p thu·∫≠n',
                f'B√¨nh lu·∫≠n b·∫°n b√°o c√°o ƒë√£ ƒë∆∞·ª£c x√°c nh·∫≠n vi ph·∫°m b·ªüi AI.',
                notification_type='success'
            )
            PushNotificationService.broadcast_admin(
                'AI: B√¨nh lu·∫≠n vi ph·∫°m',
                f'B√¨nh lu·∫≠n "{comment_preview}" b·ªã AI ph√°t hi·ªán vi ph·∫°m (conf: {confidence}).',
                url='/admin-cp/forum/',
                notification_type='warning'
            )
        elif violation is True:
            report.status = ForumCommentReport.ReportStatus.AI_FLAGGED
            logger.info(f"Comment Report #{report_id} flagged by AI for manual review.")
            
            send_report_outcome_email(report.reporter, "b√¨nh lu·∫≠n", comment_preview, 'reviewing', ai_reason)
            PushNotificationService.send_push(
                report.reporter.id,
                'B√°o c√°o ƒëang ƒë∆∞·ª£c xem x√©t',
                f'AI ƒë√£ g·∫Øn c·ªù b√¨nh lu·∫≠n b·∫°n b√°o c√°o. Admin s·∫Ω xem x√©t th√™m.',
                notification_type='info'
            )
            PushNotificationService.broadcast_admin(
                'AI: BL c·∫ßn xem x√©t',
                f'B√¨nh lu·∫≠n "{comment_preview}" b·ªã AI g·∫Øn c·ªù (conf: {confidence}). C·∫ßn admin xem x√©t.',
                url='/admin-cp/forum/',
                notification_type='warning'
            )
        else:
            send_report_outcome_email(report.reporter, "b√¨nh lu·∫≠n", comment_preview, 'rejected', ai_reason)
            PushNotificationService.send_push(
                report.reporter.id,
                'K·∫øt qu·∫£ ph√¢n t√≠ch b√°o c√°o',
                f'AI kh√¥ng ph√°t hi·ªán vi ph·∫°m trong b√¨nh lu·∫≠n b·∫°n b√°o c√°o. C·∫£m ∆°n!',
                notification_type='info'
            )
            
        report.save()
        
    except ForumCommentReport.DoesNotExist:
        logger.error(f"Comment Report #{report_id} not found.")
    except Exception as e:
        logger.error(f"Error in process_forum_comment_report: {e}")

@shared_task
def send_bulk_lesson_email(lesson_id):
    """
    Sends notification email to all users who have profiles.
    """
    from api.core.models import LearnLesson, UserProfile
    from api.utils.email_utils import send_new_lesson_email
    from django.conf import settings
    
    try:
        lesson = LearnLesson.objects.get(id=lesson_id)
        if not lesson.is_published: return
        
        emails = list(UserProfile.objects.filter(user__is_active=True).values_list('user__email', flat=True))
        emails = [e for e in emails if e] # Filter valid ones
        
        if not emails: return
        
        # Site URL fallback
        site_url = getattr(settings, 'SITE_URL', 'https://shieldcall.vn')
        lesson_url = f"{site_url}/learn/{lesson.slug}/"
        
        # Send in batches of 10 to avoid spam limits/timeouts
        for i in range(0, len(emails), 10):
            batch = emails[i:i+10]
            send_new_lesson_email(batch, lesson.title, lesson_url)
            
    except LearnLesson.DoesNotExist:
        pass
    except Exception as e:
        logger.error(f"Error in send_bulk_lesson_email: {e}")

@shared_task(name='core.analyze_content_task', bind=True)
def analyze_content_task(self, scan_event_id, content: str, urls: list = None):
    """
    Perform deep analysis on text content (Email/SMS) using LLM and URL checks.
    """
    from api.core.models import ScanEvent, ScanStatus, RiskLevel
    from api.utils.ollama_client import analyze_text_for_scam
    # from api.utils.vt_client import VTClient # Uncomment if VT is ready

    if urls is None: urls = []

    try:
        scan = ScanEvent.objects.get(id=scan_event_id)
        scan.status = ScanStatus.PROCESSING
        scan.job_id = self.request.id
        scan.save()
        
        logger.info(f"Analyzing content for ScanEvent #{scan_event_id}...")
        
        # 1. AI Analysis of Text Body
        # Summarize content if too long
        ai_input = content[:5000] 
        if urls:
            ai_input += f"\n\nContained URLs: {', '.join(urls)}"
            
        ai_res = analyze_text_for_scam(ai_input, use_web_search=True)
        
        current_risk_score = scan.risk_score
        current_details = scan.result_json or {}
        
        ai_score = ai_res.get('risk_score', 0)
        
        # Merge results - AI usually gives the comprehensive verdict
        final_score = max(current_risk_score, ai_score)
        
        if 'reasons' not in current_details:
             current_details['reasons'] = []
             
        # Add AI explanation
        ai_notice = ai_res.get('ai_notice')
        if ai_notice:
            current_details['ai_notice'] = ai_notice
            current_details['reasons'].append(ai_notice)
        explanation = ai_res.get('explanation') or ai_res.get('reason')
        if explanation:
            current_details['ai_explanation'] = explanation
        
        current_details['ai_full_response'] = ai_res
        # Include web sources and context at top level for frontend
        current_details['web_sources'] = ai_res.get('web_sources', [])
        current_details['web_context'] = ai_res.get('web_context', '')
        
        # 2. URL Analysis (Logic can be expanded here)
        if urls and final_score < 100:
             # Basic heuristic if not using VT
             suspicious_tlds = ['.xyz', '.top', '.club', '.info', '.ru', '.cn']
             for url in urls:
                 if any(tld in url for tld in suspicious_tlds):
                     final_score = max(final_score, 60)
                     current_details['reasons'].append(f"Suspicious URL TLD found: {url}")

        final_score = min(100, final_score)
        
        # Determine Level
        level = RiskLevel.SAFE
        if final_score >= 80: level = RiskLevel.RED
        elif final_score >= 40: level = RiskLevel.YELLOW
        elif final_score >= 15: level = RiskLevel.GREEN

        # Update ScanEvent
        scan.risk_score = final_score
        scan.risk_level = level
        scan.result_json = current_details
        scan.status = ScanStatus.COMPLETED
        scan.save()
        
        logger.info(f"ScanEvent #{scan_event_id} analysis complete. Score: {final_score}")
        
    except ScanEvent.DoesNotExist:
        logger.error(f"ScanEvent #{scan_event_id} not found.")
    except Exception as e:
        logger.error(f"analyze_content_task failed: {e}")
        if 'scan' in locals():
            scan.status = ScanStatus.FAILED
            scan.save()

@shared_task(name='core.perform_message_scan_task', bind=True)
def perform_message_scan_task(self, scan_event_id: int, message_text: str, images_b64: list):
    """
    Background task for message scan with real-time WS progress.
    Processes OCR per image, pattern analysis, then AI analysis.
    """
    from api.core.models import ScanEvent, ScanStatus, RiskLevel
    from api.utils.ollama_client import analyze_text_for_scam
    from api.utils.normalization import normalize_domain
    from api.utils.vt_client import VTClient
    from asgiref.sync import async_to_sync
    from channels.layers import get_channel_layer
    import base64
    import io

    channel_layer = get_channel_layer()
    group_name = f'scan_{scan_event_id}'

    def send_progress(message, step='processing', data=None):
        if not channel_layer:
            return
        try:
            payload = {
                'type': 'scan_progress',
                'message': message,
                'status': 'processing',
                'step': step,
            }
            if data is not None:
                payload['data'] = data
            async_to_sync(channel_layer.group_send)(group_name, payload)
        except Exception as e:
            logger.warning(f"[MsgScan] Event {scan_event_id}: Progress send failed: {e}")

    try:
        scan_event = ScanEvent.objects.get(id=scan_event_id)
        scan_event.status = ScanStatus.PROCESSING
        scan_event.job_id = self.request.id
        scan_event.save()

        send_progress("B·∫Øt ƒë·∫ßu ph√¢n t√≠ch tin nh·∫Øn...", step="init")

        # 1. OCR Processing
        combined_ocr_text = []
        annotated_images = []

        if images_b64:
            send_progress(f"ƒêang x·ª≠ l√Ω {len(images_b64)} ·∫£nh b·∫±ng OCR...", step="ocr")
            from api.utils.media_utils import extract_ocr_with_boxes
            from django.core.files.uploadedfile import InMemoryUploadedFile

            for idx, img_b64 in enumerate(images_b64):
                send_progress(f"ƒêang nh·∫≠n d·∫°ng text t·ª´ ·∫£nh {idx+1}/{len(images_b64)}...", step="ocr")
                try:
                    img_bytes = base64.b64decode(img_b64)
                    img_file = io.BytesIO(img_bytes)
                    img_file.name = f"image_{idx}.png"
                    img_file.size = len(img_bytes)

                    ocr_result = extract_ocr_with_boxes(img_file)
                    if ocr_result.get("text"):
                        combined_ocr_text.append(ocr_result["text"])
                        send_progress(f"·∫¢nh {idx+1}: Tr√≠ch xu·∫•t {len(ocr_result['text'])} k√Ω t·ª±", step="ocr_ok")
                    else:
                        send_progress(f"·∫¢nh {idx+1}: Kh√¥ng ph√°t hi·ªán text", step="ocr_ok")
                    if ocr_result.get("annotated_image_b64"):
                        annotated_images.append(ocr_result["annotated_image_b64"])
                except Exception as e:
                    logger.error(f"[MsgScan] OCR Error for image {idx}: {e}")
                    send_progress(f"·∫¢nh {idx+1}: L·ªói OCR - {str(e)[:50]}", step="ocr_warning")

        # 2. Combine text
        full_text = message_text or ''
        if combined_ocr_text:
            full_text = (full_text + '\n' + '\n'.join(combined_ocr_text)).strip()

        if not full_text:
            send_progress("Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ ph√¢n t√≠ch.", step="error")
            scan_event.status = ScanStatus.FAILED
            scan_event.result_json = {'error': 'Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ ph√¢n t√≠ch.'}
            scan_event.save()
            return

        # 3. Pattern Analysis + Domain scan
        send_progress("ƒêang ph√°t hi·ªán d·∫•u hi·ªáu scam (t·ª´ kh√≥a, URL)...", step="pattern_analysis")

        patterns_found = []
        scam_keywords = {
            r'otp|m√£ x√°c': 'Y√™u c·∫ßu OTP',
            r'chuy·ªÉn kho·∫£n|chuy·ªÉn ti·ªÅn': 'Giao d·ªãch t√†i ch√≠nh',
            r'c√¥ng an|vi·ªán ki·ªÉm s√°t': 'M·∫°o danh c∆° quan ch·ª©c nƒÉng',
            r'tr√∫ng th∆∞·ªüng|qu√† t·∫∑ng': 'D·ª• d·ªó tr√∫ng th∆∞·ªüng',
            r'kh√≥a t√†i kho·∫£n|phong t·ªèa': 'ƒêe d·ªça t√†i kho·∫£n',
        }
        for pattern, label in scam_keywords.items():
            if re.search(pattern, full_text.lower()):
                patterns_found.append(label)

        found_urls = re.findall(r'https?://[^\s<>"]+|www\.[^\s<>"]+', full_text)
        domain_risks = []
        max_domain_score = 0
        vt = VTClient()
        for url in found_urls:
            domain = normalize_domain(url)
            if domain:
                send_progress(f"ƒêang ki·ªÉm tra domain: {domain}...", step="domain_check")
                vt_res = vt.scan_url(url)
                score = vt_res.get('risk_score', 0)
                max_domain_score = max(max_domain_score, score)
                if score > 0:
                    domain_risks.append(f"Domain {domain} r·ªßi ro cao ({score}/100)")

        if patterns_found or domain_risks:
            send_progress(f"Ph√°t hi·ªán {len(patterns_found) + len(domain_risks)} d·∫•u hi·ªáu ƒë√°ng ng·ªù", step="pattern_done")
        else:
            send_progress("Kh√¥ng ph√°t hi·ªán d·∫•u hi·ªáu r√µ r√†ng, ƒëang chuy·ªÉn sang AI...", step="pattern_done")

        # 4. AI Analysis
        send_progress("AI ƒëang ph√¢n t√≠ch n·ªôi dung tin nh·∫Øn...", step="ai_analysis")
        ai_score = 0
        ai_explanation = ''
        ai_available = True
        ai_result_data = None
        try:
            import time
            from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeout
            ai_timeout_seconds = 90
            keepalive_every_seconds = 8
            executor = ThreadPoolExecutor(max_workers=1)
            future = executor.submit(analyze_text_for_scam, full_text)
            start_time = time.time()
            last_keepalive = 0
            ai_result_data = None

            while True:
                try:
                    ai_result_data = future.result(timeout=3)
                    break
                except FutureTimeout:
                    elapsed = time.time() - start_time
                    if elapsed - last_keepalive >= keepalive_every_seconds:
                        send_progress("AI ƒëang ph√¢n t√≠ch...", step="ping")
                        last_keepalive = elapsed
                    if elapsed >= ai_timeout_seconds:
                        ai_result_data = None
                        ai_available = False
                        future.cancel()
                        break
            executor.shutdown(wait=False, cancel_futures=True)

            if ai_result_data:
                ai_score = ai_result_data.get('risk_score', 0) or 0
                ai_explanation = ai_result_data.get('explanation') or ''
                ai_notice = ai_result_data.get('ai_notice')
                if ai_notice:
                    send_progress(ai_notice, step="ai_retry_notice")
                send_progress("AI ƒë√£ ho√†n t·∫•t ph√¢n t√≠ch.", step="ai_done")
            else:
                ai_available = False
                send_progress("AI kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng ph√¢n t√≠ch quy t·∫Øc.", step="ai_warning")
        except Exception as exc:
            ai_available = False
            logger.warning(f"[MsgScan] AI error: {exc}")
            send_progress("AI g·∫∑p l·ªói, s·ª≠ d·ª•ng ph√¢n t√≠ch quy t·∫Øc.", step="ai_warning")

        # 5. Final scoring
        send_progress("ƒêang t·ªïng h·ª£p k·∫øt qu·∫£...", step="finalizing")
        rule_score = min(100, len(patterns_found) * 15 + max_domain_score)
        final_score = max(ai_score, max_domain_score) if ai_available else rule_score
        final_score = max(0, min(100, final_score))

        if final_score >= 70:
            level = 'RED'
        elif final_score >= 40:
            level = 'YELLOW'
        elif final_score >= 10:
            level = 'GREEN'
        else:
            level = 'SAFE'

        scam_type = 'other'
        if any('c√¥ng an' in p for p in patterns_found):
            scam_type = 'police_impersonation'
        elif any('OTP' in p for p in patterns_found):
            scam_type = 'otp_steal'
        elif any('chuy·ªÉn kho·∫£n' in p for p in patterns_found):
            scam_type = 'investment_scam'
        elif domain_risks:
            scam_type = 'phishing'

        if ai_available and ai_explanation:
            explanation = ai_explanation
        elif patterns_found or domain_risks:
            explanation = f'Ph√°t hi·ªán {len(patterns_found + domain_risks)} d·∫•u hi·ªáu ƒë√°ng ng·ªù (ph√¢n t√≠ch quy t·∫Øc).'
        else:
            explanation = 'Kh√¥ng ph√°t hi·ªán d·∫•u hi·ªáu l·ª´a ƒë·∫£o r√µ r√†ng.'

        result = {
            'risk_score': final_score,
            'risk_level': level,
            'scam_type': scam_type,
            'patterns_found': list(set(patterns_found + domain_risks)),
            'explanation': explanation,
            'ai_insight': ai_explanation,
            'ai_available': ai_available,
            'ocr_text': '\n'.join(combined_ocr_text) if combined_ocr_text else '',
            'annotated_images': annotated_images,
            'annotated_image': annotated_images[0] if annotated_images else '',
            'web_sources': (ai_result_data or {}).get('web_sources', []),
            'web_context': (ai_result_data or {}).get('web_context', ''),
        }

        scan_event.result_json = {k: v for k, v in result.items() if k not in ('annotated_images', 'annotated_image')}
        scan_event.risk_score = final_score
        scan_event.risk_level = level
        scan_event.status = ScanStatus.COMPLETED
        scan_event.save()

        send_progress("Ho√†n t·∫•t ph√¢n t√≠ch tin nh·∫Øn!", step="completed", data=result)
        _notify_scan_complete(scan_event, 'Qu√©t tin nh·∫Øn ho√†n t·∫•t')
        return result

    except ScanEvent.DoesNotExist:
        logger.error(f"[MsgScan] ScanEvent #{scan_event_id} not found.")
    except Exception as e:
        logger.error(f"[MsgScan] perform_message_scan_task failed: {e}", exc_info=True)
        try:
            send_progress(f"L·ªói h·ªá th·ªëng: {str(e)[:100]}", step="error")
            scan_event = ScanEvent.objects.get(id=scan_event_id)
            scan_event.status = ScanStatus.FAILED
            scan_event.save()
        except:
            pass


@shared_task(name='core.perform_email_deep_scan')
def perform_email_deep_scan(scan_event_id: int, email_data: dict):
    """
    Async deep analysis for emails using weighted EML signals + AI enrichment.
    """
    from api.core.models import ScanEvent, RiskLevel, ScanStatus
    from api.utils.ollama_client import analyze_text_for_scam
    from api.utils.email_utils import compute_eml_weighted_risk
    from asgiref.sync import async_to_sync
    from channels.layers import get_channel_layer

    channel_layer = get_channel_layer()
    group_name = f'scan_{scan_event_id}'

    def send_progress(message, step='processing', data=None):
        if not channel_layer:
            return
        try:
            payload = {
                'type': 'scan_progress',
                'message': message,
                'status': 'processing',
                'step': step,
            }
            if data is not None:
                payload['data'] = data
            async_to_sync(channel_layer.group_send)(group_name, payload)
        except Exception as e:
            logger.warning(f"[EmailScan] Event {scan_event_id}: Progress send failed: {e}")
    
    logger.info(f"[EmailScan] Event {scan_event_id}: Starting deep scan")
    
    try:
        scan_event = ScanEvent.objects.get(id=scan_event_id)
        current_score = int(scan_event.risk_score or 0)
        analysis_type = (email_data.get('analysis_type') or 'text').lower()
        is_basic_mode = analysis_type != 'eml'

        send_progress("B·∫Øt ƒë·∫ßu ph√¢n t√≠ch chuy√™n s√¢u email...", step="init")
        send_progress(f"Ch·∫ø ƒë·ªô: {'Ph√¢n t√≠ch c∆° b·∫£n (text)' if is_basic_mode else 'Ph√¢n t√≠ch file .eml ƒë·∫ßy ƒë·ªß'}", step="init")
        if email_data.get('from'):
            send_progress(f"Ng∆∞·ªùi g·ª≠i: {email_data['from']}", step="init")
        if email_data.get('subject'):
            send_progress(f"Ti√™u ƒë·ªÅ: {email_data['subject'][:120]}", step="init")

        # Use result_json instead of details
        result_json = scan_event.result_json or {}
        security_checks = list(result_json.get('security_checks', []))
        details = list(security_checks)

        # 1) Scoring engine
        send_progress("ƒêang t√≠nh ƒëi·ªÉm r·ªßi ro email...", step="scoring")
        if is_basic_mode:
            eml_res = {
                'risk_score': current_score,
                'components': {},
                'auth_results': {},
                'details': [
                    'Ch·∫ø ƒë·ªô ph√¢n t√≠ch c∆° b·∫£n: ch·ªâ d√πng email v√† n·ªôi dung do ng∆∞·ªùi d√πng cung c·∫•p (kh√¥ng c√≥ header/metadata .eml).'
                ],
            }
            eml_score = current_score
            logger.info(f"[EmailScan] Event {scan_event_id}: BASIC mode score={eml_score}")
            send_progress(f"ƒêi·ªÉm r·ªßi ro c∆° b·∫£n: {eml_score}/100", step="scoring")
        else:
            eml_res = compute_eml_weighted_risk(email_data)
            eml_score = int(eml_res.get('risk_score', 0) or 0)
            details.extend(eml_res.get('details', []))
            logger.info(f"[EmailScan] Event {scan_event_id}: EML weighted score={eml_score}")
            send_progress(f"ƒêi·ªÉm EML weighted: {eml_score}/100", step="scoring")
            # Show auth results
            auth = eml_res.get('auth_results', {})
            if auth:
                auth_parts = []
                for k, v in auth.items():
                    status = '‚úÖ' if v in ('pass', True) else '‚ùå'
                    auth_parts.append(f"{k.upper()}: {status} {v}")
                if auth_parts:
                    send_progress(f"X√°c th·ª±c: {' | '.join(auth_parts)}", step="auth_check")
            # Show component breakdown
            components = eml_res.get('components', {})
            if components:
                comp_parts = [f"{k}: {v}" for k, v in components.items() if v]
                if comp_parts:
                    send_progress(f"Th√†nh ph·∫ßn: {', '.join(comp_parts[:6])}", step="scoring_detail")

        # 2) Content Analysis (LLM)
        body_text = email_data.get('body', '')
        subject = email_data.get('subject', '')
        sender = email_data.get('from', '')
        analysis = None
        ai_bonus = 0

        if body_text and len(body_text) > 20:
            send_progress(f"ƒêang ph√¢n t√≠ch n·ªôi dung email b·∫±ng AI ({len(body_text)} k√Ω t·ª±)...", step="ai_analysis")
            # Show detected URLs
            detected_urls = email_data.get('urls', [])
            if detected_urls:
                send_progress(f"Ph√°t hi·ªán {len(detected_urls)} URL trong email", step="url_detect")
            # Show detected attachments
            detected_attachments = email_data.get('attachments', [])
            if detected_attachments:
                send_progress(f"Ph√°t hi·ªán {len(detected_attachments)} file ƒë√≠nh k√®m", step="attachment_detect")
            security_lines = '\n'.join(f"- {item}" for item in security_checks[:20]) if security_checks else '- Kh√¥ng c√≥ d·ªØ li·ªáu ki·ªÉm tra b·∫£o m·∫≠t domain.'
            security_context = (
                "K·∫øt qu·∫£ ki·ªÉm tra b·∫£o m·∫≠t domain (h·ªá th·ªëng backend ƒë√£ x√°c th·ª±c, c√≥ th·ªÉ d√πng tr·ª±c ti·∫øp):\n"
                f"{security_lines}"
            )

            # Build a Vietnamese-context enriched text block for the AI
            if is_basic_mode:
                analysis_input = (
                    "CH·∫æ ƒê·ªò PH√ÇN T√çCH C∆† B·∫¢N (TEXT-ONLY):\n"
                    "- Ch·ªâ s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi.\n"
                    "- ƒê∆Ø·ª¢C PH√âP d√πng k·∫øt qu·∫£ SPF/DKIM/DMARC/MX t·ª´ ph·∫ßn 'K·∫øt qu·∫£ ki·ªÉm tra b·∫£o m·∫≠t domain' b√™n d∆∞·ªõi.\n"
                    "- KH√îNG suy di·ªÖn ho·∫∑c k·∫øt lu·∫≠n v·ªÅ header email/metadata SMTP, "
                    "ƒë·ªô uy t√≠n domain qua ScamAdviser/Trustpilot/Sitejabber n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu x√°c th·ª±c t∆∞∆°ng ·ª©ng.\n"
                    "- Kh√¥ng ƒë∆∞a nh·∫≠n ƒë·ªãnh v·ªÅ WHOIS/Tranco/ESP n·∫øu kh√¥ng c√≥ b·∫±ng ch·ª©ng tr·ª±c ti·∫øp trong input.\n\n"
                    f"ƒê·ªãa ch·ªâ g·ª≠i: {sender}\n"
                    f"Ti√™u ƒë·ªÅ: {subject}\n"
                    f"{security_context}\n\n"
                    f"N·ªôi dung:\n{body_text[:4000]}"
                )
            else:
                eml_components = json.dumps(eml_res.get('components', {}), ensure_ascii=False)
                eml_auth = json.dumps(eml_res.get('auth_results', {}), ensure_ascii=False)
                eml_detail_lines = '\n'.join(f"- {item}" for item in (eml_res.get('details') or [])[:20])
                analysis_input = (
                    "CH·∫æ ƒê·ªò PH√ÇN T√çCH .EML (FULL):\n"
                    f"ƒê·ªãa ch·ªâ g·ª≠i: {sender}\n"
                    f"Ti√™u ƒë·ªÅ: {subject}\n"
                    f"{security_context}\n\n"
                    "K·∫øt qu·∫£ ph√¢n t√≠ch EML weighted (h·ªá th·ªëng backend ƒë√£ t√≠nh s·∫µn, c√≥ th·ªÉ d√πng tr·ª±c ti·∫øp):\n"
                    f"- EML Weighted Risk Score: {eml_score}/100\n"
                    f"- Components: {eml_components}\n"
                    f"- Auth Results: {eml_auth}\n"
                    f"- EML Details:\n{eml_detail_lines if eml_detail_lines else '- Kh√¥ng c√≥ chi ti·∫øt EML b·ªï sung.'}\n\n"
                    f"N·ªôi dung:\n{body_text[:4000]}"
                )
            logger.info(f"[EmailScan] Event {scan_event_id}: Analyzing body text ({len(body_text)} chars)")
            if not is_basic_mode:
                send_progress("ƒêang tra c·ª©u c∆° s·ªü d·ªØ li·ªáu l·ª´a ƒë·∫£o + t√¨m ki·∫øm web...", step="web_search")
            analysis = analyze_text_for_scam(analysis_input, use_web_search=(not is_basic_mode))
            logger.info(f"[EmailScan] Event {scan_event_id}: AI Result: {analysis}")
            ai_notice = analysis.get('ai_notice') if analysis else None
            if ai_notice:
                send_progress(ai_notice, step="ai_retry_notice")
            # Show web sources found
            if analysis and analysis.get('web_sources'):
                src_list = ', '.join(str(s) for s in analysis['web_sources'][:5])
                send_progress(f"Ngu·ªìn t√¨nh b√°o: {src_list}", step="web_sources_ok")
            send_progress("AI ƒë√£ ho√†n t·∫•t ph√¢n t√≠ch n·ªôi dung.", step="ai_done")
            
            if analysis:
                if analysis.get('is_scam'):
                    ai_bonus = min(35, int((analysis.get('risk_score', 0) or 0) * 0.45))
                else:
                    ai_bonus = -10 if int(analysis.get('risk_score', 0) or 0) <= 20 else 0
                details.extend(analysis.get('indicators', []))
                ai_explain = analysis.get('explanation', '')
                if ai_explain:
                    details.append(f"AI nh·∫≠n ƒë·ªãnh: {ai_explain}")
        else:
            logger.info(f"[EmailScan] Event {scan_event_id}: Body text too short, skipping AI")
            send_progress("N·ªôi dung email qu√° ng·∫Øn, b·ªè qua ph√¢n t√≠ch AI.", step="ai_skipped")

        # 3) Finalize - combine preliminary score, weighted EML score, and AI adjustment
        send_progress("ƒêang t·ªïng h·ª£p k·∫øt qu·∫£ ph√¢n t√≠ch...", step="finalizing")
        combined_base = max(current_score, eml_score)
        final_score = max(0, min(100, combined_base + ai_bonus))
        logger.info(f"[EmailScan] Event {scan_event_id}: Final Score: {final_score}")
        send_progress(f"ƒêi·ªÉm t·ªïng k·∫øt: {final_score}/100 (base={combined_base}, AI adj={ai_bonus:+d})", step="score_final")

        # Remove duplicated analysis lines (e.g., DMARC/SPF repeated by multiple analyzers)
        deduped_details = []
        seen_detail_keys = set()
        for item in details:
            text = str(item).strip()
            if not text:
                continue
            normalized_key = re.sub(r'\s+', ' ', text).lower()
            if normalized_key in seen_detail_keys:
                continue
            seen_detail_keys.add(normalized_key)
            deduped_details.append(text)
        
        # Determine Risk Level
        if final_score >= 80:
            scan_event.risk_level = RiskLevel.RED
        elif final_score >= 40:
            scan_event.risk_level = RiskLevel.YELLOW
        else:
            scan_event.risk_level = RiskLevel.GREEN
            
        scan_event.risk_score = final_score
        result_json['analysis_result'] = deduped_details[:40]
        result_json['eml_weighted_score'] = eml_score
        result_json['eml_score_components'] = eml_res.get('components', {})
        result_json['email_auth_results'] = eml_res.get('auth_results', {})
        result_json['analysis_type'] = analysis_type
        result_json['detected_urls'] = email_data.get('urls', [])
        result_json['detected_attachments'] = email_data.get('attachments', [])
        # Include grounded intelligence from AI analysis
        if analysis:
            result_json['web_sources'] = analysis.get('web_sources', [])
            result_json['web_context'] = analysis.get('web_context', '')
            result_json['searched_urls'] = analysis.get('searched_urls', [])
        scan_event.result_json = result_json
        scan_event.status = ScanStatus.COMPLETED
        scan_event.save()
        
        send_progress("Ho√†n t·∫•t ph√¢n t√≠ch chuy√™n s√¢u email!", step="completed", data={
            'risk_score': final_score,
            'risk_level': str(scan_event.risk_level),
            'result': result_json
        })
        logger.info(f"[EmailScan] Event {scan_event_id}: Completed Successfully. Level: {scan_event.risk_level}")
        _notify_scan_complete(scan_event, 'Qu√©t email chuy√™n s√¢u ho√†n t·∫•t')
        
    except ScanEvent.DoesNotExist:
        logger.error(f"[EmailScan] ScanEvent #{scan_event_id} not found.")
    except Exception as e:
        logger.error(f"[EmailScan] perform_email_deep_scan failed: {e}", exc_info=True)
        try:
                        scan_event = ScanEvent.objects.get(id=scan_event_id)
                        scan_event.status = ScanStatus.FAILED
                        scan_event.save()
        except: pass


@shared_task(name='core.perform_file_scan_task', bind=True)
def perform_file_scan_task(self, scan_event_id, file_path):
    """
    Background task to scan an uploaded file using VirusTotal.
    Called from ScanFileView after saving the file to a temp location.
    """
    import os
    import time
    from api.core.models import ScanEvent, ScanStatus, RiskLevel
    from api.utils.vt_client import VTClient

    start_time = time.time()

    try:
        scan_event = ScanEvent.objects.get(id=scan_event_id)
        scan_event.status = ScanStatus.PROCESSING
        scan_event.job_id = self.request.id
        scan_event.save()

        file_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
        logger.info(f"[FileScan] Event {scan_event_id}: START ‚Äî file='{file_name}', size={file_size} bytes")

        # Scan with VirusTotal (timeout 5 min)
        logger.info(f"[FileScan] Event {scan_event_id}: Uploading to VirusTotal...")
        vt = VTClient()
        vt_result = vt.scan_file(file_path, timeout=300)
        elapsed = time.time() - start_time
        logger.info(f"[FileScan] Event {scan_event_id}: VT scan returned after {elapsed:.1f}s ‚Äî result={'OK' if vt_result else 'None'}")

        if vt_result:
            malicious = vt_result.get('malicious', 0)
            suspicious = vt_result.get('suspicious', 0)
            harmless = vt_result.get('harmless', 0)
            undetected = vt_result.get('undetected', 0)
            total = vt_result.get('total', 0) or (malicious + suspicious + harmless + undetected)

            # Calculate risk score (0-100)
            if total > 0:
                risk_score = min(100, int(((malicious * 1.0 + suspicious * 0.5) / total) * 100))
            else:
                risk_score = 0

            # Determine risk level
            if malicious >= 5 or risk_score >= 70:
                risk_level = RiskLevel.RED
            elif malicious >= 1 or suspicious >= 3 or risk_score >= 30:
                risk_level = RiskLevel.YELLOW
            else:
                risk_level = RiskLevel.GREEN

            details = []
            if malicious > 0:
                details.append(f"‚ö†Ô∏è {malicious}/{total} engine ph√°t hi·ªán m√£ ƒë·ªôc.")
            if suspicious > 0:
                details.append(f"üîç {suspicious}/{total} engine ƒë√°nh gi√° ƒë√°ng ng·ªù.")
            if harmless > 0:
                details.append(f"‚úÖ {harmless}/{total} engine ƒë√°nh gi√° an to√†n.")
            if undetected > 0:
                details.append(f"‚ùî {undetected}/{total} engine kh√¥ng ph√°t hi·ªán g√¨.")

            scan_event.result_json = {
                'file_name': file_name,
                'file_size': file_size,
                'malicious': malicious,
                'suspicious': suspicious,
                'harmless': harmless,
                'undetected': undetected,
                'total': total,
                'risk_score': risk_score,
                'risk_level': risk_level,
                'details': details,
                'summary': f"K·∫øt qu·∫£ qu√©t: {malicious} m√£ ƒë·ªôc, {suspicious} ƒë√°ng ng·ªù tr√™n t·ªïng {total} engine.",
            }
            scan_event.risk_score = risk_score
            scan_event.risk_level = risk_level
        else:
            # VT scan failed or returned None
            logger.warning(f"[FileScan] Event {scan_event_id}: VirusTotal returned no result after {elapsed:.1f}s")
            scan_event.result_json = {
                'file_name': file_name,
                'file_size': file_size,
                'risk_score': 0,
                'risk_level': RiskLevel.GREEN,
                'details': ['Kh√¥ng th·ªÉ qu√©t file qua VirusTotal. Vui l√≤ng th·ª≠ l·∫°i sau.'],
                'summary': 'Qu√©t file th·∫•t b·∫°i. Kh√¥ng c√≥ k·∫øt qu·∫£ t·ª´ VirusTotal.',
            }
            scan_event.risk_score = 0
            scan_event.risk_level = RiskLevel.GREEN

        scan_event.status = ScanStatus.COMPLETED
        scan_event.save()
        total_time = time.time() - start_time
        logger.info(f"[FileScan] Event {scan_event_id}: COMPLETED in {total_time:.1f}s. Risk: {scan_event.risk_level}, Score: {scan_event.risk_score}")

        # Send push notification to user
        _notify_scan_complete(scan_event, f"Qu√©t file '{file_name}' ho√†n t·∫•t")

    except ScanEvent.DoesNotExist:
        logger.error(f"[FileScan] ScanEvent #{scan_event_id} not found.")
    except Exception as e:
        logger.error(f"[FileScan] perform_file_scan_task failed after {time.time() - start_time:.1f}s: {e}", exc_info=True)
        try:
            scan_event = ScanEvent.objects.get(id=scan_event_id)
            scan_event.status = ScanStatus.FAILED
            scan_event.result_json = {'error': str(e)}
            scan_event.save()
        except Exception:
            pass
    finally:
        # Cleanup temp file
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"[FileScan] Cleaned up temp file: {file_path}")
        except Exception as cleanup_err:
            logger.warning(f"[FileScan] Failed to clean up temp file: {cleanup_err}")


# ---------------------------------------------------------------------------
# Magic Create Lesson ‚Äî runs AI generation in Celery with WS progress
# ---------------------------------------------------------------------------

def _send_task_progress(task_id, status, message, step=None, data=None):
    """Helper to send progress over WebSocket for a generic task."""
    from channels.layers import get_channel_layer
    from asgiref.sync import async_to_sync
    channel_layer = get_channel_layer()
    if channel_layer:
        async_to_sync(channel_layer.group_send)(f'task_{task_id}', {
            'type': 'task_progress',
            'status': status,
            'message': message,
            'step': step,
            'data': data,
        })


@shared_task(name='core.magic_create_lesson_task', bind=True, max_retries=1)
def magic_create_lesson_task(self, task_id, raw_text):
    """
    AI-powered lesson generation via Celery.
    Generates: lesson content (HTML), 5 quizzes, rich scenario (10+ steps with AI roleplay).
    Sends PARTIAL progress updates via WebSocket at each stage.
    """
    import re as _re
    import time as _time_mod
    import markdown as md
    from api.utils.ollama_client import generate_response, stream_response

    def _md_to_html(text):
        """Convert markdown to clean HTML for CKEditor."""
        if not text:
            return ''
        # If it already looks like HTML, return as-is
        if '<h2' in text or '<h3' in text or '<p>' in text:
            return text
        try:
            html = md.markdown(text, extensions=['extra', 'nl2br', 'sane_lists'])
        except Exception:
            # Fallback manual conversion
            html = text
            html = _re.sub(r'^### (.+)$', r'<h3>\1</h3>', html, flags=_re.MULTILINE)
            html = _re.sub(r'^## (.+)$', r'<h2>\1</h2>', html, flags=_re.MULTILINE)
            html = _re.sub(r'^# (.+)$', r'<h1>\1</h1>', html, flags=_re.MULTILINE)
            html = _re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', html)
            html = _re.sub(r'\*(.+?)\*', r'<em>\1</em>', html)
            html = _re.sub(r'^- (.+)$', r'<li>\1</li>', html, flags=_re.MULTILINE)
            html = _re.sub(r'(<li>.*?</li>\n?)+', r'<ul>\g<0></ul>', html)
            html = _re.sub(r'\n{2,}', '</p><p>', html)
            html = '<p>' + html + '</p>'
            html = html.replace('<p></p>', '')
        return html

    # ‚îÄ‚îÄ‚îÄ‚îÄ STAGE 1: Analyze ‚îÄ‚îÄ‚îÄ‚îÄ
    _send_task_progress(task_id, 'processing', 'ƒêang ph√¢n t√≠ch vƒÉn b·∫£n ngu·ªìn...', step=1)

    # ‚îÄ‚îÄ‚îÄ‚îÄ STAGE 2: Generate lesson content (streamed) ‚îÄ‚îÄ‚îÄ‚îÄ
    _send_task_progress(task_id, 'processing', 'AI ƒëang ph√¢n t√≠ch v√† t·∫°o n·ªôi dung...', step=2)

    TITLE_SCHEMA = {
        "type": "object",
        "properties": {
            "title": {"type": "string"},
            "category": {"type": "string", "enum": ["news", "guide", "alert", "story"]}
        },
        "required": ["title", "category"]
    }

    try:
        # ‚îÄ‚îÄ Step 2a: Extract title + category via structured JSON (fast) ‚îÄ‚îÄ
        title_prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† ƒë·∫∑t ti√™u ƒë·ªÅ b√†i h·ªçc gi√°o d·ª•c an ninh m·∫°ng.
---
{raw_text[:1500]}
---
Tr·∫£ v·ªÅ JSON: {{"title": "Ti√™u ƒë·ªÅ h·∫•p d·∫´n, gi√°o d·ª•c", "category": "news|guide|alert|story"}}"""

        logger.info(f"[MagicCreate] Task {task_id}: Calling generate_response for title (text_len={len(raw_text)})")
        title_resp = generate_response(
            prompt=title_prompt,
            system_prompt="Tr·∫£ v·ªÅ JSON thu·∫ßn, kh√¥ng markdown code block.",
            format_schema=TITLE_SCHEMA,
            max_tokens=6000,  # Extra room for thinking tokens
        )
        title_data = _parse_json_safe(title_resp)
        if not title_data or not title_data.get('title'):
            # Fallback: use first 80 chars of raw text as title
            title_data = {'title': raw_text[:80].strip().split('\n')[0], 'category': 'guide'}
            logger.warning(f"[MagicCreate] Task {task_id}: Title extraction failed, using fallback")

        logger.info(f"[MagicCreate] Task {task_id}: Title: {title_data['title']}")

        # Send title immediately
        _send_task_progress(task_id, 'processing', 'AI ƒëang vi·∫øt n·ªôi dung b√†i h·ªçc...', step=2, data={
            'title': title_data['title'],
            'category': title_data.get('category', 'guide'),
        })

        # ‚îÄ‚îÄ Step 2b: Generate lesson content (non-streaming for reliability) ‚îÄ‚îÄ
        content_prompt = f"""VƒÉn b·∫£n ngu·ªìn:
---
{raw_text}
---

Vi·∫øt b√†i h·ªçc gi√°o d·ª•c v·ªÅ an ninh m·∫°ng b·∫±ng Markdown t·ª´ vƒÉn b·∫£n tr√™n.

Y√äU C·∫¶U:
- Bao g·ªìm c√°c ph·∫ßn: ## T·ªïng quan, ## Th·ªß ƒëo·∫°n l·ª´a ƒë·∫£o, ## D·∫•u hi·ªáu nh·∫≠n bi·∫øt, ## C√°ch ph√≤ng tr√°nh, ## K·∫øt lu·∫≠n
- M·ªói ph·∫ßn ph·∫£i c√≥ √≠t nh·∫•t 3 bullet points ho·∫∑c 2 ƒëo·∫°n vƒÉn
- D√πng v√≠ d·ª• th·ª±c t·∫ø minh h·ªça, ng√¥n ng·ªØ d·ªÖ hi·ªÉu cho m·ªçi l·ª©a tu·ªïi
- T·ªïng √≠t nh·∫•t 500 t·ª´ tr·ªü l√™n
- Vi·∫øt Markdown thu·∫ßn (## heading, ### sub, **bold**, - bullet, > blockquote)
- KH√îNG b·ªçc trong code block, KH√îNG tr·∫£ v·ªÅ JSON"""

        logger.info(f"[MagicCreate] Task {task_id}: Generating content (non-streaming), prompt_len={len(content_prompt)}")
        _send_task_progress(task_id, 'processing', 'AI ƒëang vi·∫øt n·ªôi dung b√†i h·ªçc...', step=2)

        _accumulated_md = ''
        try:
            _accumulated_md = generate_response(
                prompt=content_prompt,
                system_prompt="B·∫°n l√† chuy√™n gia an ninh m·∫°ng Vi·ªát Nam. Vi·∫øt b√†i h·ªçc Markdown chi ti·∫øt, d·ªÖ hi·ªÉu. Ch·ªâ vi·∫øt n·ªôi dung Markdown, kh√¥ng JSON, kh√¥ng code block.",
                max_tokens=12000,   # High limit: ~3k thinking + ~9k content tokens for 500+ word lesson
                tools=[],           # Disable tools ‚Äî model should write content, not search
                skip_filter=True,   # Don't strip CJK from content
            ) or ''
            logger.info(f"[MagicCreate] Task {task_id}: generate_response returned {len(_accumulated_md)} chars")
            if _accumulated_md:
                logger.info(f"[MagicCreate] Task {task_id}: Content preview (first 300): {_accumulated_md[:300]!r}")
                logger.info(f"[MagicCreate] Task {task_id}: Content preview (last 200): {_accumulated_md[-200:]!r}")
        except Exception as gen_err:
            logger.error(f"[MagicCreate] Task {task_id}: Content generation error: {gen_err}", exc_info=True)

        # Final content
        _content_html = _md_to_html(_accumulated_md) if _accumulated_md.strip() else ''
        logger.info(f"[MagicCreate] Task {task_id}: md_len={len(_accumulated_md)}, html_len={len(_content_html)}")
        if not _content_html:
            logger.error(f"[MagicCreate] Task {task_id}: Content generation produced empty result")
            _send_task_progress(task_id, 'error', 'AI kh√¥ng t·∫°o ƒë∆∞·ª£c n·ªôi dung b√†i h·ªçc. Th·ª≠ l·∫°i.')
            return

        content_data = {
            'title': title_data['title'],
            'category': title_data.get('category', 'guide'),
            'content': _content_html,
        }

        # Send complete content at once
        _send_task_progress(task_id, 'processing', 'N·ªôi dung b√†i h·ªçc ƒë√£ t·∫°o xong!', step=2, data={
            'title': content_data['title'],
            'category': content_data.get('category', 'guide'),
            'content': content_data['content']
        })
        logger.info(f"[MagicCreate] Task {task_id}: Content generated - {content_data['title']} "
                    f"({len(_accumulated_md)} chars md, {len(_content_html)} chars html)")

    except Exception as e:
        logger.error(f"[MagicCreate] Task {task_id} content failed: {e}", exc_info=True)
        _send_task_progress(task_id, 'error', f'L·ªói t·∫°o n·ªôi dung: {str(e)}')
        return

    # ‚îÄ‚îÄ‚îÄ‚îÄ STAGE 3: Generate 5 quizzes ‚îÄ‚îÄ‚îÄ‚îÄ
    _send_task_progress(task_id, 'processing', 'AI ƒëang t·∫°o 5 c√¢u h·ªèi quiz...', step=3)

    quiz_prompt = f"""D·ª±a tr√™n b√†i h·ªçc "{content_data['title']}" v·ªõi n·ªôi dung sau:
---
{raw_text[:2000]}
---

H√£y t·∫°o CH√çNH X√ÅC 5 c√¢u h·ªèi tr·∫Øc nghi·ªám ki·ªÉm tra ki·∫øn th·ª©c AN NINH M·∫†NG.

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. M·ªói c√¢u h·ªèi PH·∫¢I li√™n quan TR·ª∞C TI·∫æP ƒë·∫øn n·ªôi dung b√†i h·ªçc ·ªü tr√™n
2. TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c t·∫°o c√¢u h·ªèi chung chung ki·ªÉu "C√¢u h·ªèi b·ªï sung #1"
3. TUY·ªÜT ƒê·ªêI KH√îNG d√πng "ƒê√°p √°n A", "ƒê√°p √°n B", "ƒê√°p √°n C", "ƒê√°p √°n D" l√†m n·ªôi dung ƒë√°p √°n
4. M·ªói ƒë√°p √°n ph·∫£i l√† m·ªôt C√ÇU TR·∫¢ L·ªúI C·ª§ TH·ªÇ, c√≥ nghƒ©a, ph√¢n bi·ªát r√µ r√†ng
5. M·ªói c√¢u c√≥ question_type: "single_choice" | "multiple_choice" | "true_false"
6. correct_answer PH·∫¢I tr√πng CH√çNH X√ÅC v·ªõi 1 option (ƒë·ªÉ t∆∞∆°ng th√≠ch h·ªá th·ªëng c≈©)
7. correct_answers l√† M·∫¢NG ch·ª©a t·∫•t c·∫£ ƒë√°p √°n ƒë√∫ng (v·ªõi single th√¨ m·∫£ng c√≥ 1 ph·∫ßn t·ª≠)
8. explanation ph·∫£i gi·∫£i th√≠ch CHI TI·∫æT (√≠t nh·∫•t 30 t·ª´) t·∫°i sao ƒë√°p √°n ƒë√≥ ƒë√∫ng

5 c√¢u h·ªèi ph·∫£i ƒêA D·∫†NG v·ªÅ ch·ªß ƒë·ªÅ:
1. Nh·∫≠n bi·∫øt d·∫•u hi·ªáu l·ª´a ƒë·∫£o c·ª• th·ªÉ trong b√†i
2. C√°ch x·ª≠ l√Ω ƒë√∫ng khi g·∫∑p t√¨nh hu·ªëng t∆∞∆°ng t·ª±
3. Ki·∫øn th·ª©c an ninh m·∫°ng li√™n quan ƒë·∫øn b√†i h·ªçc
4. ƒê√°nh gi√° m·ª©c ƒë·ªô r·ªßi ro c·ªßa h√†nh vi c·ª• th·ªÉ
5. Bi·ªán ph√°p ph√≤ng tr√°nh th·ª±c t·∫ø ng∆∞·ªùi d√πng c√≥ th·ªÉ √°p d·ª•ng

V√ç D·ª§ FORMAT ƒê√öNG:
{{{{
  "question": "Khi nh·∫≠n ƒë∆∞·ª£c cu·ªôc g·ªçi t·ª± x∆∞ng l√† c√¥ng an y√™u c·∫ßu chuy·ªÉn ti·ªÅn, b·∫°n n√™n l√†m g√¨?",
    "question_type": "single_choice",
  "options": ["Chuy·ªÉn ti·ªÅn ngay theo y√™u c·∫ßu", "G√°c m√°y v√† g·ªçi tr·ª±c ti·∫øp ƒë·∫øn c√¥ng an ƒë·ªãa ph∆∞∆°ng ƒë·ªÉ x√°c minh", "Cung c·∫•p th√¥ng tin t√†i kho·∫£n ƒë·ªÉ ki·ªÉm tra", "T·∫£i ·ª©ng d·ª•ng theo link ƒë∆∞·ª£c g·ª≠i"],
  "correct_answer": "G√°c m√°y v√† g·ªçi tr·ª±c ti·∫øp ƒë·∫øn c√¥ng an ƒë·ªãa ph∆∞∆°ng ƒë·ªÉ x√°c minh",
    "correct_answers": ["G√°c m√°y v√† g·ªçi tr·ª±c ti·∫øp ƒë·∫øn c√¥ng an ƒë·ªãa ph∆∞∆°ng ƒë·ªÉ x√°c minh"],
  "explanation": "C√¥ng an th·∫≠t s·∫Ω kh√¥ng bao gi·ªù y√™u c·∫ßu chuy·ªÉn ti·ªÅn qua ƒëi·ªán tho·∫°i. C√°ch an to√†n nh·∫•t l√† g√°c m√°y v√† t·ª± m√¨nh li√™n h·ªá tr·ª±c ti·∫øp c∆° quan c√¥ng an qua s·ªë ƒëi·ªán tho·∫°i ch√≠nh th·ª©c."
}}}}

Tr·∫£ v·ªÅ JSON: {{"quizzes": [5 c√¢u h·ªèi theo format tr√™n]}}"""

    QUIZ_SCHEMA = {
        "type": "object",
        "properties": {
            "quizzes": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "question": {"type": "string"},
                        "question_type": {"type": "string", "enum": ["single_choice", "multiple_choice", "true_false"]},
                        "options": {"type": "array", "items": {"type": "string"}, "minItems": 4, "maxItems": 4},
                        "correct_answer": {"type": "string"},
                        "correct_answers": {"type": "array", "items": {"type": "string"}, "minItems": 1},
                        "explanation": {"type": "string"}
                    },
                    "required": ["question", "question_type", "options", "correct_answer", "correct_answers", "explanation"]
                },
                "minItems": 5
            }
        },
        "required": ["quizzes"]
    }

    max_quiz_retries = 3
    quizzes = []
    for _quiz_attempt in range(max_quiz_retries):
        try:
            logger.info(f"[MagicCreate] Task {task_id}: Quiz attempt {_quiz_attempt+1}/{max_quiz_retries}")
            quiz_resp = generate_response(
                prompt=quiz_prompt,
                system_prompt="B·∫°n l√† chuy√™n gia t·∫°o quiz gi√°o d·ª•c an ninh m·∫°ng. Tr·∫£ v·ªÅ JSON thu·∫ßn, kh√¥ng markdown code block. M·ªói c√¢u h·ªèi ph·∫£i c·ª• th·ªÉ, m·ªói ƒë√°p √°n ph·∫£i c√≥ n·ªôi dung th·ª±c t·∫ø, KH√îNG d√πng ƒê√°p √°n A/B/C/D.",
                format_schema=QUIZ_SCHEMA,
                max_tokens=8000,  # Extra room: ~3k thinking + ~5k for 5 questions JSON
            )
            _preview = repr(quiz_resp[:300]) if quiz_resp else 'None'
            logger.info(f"[MagicCreate] Task {task_id}: quiz_resp type={type(quiz_resp).__name__}, "
                        f"len={len(quiz_resp) if quiz_resp else 0}, preview={_preview}")
            quiz_data = _parse_json_safe(quiz_resp)
            logger.info(f"[MagicCreate] Task {task_id}: parsed quiz_data keys={list(quiz_data.keys()) if quiz_data else 'None'}")
            raw_quizzes = quiz_data.get('quizzes', []) if quiz_data else []

            # Validate each quiz ‚Äî reject placeholders
            valid_quizzes = []
            for q in raw_quizzes:
                if not (q.get('question') and q.get('options') and len(q['options']) >= 4):
                    continue
                # Reject placeholder quizzes
                q_lower = q['question'].lower()
                if 'b·ªï sung' in q_lower or 'c√¢u h·ªèi #' in q_lower:
                    logger.warning(f"[MagicCreate] Rejected placeholder quiz: {q['question'][:80]}")
                    continue
                # Reject generic options
                opts_lower = [o.lower().strip() for o in q['options']]
                if opts_lower == ['ƒë√°p √°n a', 'ƒë√°p √°n b', 'ƒë√°p √°n c', 'ƒë√°p √°n d']:
                    logger.warning(f"[MagicCreate] Rejected generic-options quiz: {q['question'][:80]}")
                    continue
                question_type = q.get('question_type') or 'single_choice'
                if question_type not in ['single_choice', 'multiple_choice', 'true_false']:
                    question_type = 'single_choice'
                q['question_type'] = question_type

                if question_type == 'true_false':
                    q['options'] = ['ƒê√∫ng', 'Sai', '', '']

                # Ensure correct answers are valid and compatible
                correct_answers = q.get('correct_answers') or []
                if not isinstance(correct_answers, list):
                    correct_answers = []
                valid_answers = [ans for ans in correct_answers if ans in q['options']]

                if q.get('correct_answer') in q['options']:
                    if q.get('correct_answer') not in valid_answers:
                        valid_answers.append(q.get('correct_answer'))

                if not valid_answers:
                    valid_answers = [q['options'][0]]

                if question_type != 'multiple_choice':
                    valid_answers = [valid_answers[0]]

                q['correct_answers'] = valid_answers
                q['correct_answer'] = valid_answers[0]
                if not q.get('explanation'):
                    q['explanation'] = 'Xem l·∫°i n·ªôi dung b√†i h·ªçc ƒë·ªÉ hi·ªÉu r√µ h∆°n.'
                valid_quizzes.append(q)

            if len(valid_quizzes) >= 3:
                quizzes = valid_quizzes[:5]
                logger.info(f"[MagicCreate] Task {task_id}: Quiz attempt {_quiz_attempt+1} OK ‚Äî {len(quizzes)} valid quizzes")
                break
            else:
                logger.warning(f"[MagicCreate] Task {task_id}: Quiz attempt {_quiz_attempt+1} only {len(valid_quizzes)} valid, retrying...")
                if _quiz_attempt < max_quiz_retries - 1:
                    _send_task_progress(task_id, 'processing', f'Quiz ch∆∞a ƒë·ªß ch·∫•t l∆∞·ª£ng ({len(valid_quizzes)}), th·ª≠ l·∫°i...', step=3)
        except Exception as e:
            logger.error(f"[MagicCreate] Task {task_id}: Quiz attempt {_quiz_attempt+1} failed: {e}", exc_info=True)
            if _quiz_attempt < max_quiz_retries - 1:
                _send_task_progress(task_id, 'processing', 'Quiz g·∫∑p l·ªói, th·ª≠ l·∫°i...', step=3)

    if not quizzes:
        logger.error(f"[MagicCreate] Task {task_id}: All quiz attempts failed, continuing without quiz")
        _send_task_progress(task_id, 'processing', 'Kh√¥ng t·∫°o ƒë∆∞·ª£c quiz ch·∫•t l∆∞·ª£ng, ti·∫øp t·ª•c...', step=3, data={'quizzes': []})
    else:
        # Send quizzes progressively one by one
        for _qi, _q in enumerate(quizzes):
            _time_mod.sleep(0.3)
            _send_task_progress(task_id, 'processing', f'Quiz {_qi+1}/{len(quizzes)}', step=3, data={'quiz_item': _q, 'quiz_index': _qi})
        # Send all quizzes as final summary
        _send_task_progress(task_id, 'processing', f'ƒê√£ t·∫°o {len(quizzes)} c√¢u quiz!', step=3, data={'quizzes': quizzes})
        logger.info(f"[MagicCreate] Task {task_id}: {len(quizzes)} quizzes generated")

    # ‚îÄ‚îÄ‚îÄ‚îÄ STAGE 4: Generate rich scenario ‚îÄ‚îÄ‚îÄ‚îÄ
    _send_task_progress(task_id, 'processing', 'AI ƒëang t·∫°o k·ªãch b·∫£n h·ªôi tho·∫°i chi ti·∫øt...', step=4)

    scenario_prompt = f"""D·ª±a tr√™n b√†i h·ªçc "{content_data['title']}" v·ªõi n·ªôi dung:
---
{raw_text[:2000]}
---

H√£y t·∫°o m·ªôt K·ªäCH B·∫¢N H·ªòI THO·∫†I s·ªëng ƒë·ªông gi·ªØa k·∫ª l·ª´a ƒë·∫£o v√† n·∫°n nh√¢n.

Y√äU C·∫¶U K·ªäCH B·∫¢N:
- K·∫ª l·ª´a ƒë·∫£o (scammer) ph·∫£i r·∫•t thuy·∫øt ph·ª•c, d√πng k·ªπ thu·∫≠t t√¢m l√Ω th·ª±c t·∫ø: t·∫°o c·∫£m gi√°c c·∫•p b√°ch, gi·∫£ danh uy t√≠n, ƒëe d·ªça nh·∫π, t·∫°o ni·ªÅm tin gi·∫£
- AI ƒê√ìNG VAI k·∫ª l·ª´a ƒë·∫£o th·∫≠t s·ªëng ƒë·ªông (nh∆∞ng ƒë·ªÉ gi√°o d·ª•c, m·ªói b∆∞·ªõc c√≥ ghi ch√∫ ph√¢n t√≠ch th·ªß ƒëo·∫°n)
- N·∫°n nh√¢n ban ƒë·∫ßu hoang mang, d·∫ßn d·∫ßn nh·∫≠n ra d·∫•u hi·ªáu kh·∫£ nghi
- narrator (ng∆∞·ªùi k·ªÉ) gi·∫£i th√≠ch t√¢m l√Ω, ph√¢n t√≠ch k·ªπ thu·∫≠t
- √çt nh·∫•t 10 b∆∞·ªõc h·ªôi tho·∫°i, bao g·ªìm c√°c giai ƒëo·∫°n:
  ‚Ä¢ Ti·∫øp c·∫≠n (2 b∆∞·ªõc): K·∫ª l·ª´a ƒë·∫£o g·ªçi/nh·∫Øn, x∆∞ng danh
  ‚Ä¢ T·∫°o ni·ªÅm tin (2 b∆∞·ªõc): ƒê·ªçc th√¥ng tin c√° nh√¢n, g√¢y tin t∆∞·ªüng
  ‚Ä¢ G√¢y hoang (2 b∆∞·ªõc): T·∫°o t√¨nh hu·ªëng c·∫•p b√°ch, ƒëe d·ªça
  ‚Ä¢ Y√™u c·∫ßu h√†nh ƒë·ªông (2 b∆∞·ªõc): ƒê√≤i OTP, chuy·ªÉn ti·ªÅn, t·∫£i app
  ‚Ä¢ Nh·∫≠n di·ªán (1 b∆∞·ªõc): N·∫°n nh√¢n ph√°t hi·ªán d·∫•u hi·ªáu l·∫°
  ‚Ä¢ K·∫øt lu·∫≠n (1 b∆∞·ªõc): narrator t·ªïng k·∫øt b√†i h·ªçc

M·ªói step PH·∫¢I C√ì:
- speaker: "scammer" | "victim" | "narrator"
- text: L·ªùi tho·∫°i C·ª§ TH·ªÇ, t·ª± nhi√™n, gi·ªëng th·∫≠t (KH√îNG ƒë·ªÉ tr·ªëng, KH√îNG ƒë·ªÉ placeholder)
- note: Ghi ch√∫ ph√¢n t√≠ch (b·∫Øt bu·ªôc cho m·ªçi b∆∞·ªõc)

Tr·∫£ v·ªÅ JSON:
{{
  "scenario": {{
    "title": "Ti√™u ƒë·ªÅ k·ªãch b·∫£n",
    "description": "M√¥ t·∫£ ng·∫Øn g·ªçn t√¨nh hu·ªëng (2-3 c√¢u)",
    "content_json": {{
      "steps": [
        {{"speaker": "scammer", "text": "L·ªùi tho·∫°i c·ª• th·ªÉ", "note": "Ph√¢n t√≠ch th·ªß ƒëo·∫°n"}}
      ]
    }}
  }}
}}

QUAN TR·ªåNG: B·∫Øt bu·ªôc ph·∫£i t·∫°o CH√çNH X√ÅC t·ª´ 10 ƒë·∫øn 14 step trong m·∫£ng steps. Kh√¥ng ƒë∆∞·ª£c √≠t h∆°n 10 b∆∞·ªõc. M·ªói step ph·∫£i c√≥ n·ªôi dung text d√†i √≠t nh·∫•t 2 c√¢u. N·∫øu t·∫°o √≠t h∆°n 10, k·∫øt qu·∫£ s·∫Ω b·ªã t·ª´ ch·ªëi v√† ph·∫£i l√†m l·∫°i."""

    SCENARIO_SCHEMA = {
        "type": "object",
        "properties": {
            "scenario": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "description": {"type": "string"},
                    "content_json": {
                        "type": "object",
                        "properties": {
                            "steps": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "speaker": {"type": "string", "enum": ["scammer", "victim", "narrator"]},
                                        "text": {"type": "string"},
                                        "note": {"type": "string"}
                                    },
                                    "required": ["speaker", "text", "note"]
                                },
                                "minItems": 10
                            }
                        },
                        "required": ["steps"]
                    }
                },
                "required": ["title", "description", "content_json"]
            }
        },
        "required": ["scenario"]
    }

    try:
        scenario = None
        max_scenario_retries = 3
        for attempt in range(max_scenario_retries):
            logger.info(f"[MagicCreate] Task {task_id}: Calling generate_response for scenario (attempt {attempt+1}/{max_scenario_retries})")
            scenario_resp = generate_response(
                prompt=scenario_prompt,
                system_prompt="B·∫°n l√† nh√† bi√™n k·ªãch an ninh m·∫°ng. T·∫°o k·ªãch b·∫£n s·ªëng ƒë·ªông, gi√°o d·ª•c, c√≥ ph√¢n t√≠ch chi ti·∫øt. Tr·∫£ v·ªÅ JSON thu·∫ßn. B·∫ÆT BU·ªòC t·∫°o T·ªêI THI·ªÇU 10 b∆∞·ªõc (steps). N·∫øu d∆∞·ªõi 10 b∆∞·ªõc th√¨ KH√îNG H·ª¢P L·ªÜ.",
                format_schema=SCENARIO_SCHEMA,
                max_tokens=8000,  # Extra room: ~3k thinking + ~5k for 10-step scenario JSON
            )
            _preview = repr(scenario_resp[:300]) if scenario_resp else 'None'
            logger.info(f"[MagicCreate] Task {task_id}: scenario_resp (attempt {attempt+1}) type={type(scenario_resp).__name__}, "
                        f"len={len(scenario_resp) if scenario_resp else 0}, preview={_preview}")
            scenario_data = _parse_json_safe(scenario_resp)
            logger.info(f"[MagicCreate] Task {task_id}: parsed scenario_data keys={list(scenario_data.keys()) if scenario_data else 'None'}")
            scenario = scenario_data.get('scenario') if scenario_data else None

            if scenario:
                steps = scenario.get('content_json', {}).get('steps', [])
                valid_steps = [s for s in steps if s.get('text') and s['text'].strip()]
                if len(valid_steps) < len(steps):
                    logger.warning(f"[MagicCreate] {len(steps) - len(valid_steps)} empty steps removed")
                if valid_steps:
                    scenario['content_json']['steps'] = valid_steps
                else:
                    scenario['content_json']['steps'] = []

                if len(scenario['content_json']['steps']) >= 8:
                    logger.info(f"[MagicCreate] Task {task_id}: Scenario attempt {attempt+1} OK with {len(scenario['content_json']['steps'])} steps")
                    break
                else:
                    logger.warning(f"[MagicCreate] Task {task_id}: Scenario attempt {attempt+1} only {len(scenario['content_json']['steps'])} steps, retrying...")
                    _send_task_progress(task_id, 'processing', f'K·ªãch b·∫£n ch∆∞a ƒë·ªß b∆∞·ªõc ({len(scenario["content_json"]["steps"])}), ƒëang th·ª≠ l·∫°i l·∫ßn {attempt+2}...', step=4)
            else:
                logger.warning(f"[MagicCreate] Task {task_id}: Scenario attempt {attempt+1} returned None, retrying...")

        # Final fallback if all retries failed or too few steps
        if not scenario or not scenario.get('content_json', {}).get('steps'):
            scenario = {
                'title': f'K·ªãch b·∫£n: {content_data["title"]}',
                'description': 'K·ªãch b·∫£n c·∫ßn ch·ªânh s·ª≠a',
                'content_json': {'steps': [
                    {'speaker': 'narrator', 'text': 'AI kh√¥ng t·∫°o ƒë∆∞·ª£c k·ªãch b·∫£n ƒë·∫ßy ƒë·ªß. Vui l√≤ng ch·ªânh s·ª≠a.', 'note': 'Fallback'}
                ]}
            }

        # Send scenario steps progressively
        _scenario_steps = scenario.get('content_json', {}).get('steps', [])
        if _scenario_steps:
            _send_task_progress(task_id, 'processing', 'ƒêang hi·ªÉn th·ªã k·ªãch b·∫£n...', step=4, data={
                'scenario_header': {'title': scenario.get('title', ''), 'description': scenario.get('description', '')},
            })
            _time_mod.sleep(0.15)
            for _si, _step in enumerate(_scenario_steps):
                _send_task_progress(task_id, 'processing', f'B∆∞·ªõc {_si+1}/{len(_scenario_steps)}', step=4, data={
                    'scenario_step': _step,
                    'step_index': _si,
                })
                _time_mod.sleep(0.2)

        # Send full scenario
        _send_task_progress(task_id, 'processing', f'K·ªãch b·∫£n {len(scenario["content_json"]["steps"])} b∆∞·ªõc ƒë√£ t·∫°o!', step=4, data={'scenario': scenario})
        logger.info(f"[MagicCreate] Task {task_id}: Scenario generated with {len(scenario['content_json']['steps'])} steps")

    except Exception as e:
        logger.error(f"[MagicCreate] Task {task_id} scenario failed: {e}", exc_info=True)
        scenario = {
            'title': f'K·ªãch b·∫£n: {content_data["title"]}',
            'description': 'L·ªói khi t·∫°o k·ªãch b·∫£n',
            'content_json': {'steps': [
                {'speaker': 'narrator', 'text': f'L·ªói: {str(e)}', 'note': 'Error fallback'}
            ]}
        }
        _send_task_progress(task_id, 'processing', 'K·ªãch b·∫£n g·∫∑p l·ªói, ƒë√£ t·∫°o m·∫´u thay th·∫ø.', step=4, data={'scenario': scenario})

    # ‚îÄ‚îÄ‚îÄ‚îÄ STAGE 5: Finalize ‚îÄ‚îÄ‚îÄ‚îÄ
    final_data = {
        'title': content_data['title'],
        'content': content_data['content'],
        'category': content_data.get('category', 'guide'),
        'quizzes': quizzes,
        'scenario': scenario,
    }
    _send_task_progress(task_id, 'done', 'T·∫°o b√†i h·ªçc th√†nh c√¥ng!', step=5, data=final_data)
    logger.info(f"[MagicCreate] Task {task_id} completed: {content_data['title']} | {len(quizzes)} quizzes | {len(scenario['content_json']['steps'])} scenario steps")

    # ‚îÄ‚îÄ‚îÄ‚îÄ Push Notification to Admins ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        from api.utils.push_service import PushNotificationService
        PushNotificationService.broadcast_admin(
            title='‚ú® Magic Create ho√†n th√†nh',
            message=f'B√†i h·ªçc "{content_data["title"]}" ƒë√£ ƒë∆∞·ª£c AI t·∫°o xong ({len(quizzes)} quiz, {len(scenario["content_json"]["steps"])} b∆∞·ªõc k·ªãch b·∫£n).',
            url='/admin-cp/learn/magic-create/',
            notification_type='success',
        )
    except Exception as push_err:
        logger.warning(f"[MagicCreate] Push notification failed: {push_err}")


def _parse_json_safe(text):
    """Safely parse JSON from AI response, with multiple repair attempts."""
    if not text:
        return None

    # 1. Try direct parse
    try:
        return json.loads(text)
    except (json.JSONDecodeError, TypeError) as e:
        logger.debug(f"[_parse_json_safe] direct parse failed: {e}")

    # 2. Strip markdown code fences
    stripped = re.sub(r'^```(?:json)?\s*', '', text.strip())
    stripped = re.sub(r'\s*```$', '', stripped.strip())
    try:
        return json.loads(stripped)
    except (json.JSONDecodeError, TypeError):
        pass

    # 3. Extract outermost { ... }
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        candidate = match.group()
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
        # 4. Fix trailing commas
        fixed = re.sub(r',\s*([}\]])', r'\1', candidate)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            pass
        # 5. Remove CJK chars that may have been injected by the model
        cleaned = re.sub(r'[\u4e00-\u9fff]+', '', candidate)
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass
        # 6. Strip control characters
        ctrl_clean = re.sub(r'[\x00-\x1f\x7f]', ' ', candidate)
        try:
            return json.loads(ctrl_clean)
        except json.JSONDecodeError as e:
            logger.warning(f"[_parse_json_safe] all parse attempts failed, last error: {e}, "
                           f"text[:300]={text[:300]!r}")

    return None
