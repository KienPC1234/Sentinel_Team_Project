"""
Ollama LLM Integration using the official 'ollama' Python library.
Connects to the configured Ollama endpoint.
Settings are read from django.conf.settings:
  - OLLAMA_BASE_URL
  - LLM_MODEL
  - LLM_TEMPERATURE
  - LLM_MAX_TOKENS
"""

import json
import logging
import os
import re
from typing import Optional, Dict, Any, Generator, List

from django.conf import settings
import ollama

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Read from Django settings with sensible defaults
OLLAMA_BASE_URL = getattr(settings, 'OLLAMA_BASE_URL', 'http://localhost:11434')
OLLAMA_API_KEY = getattr(settings, 'OLLAMA_API_KEY', None)

# Mirror OLLAMA_API_KEY into the environment for any ollama library internals
# that may read it (e.g. the search_agent client.chat call).
if OLLAMA_API_KEY:
    os.environ.setdefault('OLLAMA_API_KEY', OLLAMA_API_KEY)
DEFAULT_MODEL = getattr(settings, 'LLM_MODEL', 'neural-chat')
SMALL_MODEL = getattr(settings, 'SMALL_MODEL', DEFAULT_MODEL)
LLM_TEMPERATURE = getattr(settings, 'LLM_TEMPERATURE', 0.3)
LLM_MAX_TOKENS = getattr(settings, 'LLM_MAX_TOKENS', 1800)
DEBUG_LLM = getattr(settings, 'DEBUG_LLM', True)  # Default True for ShieldCall Debugging

# ---------------------------------------------------------------------------
# Custom tool schemas for the search agent.
# We define our OWN plain function objects so the model receives clean,
# auth-free tool specs.  When the model requests these tools we dispatch
# to web_search_query() / web_fetch_url() ourselves — the model never sees
# any authentication metadata and therefore never refuses to call them.
# ---------------------------------------------------------------------------

def _tool_web_search(query: str, max_results: int = 5):
    """Search the web for pages matching a query.

    Args:
        query: The search query string.
        max_results: Maximum number of results to return (default 5).

    Returns:
        A list of search results with title, url, and content fields.
    """
    return web_search_query(query, max_results=max_results)


def _tool_web_fetch(url: str):
    """Fetch the content of a web page by URL.

    Args:
        url: The URL of the web page to fetch.

    Returns:
        A dict containing title, content, and links of the fetched page.
    """
    return web_fetch_url(url)


def _tool_lookup_scamadviser(domain: str):
    """Check a domain or website against ScamAdviser's trust score database.

    Use this tool whenever you need to assess whether a website or domain is
    safe, suspicious, or a known scam site. ScamAdviser provides automated
    trust scores, registration details, hosting country, and risk indicators.

    Args:
        domain: The domain name to look up, e.g. 'example.com'. Strip any
                leading 'http://' or 'https://' scheme before passing.

    Returns:
        A dict with title and content fields containing the ScamAdviser trust
        assessment and risk indicators for the domain.
    """
    return lookup_scamadviser(domain)


def _tool_lookup_scamwave(query: str):
    """Search the ScamWave scammer database for reports matching a query.

    Use this tool to look up phone numbers, bank account numbers, names, or
    keywords in the ScamWave community-sourced scam report database. It
    surfaces victim-reported fraud cases, blacklisted numbers, and warnings.

    Args:
        query: Phone number, bank account number, full name, or keyword to
               search for in the ScamWave database.

    Returns:
        A dict with title and content fields containing matching scam reports
        and listings from the ScamWave database.
    """
    return lookup_scamwave(query)


def _tool_lookup_trustpilot(domain: str):
    """Check a domain's reputation and user reviews on Trustpilot.

    Use this tool to find user feedback, star ratings, and complaint details
    for a specific domain. Trustpilot is a major source for verifying legitimacy.

    Args:
        domain: The domain name to check (e.g. 'example.com').
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    return web_fetch_url(f"https://www.trustpilot.com/review/{clean}")


def _tool_lookup_sitejabber(domain: str):
    """Check a domain's reputation and user reviews on sitejabber.

    Use this tool to find consumer reviews and business ratings. sitejabber
    often has reviews for smaller online businesses that Trustpilot might miss.

    Args:
        domain: The domain name to check (e.g. 'example.com').
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    return web_fetch_url(f"https://www.sitejabber.com/reviews/{clean}")


def _tool_lookup_tranco(domain: str):
    """Check a domain's popularity ranking on Tranco list.

    Args:
        domain: Domain name (e.g. 'example.com').

    Returns:
        A dict with rank and history if available.
    """
    return lookup_tranco(domain)


# Map tool name -> our Python dispatcher (used in search_agent loop)
WEB_TOOLS: Dict[str, Any] = {
    '_tool_web_search': _tool_web_search,
    '_tool_web_fetch': _tool_web_fetch,
    '_tool_lookup_scamwave': _tool_lookup_scamwave,
    '_tool_lookup_trustpilot': _tool_lookup_trustpilot,
    '_tool_lookup_sitejabber': _tool_lookup_sitejabber,
    '_tool_lookup_tranco': _tool_lookup_tranco,
}

# Tool definitions for ShieldCall AI
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "scan_phone",
            "description": "Scan a phone number for scam/risk reports.",
            "parameters": {
                "type": "object",
                "properties": {
                    "phone": {"type": "string", "description": "The phone number to scan"}
                },
                "required": ["phone"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "scan_url",
            "description": "Scan a domain or URL for phishing/malware risk.",
            "parameters": {
                "type": "object",
                "properties": {
                    "url": {"type": "string", "description": "The domain or URL to scan"}
                },
                "required": ["url"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "scan_bank_account",
            "description": "Scan a bank account number for fraud reports.",
            "parameters": {
                "type": "object",
                "properties": {
                    "account_number": {"type": "string", "description": "The bank account number"},
                    "bank_name": {"type": "string", "description": "Optional bank name (e.g. MB, VCB)"}
                },
                "required": ["account_number"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "Search the internet for real-time information and news.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "The search query"},
                    "max_results": {"type": "integer", "description": "Max results (1-10)"}
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "web_fetch",
            "description": "Fetch and read the content of a specific web page.",
            "parameters": {
                "type": "object",
                "properties": {
                    "url": {"type": "string", "description": "The URL to fetch"}
                },
                "required": ["url"]
            }
        }
    }
]

# ---------------------------------------------------------------------------
# STRUCTURED OUTPUT SCHEMAS
# ---------------------------------------------------------------------------

# Schema for structured classification output
CLASSIFICATION_SCHEMA = {
    "type": "object",
    "properties": {
        "classification": {"type": "string", "enum": ["safe", "suspicious", "dangerous"]},
        "suggested_action": {"type": "string", "enum": ["NONE", "BLOCK", "REPORT"]},
        "confidence": {"type": "number"}
    },
    "required": ["classification", "suggested_action", "confidence"]
}

# Schema for structured scam analysis output
SCAM_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "is_scam": {"type": "boolean"},
        "risk_score": {"type": "integer"},
        "indicators": {"type": "array", "items": {"type": "string"}},
        "explanation": {"type": "string"},
        "web_sources": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["is_scam", "risk_score", "indicators", "explanation", "web_sources"]
}

# Initialize client
# Auto-correct bare 'ollama.com' to 'api.ollama.com' — the former is the
# website, the latter is the Ollama cloud API endpoint used by the Python SDK.
_effective_base_url = OLLAMA_BASE_URL
if OLLAMA_API_KEY and 'ollama.com' in OLLAMA_BASE_URL and 'api.ollama.com' not in OLLAMA_BASE_URL:
    _effective_base_url = 'https://api.ollama.com'
    logger.info(
        f"OLLAMA_BASE_URL '{OLLAMA_BASE_URL}' corrected to '{_effective_base_url}' "
        f"(cloud API endpoint required for authenticated models)."
    )

client_kwargs = {'host': _effective_base_url}
if OLLAMA_API_KEY:
    client_kwargs['headers'] = {
        'Authorization': f'Bearer {OLLAMA_API_KEY}',
    }

client = ollama.Client(**client_kwargs)

# ---------------------------------------------------------------------------
# Retry helper — wraps client.chat() with automatic retries on 503 / network
# errors.  Max 3 attempts with exponential backoff (2s, 4s).
# ---------------------------------------------------------------------------
import time as _time

def _chat_with_retry(max_retries: int = 4, **chat_kwargs):
    """Call client.chat() with retry on 503 / transient errors."""
    last_exc = None
    for attempt in range(1, max_retries + 1):
        try:
            return client.chat(**chat_kwargs)
        except Exception as e:
            last_exc = e
            err_str = str(e)
            # Match 503, 502, 504 (timeout), 429 (rate limit)
            is_retryable = any(tok in err_str for tok in ['503', '502', '504', '429', 'Service Unavailable',
                                                           'temporarily unavailable', 'overloaded',
                                                           'Connection', 'Timeout', 'remote end closed'])
            if is_retryable and attempt < max_retries:
                wait = 2 ** attempt  # 2s, 4s, 8s
                logger.warning(f"[Ollama] Attempt {attempt}/{max_retries} failed ({err_str[:120]}), "
                               f"retrying in {wait}s...")
                _time.sleep(wait)
                continue
            
            logger.error(f"[Ollama] Final attempt {attempt} failed: {err_str}")
            raise last_exc


def _chat_stream_with_retry(max_retries: int = 4, **chat_kwargs):
    """Call client.chat(stream=True) with retry on 503 / transient errors.
    Returns the streaming iterator."""
    chat_kwargs['stream'] = True
    last_exc = None
    for attempt in range(1, max_retries + 1):
        try:
            return client.chat(**chat_kwargs)
        except Exception as e:
            last_exc = e
            err_str = str(e)
            is_retryable = any(tok in err_str for tok in ['503', '502', '504', '429', 'Service Unavailable',
                                                           'temporarily unavailable', 'overloaded',
                                                           'Connection', 'Timeout', 'remote end closed'])
            if is_retryable and attempt < max_retries:
                wait = 2 ** attempt
                logger.warning(f"[Ollama] Stream attempt {attempt}/{max_retries} failed ({err_str[:120]}), "
                               f"retrying in {wait}s...")
                _time.sleep(wait)
                continue
            
            logger.error(f"[Ollama] Final stream attempt {attempt} failed: {err_str}")
            raise last_exc


# Specialized logger for LLM traffic
llm_logger = logging.getLogger('pkv.llm')
llm_logger.setLevel(logging.INFO)
if not llm_logger.handlers:
    fh = logging.FileHandler('llm_access.log')
    fh.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
    llm_logger.addHandler(fh)

def _log_llm(prompt: str, response: str, system_prompt: str = None):
    """Detailed logging of LLM interactions"""
    log_data = {
        'system_prompt': system_prompt,
        'prompt_length': len(prompt),
        'prompt_preview': prompt[:200] + '...' if len(prompt) > 200 else prompt,
        'response_length': len(response) if response else 0,
        'response_preview': response[:200] + '...' if response else ""
    }
    llm_logger.info(f"LLM Call: {json.dumps(log_data, ensure_ascii=False)}")
    
def filter_thinking(text: str) -> str:
    """
    Removes igid blocks from model responses.
    Used for reasoning-capable models like DeepSeek.
    """
    if not text:
        return ""
    # Remove content between igid and igid tags
    return re.sub(r'igid.*?igid', '', text, flags=re.DOTALL).strip()


def filter_hallucinations(text: str) -> str:
    """
    Filter out common AI hallucinations like unexpected CJK characters
    in a non-CJK context, especially the 'strange unicode' characters
    reported by the user (e.g., 自称).
    """
    if not text:
        return text
    # Strip CJK Unified Ideographs if found in isolation or in strange spots
    # Range: \u4e00-\u9fff (Common Chinese)
    # We mainly target the "strange" ones like 自称 \u81ea\u79f0
    text = re.sub(r'[\u4e00-\u9fff]+', '', text)
    return text


def filter_tool_call_artifacts(text: str) -> str:
    """
    Remove accidental tool-call JSON artifacts from agent textual output.

    Some models occasionally emit raw objects like:
    {"name":"_tool_lookup_scamadviser","arguments":{...}}
    in plain content instead of structured ``tool_calls``.
    """
    if not text:
        return ""

    cleaned = text

    # Remove XML-like empty tool blocks emitted by some models.
    cleaned = re.sub(r'<\s*tools\s*>\s*<\s*/\s*tools\s*>', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'</?\s*tools\s*>', '', cleaned, flags=re.IGNORECASE)

    # Remove compact/raw tool call JSON objects (single or multiple objects).
    cleaned = re.sub(
        r'\{\s*"name"\s*:\s*"_tool_[^\"]+"\s*,\s*"arguments"\s*:\s*\{[^{}]*\}\s*\}\s*',
        '',
        cleaned,
        flags=re.DOTALL,
    )

    # Remove any line that still contains explicit tool invocations.
    cleaned = "\n".join(
        line for line in cleaned.splitlines()
        if '_tool_' not in line and '"arguments"' not in line
    )

    # Normalize excessive blank lines.
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    return cleaned.strip()

def _extract_json(text: str) -> Optional[dict]:
    """
    Extract and parse the first JSON object from an LLM response.

    Handles markdown fenced code blocks, trailing commas, and ASCII control
    characters.  Works cleanly on `message.content` which, when the official
    chat-loop pattern is used with think=True, contains only the final answer
    (no thinking prose to confuse the extractor).
    """
    if not text:
        return None

    # 1. Prefer JSON inside a markdown code fence
    fence_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
    candidate = fence_match.group(1) if fence_match else None

    if candidate is None:
        # 2. Outermost { … } block
        brace_match = re.search(r'\{.*\}', text, re.DOTALL)
        if not brace_match:
            return None
        candidate = brace_match.group(0)

    # Strip ASCII control characters
    candidate = re.sub(r'[\x00-\x1F\x7F]', ' ', candidate)

    try:
        return json.loads(candidate)
    except json.JSONDecodeError:
        # Remove trailing commas (common model mistake) and retry
        fixed = re.sub(r',\s*([}\]])', r'\1', candidate)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            return None


def message_to_dict(msg) -> Dict[str, Any]:
    """
    Convert an ollama ``Message`` object (or any mapping) to a plain Python
    dict suitable for JSON serialisation, logging, or re-inserting into a
    messages list.

    Only fields that are actually set are included so the output stays clean.
    Handles:
      - ``role``        (str, always present)
      - ``content``     (str, omitted when None/empty)
      - ``thinking``    (str, omitted when None/empty)
      - ``tool_name``   (str, omitted when None)
      - ``tool_calls``  (list of dicts, omitted when empty/None)
      - ``images``      (list, omitted when empty/None)

    ``ToolCall`` objects are recursively converted to::

        {"function": {"name": "...", "arguments": {...}}}
    """
    # Already a plain dict — normalise and return
    if isinstance(msg, dict):
        return {k: v for k, v in msg.items() if v is not None}

    d: Dict[str, Any] = {'role': msg.role}

    content = getattr(msg, 'content', None)
    if content:
        d['content'] = content

    thinking = getattr(msg, 'thinking', None)
    if thinking:
        d['thinking'] = thinking

    tool_name = getattr(msg, 'tool_name', None)
    if tool_name:
        d['tool_name'] = tool_name

    tool_calls = getattr(msg, 'tool_calls', None)
    if tool_calls:
        serialised = []
        for tc in tool_calls:
            if isinstance(tc, dict):
                serialised.append(tc)
            else:
                # ollama ToolCall object
                fn = tc.function
                serialised.append({
                    'function': {
                        'name': fn.name,
                        'arguments': dict(fn.arguments) if fn.arguments else {},
                    }
                })
        d['tool_calls'] = serialised

    images = getattr(msg, 'images', None)
    if images:
        d['images'] = list(images)

    return d


def is_ollama_available() -> bool:
    """Check if Ollama service is running."""
    try:
        client.list()
        return True
    except Exception as e:
        logger.warning(f"Ollama not available: {e}")
        return False

def get_available_models() -> list:
    """Get list of available models in Ollama."""
    try:
        models = client.list()
        return [m['name'] for m in models['models']]
    except Exception as e:
        logger.error(f"Error getting Ollama models: {e}")
    return []

def generate_response(
    prompt: str,
    model: str = None,
    system_prompt: str = None,
    num_ctx: int = None,
    max_tokens: int = None,
    tools: list = None,
    tool_dispatch: Dict[str, Any] = None,
    format_schema: dict = None,
    max_loop_iterations: int = 15,
) -> Optional[str]:
    """
    Generate a non-streamed response from Ollama.

    When ``format_schema`` is provided, the model is guaranteed to return
    valid JSON matching the schema, simplifying the loop logic.
    """
    model = model or DEFAULT_MODEL
    if tools is None:
        tools = TOOLS
    tool_dispatch = tool_dispatch or {}

    try:
        messages: List[Dict] = []
        if system_prompt:
            messages.append({'role': 'system', 'content': system_prompt})
        messages.append({'role': 'user', 'content': prompt})

        options: Dict[str, Any] = {
            'temperature': LLM_TEMPERATURE,
            'num_predict': max_tokens or LLM_MAX_TOKENS,
        }
        if num_ctx:
            options['num_ctx'] = num_ctx

        reply = ''
        response = None
        iteration = 0

        while iteration < max_loop_iterations:
            logger.info(f"[generate_response iter={iteration}] calling model '{model}' "
                        f"with {len(messages)} message(s), tools={'yes' if tools else 'no'}")

            chat_kwargs: Dict[str, Any] = {
                'model': model,
                'messages': messages,
                'options': options,
                'think': "low",
            }
            if tools:
                chat_kwargs['tools'] = tools
            if format_schema:
                chat_kwargs['format'] = format_schema

            response = _chat_with_retry(**chat_kwargs)
            assistant_msg = message_to_dict(response.message)
            messages.append(assistant_msg)

            part = response.message.content or ''
            tool_calls = response.message.tool_calls

            iteration += 1

            if tool_calls:
                for tool_call in tool_calls:
                    tool_name = tool_call.function.name
                    args = tool_call.function.arguments or {}
                    tool_func = tool_dispatch.get(tool_name)
                    result_str = tool_func(**args) if tool_func else f"Tool '{tool_name}' not available."
                    messages.append({'role': 'tool', 'content': result_str, 'tool_name': tool_name})
                iteration += 1
                continue
            elif part:
                reply = part.strip()
                logger.info(f"[generate_response iter={iteration}] ✅ final answer (no tools), exiting loop")
                break

        if iteration >= max_loop_iterations:
            logger.warning(f"[generate_response] hit max_loop_iterations={max_loop_iterations}, returning best content so far")

        _log_llm(prompt, reply, system_prompt)
        return filter_hallucinations(reply)

    except Exception as e:
        logger.error(f"Error calling Ollama: {e}")
        return None

def stream_response(prompt: str, system_prompt: str = None, model: str = None) -> Generator[str, None, None]:
    """
    Stream a response from Ollama, filtering out igid blocks in real-time.
    """
    model = model or DEFAULT_MODEL
    messages = []
    if system_prompt:
        messages.append({'role': 'system', 'content': system_prompt})
    messages.append({'role': 'user', 'content': prompt})

    try:
        stream = _chat_stream_with_retry(
            model=model,
            messages=messages,
            options={
                'temperature': LLM_TEMPERATURE,
                'num_predict': LLM_MAX_TOKENS,
            },
        )

        is_thinking = False
        full_response = []
        think_buffer = ""

        for chunk in stream:
            content = filter_hallucinations(chunk['message']['content'])
            if not content:
                continue

            if not is_thinking:
                if "igid" in content:
                    is_thinking = True
                    yield "__STATUS__:thinking"
                    parts = content.split("igid", 1)
                    if parts[0]:
                        yield parts[0]
                        full_response.append(parts[0])
                    if "igid" in parts[1]:
                        is_thinking = False
                        subparts = parts[1].split("igid", 1)
                        if subparts[1]:
                            yield subparts[1]
                            full_response.append(subparts[1])
                    continue
                else:
                    yield content
                    full_response.append(content)
            else:
                if "igid" in content:
                    is_thinking = False
                    parts = content.split("igid", 1)
                    if len(parts) > 1 and parts[1]:
                        yield parts[1]
                        full_response.append(parts[1])
                else:
                    # Optional: periodically yield thinking status to keep connection alive
                    if len(full_response) % 20 == 0:
                        yield "__STATUS__:thinking"

        _log_llm(prompt, "".join(full_response), system_prompt)

    except Exception as e:
        logger.error(f"Error in Ollama stream: {e}")
        yield f"Lỗi kết nối AI: {str(e)}"


# ---------------------------------------------------------------------------
# AI Assistant tool wrappers (parameter names match TOOLS definitions)
# ---------------------------------------------------------------------------

def _assistant_scan_phone(phone: str):
    """Scan a phone number: check ScamWave then web search for reports."""
    results = {}
    sw = _tool_lookup_scamwave(query=phone)
    if sw and sw.get('content'):
        results['scamwave'] = sw['content'][:2000]
    ws = web_search_query(f"lừa đảo số điện thoại {phone} scam report", max_results=3)
    if ws:
        results['web_results'] = ws
    return results or f"Không tìm thấy báo cáo lừa đảo về số {phone}."


def _assistant_scan_url(url: str):
    """Scan a URL/domain: check ScamAdviser, Trustpilot, Tranco and web search."""
    clean = re.sub(r'^https?://', '', url).split('/')[0].strip()
    results = {}
    sa = lookup_scamadviser(clean)
    if sa and sa.get('content'):
        results['scamadviser'] = sa['content'][:2000]
    tp = _tool_lookup_trustpilot(domain=clean)
    if tp and tp.get('content'):
        results['trustpilot'] = tp['content'][:2000]
    tr = lookup_tranco(clean)
    if tr:
        results['tranco'] = tr
    ws = web_search_query(f"{clean} scam lừa đảo review", max_results=3)
    if ws:
        results['web_results'] = ws
    return results or f"Không tìm thấy thông tin rủi ro về {url}."


def _assistant_scan_bank_account(account_number: str, bank_name: str = ""):
    """Scan a bank account number for fraud reports on ScamWave + web."""
    query = f"{account_number} {bank_name}".strip()
    results = {}
    sw = _tool_lookup_scamwave(query=query)
    if sw and sw.get('content'):
        results['scamwave'] = sw['content'][:2000]
    ws = web_search_query(f"lừa đảo tài khoản ngân hàng {query} scam", max_results=3)
    if ws:
        results['web_results'] = ws
    return results or f"Không tìm thấy báo cáo lừa đảo về tài khoản {account_number}."


def stream_chat_ai(messages: list, model: str = None, tool_dispatch: dict = None, debug: bool = False) -> Generator[str, None, None]:
    """
    Improved streaming chat for ShieldCall AI.
    Handles multi-turn tool loops, thinking streaming, and search result yielding.
    """
    if debug or DEBUG_LLM:
        yield f"__DEBUG_MODEL__:{model or DEFAULT_MODEL}"
        # Small summary of history
        yield f"__DEBUG_HISTORY_COUNT__:{len(messages)}"
    model = model or DEFAULT_MODEL
    tool_dispatch = tool_dispatch or {}
    # Combine with standard tools
    all_dispatch = {
        'web_search': web_search_query,
        'web_fetch': web_fetch_url,
        'scan_phone': _assistant_scan_phone,
        'scan_url': _assistant_scan_url,
        'scan_bank_account': _assistant_scan_bank_account,
    }
    all_dispatch.update(tool_dispatch)

    max_loops = 10
    iteration = 0

    while iteration < max_loops:
        try:
            chat_params = {
                'model': model,
                'messages': messages,
                'options': {
                    'temperature': LLM_TEMPERATURE,
                    'num_predict': LLM_MAX_TOKENS,
                },
                'stream': True,
                'think': True  # Enable thinking if supported
            }
            if TOOLS:
                chat_params['tools'] = TOOLS

            stream = _chat_stream_with_retry(**chat_params)
            
            current_tool_calls = []
            current_content = []
            is_thinking = False

            for chunk in stream:
                # Handle both dict-like and object-like access for chunk/message
                msg = getattr(chunk, 'message', {})
                if not msg and hasattr(chunk, 'get'):
                    msg = chunk.get('message', {})

                # Handle Thinking
                thinking = getattr(msg, 'thinking', "")
                if not thinking and hasattr(msg, 'get'):
                    thinking = msg.get('thinking', "")
                
                if thinking:
                    yield f"__THINK__:{thinking}"
                
                # Handle Content
                content = getattr(msg, 'content', "")
                if not content and hasattr(msg, 'get'):
                    content = msg.get('content', "")
                
                if content:
                    content = filter_hallucinations(content)
                    if content: # Could be empty after filtering
                        yield content
                        current_content.append(content)

                # Collect Tool Calls
                tool_calls = getattr(msg, 'tool_calls', [])
                if not tool_calls and hasattr(msg, 'get'):
                    tool_calls = msg.get('tool_calls', [])
                
                if tool_calls:
                    current_tool_calls.extend(tool_calls)

            # If no tool calls, we are done
            if not current_tool_calls:
                # Log the final response
                _log_llm("Chat history...", "".join(current_content))
                break

            # Handle Tool Calls
            messages.append({'role': 'assistant', 'content': "".join(current_content), 'tool_calls': current_tool_calls})
            
            serializable_calls = []
            for tc in current_tool_calls:
                tool_name = tc.function.name
                args = tc.function.arguments or {}
                
                # Yield marker for UI
                yield f"__STATUS__:executing_tool:{tool_name}"
                
                serializable_calls.append({
                    'function': {'name': tool_name, 'arguments': args}
                })

                # Execute
                tool_func = all_dispatch.get(tool_name)
                if tool_func:
                    try:
                        result = tool_func(**args)
                        
                        # Special handling for search results to show in UI
                        if tool_name == 'web_search' and isinstance(result, list):
                            yield f"__SEARCH_RESULTS__:{json.dumps(result)}"
                        
                        if result is None:
                            result_str = "Không có kết quả."
                        elif isinstance(result, (dict, list)):
                            result_str = json.dumps(result, ensure_ascii=False)
                            if len(result_str) > 8000:
                                result_str = result_str[:8000] + "..."
                        else:
                            result_str = str(result)[:8000]
                    except Exception as e:
                        logger.error(f"Tool {tool_name} execution error: {e}")
                        result_str = f"Lỗi khi thực thi công cụ: {str(e)}"
                else:
                    result_str = f"Tool '{tool_name}' not found."

                messages.append({
                    'role': 'tool',
                    'content': result_str,
                    'tool_name': tool_name
                })

            yield f"__TOOL_CALLS__:{json.dumps(serializable_calls)}"
            iteration += 1
            continue

        except Exception as e:
            logger.error(f"Error in stream_chat_ai: {e}", exc_info=True)
            yield f"Error: {str(e)}"
            if debug or DEBUG_LLM:
                yield f"__DEBUG_ERROR__:{str(e)}"
            break

def classify_message(message: str, model: str = None) -> Dict[str, Any]:
    """
    Classify a message and suggest actions.
    """
    prompt = f"""Bạn là chuyên gia an ninh mạng. Hãy phân loại tin nhắn sau và đề xuất hành động xử lý.
Trả về KẾT QUẢ THUẦN JSON (không có markdown, không có văn bản thừa).
JSON phải có các trường: classification (safe/suspicious/dangerous), suggested_action (NONE/BLOCK/REPORT), confidence (số thực 0-1).
Tin nhắn cần phân loại: {message}"""

    response = generate_response(prompt, model, tools=[], format_schema=CLASSIFICATION_SCHEMA)

    if response:
        parsed = _extract_json(response)
        if parsed:
            return parsed

    return {
        "classification": "unknown",
        "suggested_action": "NONE",
        "confidence": 0.5,
    }

# ---------------------------------------------------------------------------
# Web search / fetch helpers
# These call https://ollama.com/api/web_search and /web_fetch directly via
# requests so we control the Authorization header ourselves rather than
# relying on the ollama library's global client (which is initialised at
# import time and does not pick up our OLLAMA_API_KEY reliably).
# ---------------------------------------------------------------------------

_OLLAMA_WEB_API_BASE = "https://ollama.com/api"


def _web_api_headers() -> Dict[str, str]:
    """Build auth headers for the Ollama web API."""
    key = OLLAMA_API_KEY or os.environ.get('OLLAMA_API_KEY', '')
    return {
        'Authorization': f'Bearer {key}',
        'Content-Type': 'application/json',
    }


def web_search_query(query: str, max_results: int = 5) -> Optional[List[Dict]]:
    """
    Perform a web search via Ollama's web search REST API.
    (https://ollama.com/api/web_search)
    Requires OLLAMA_API_KEY in Django settings.

    Returns a list of result dicts with keys: title, url, content.
    """
    import requests as _requests
    try:
        resp = _requests.post(
            f"{_OLLAMA_WEB_API_BASE}/web_search",
            headers=_web_api_headers(),
            json={"query": query, "max_results": max_results},
            timeout=15,
        )
        resp.raise_for_status()
        data = resp.json()
        results = data.get('results', [])
        return results
    except Exception as e:
        logger.error(f"web_search_query error: {e}")
        return None


def web_fetch_url(url: str) -> Optional[Dict]:
    """
    Fetch a web page. Primary: Ollama web_fetch API. Fallback: local requests + BS4.

    Returns a dict with keys: title, content, links.
    """
    import requests as _requests
    from bs4 import BeautifulSoup

    if not url:
        return None

    target_url = url.strip()
    if not re.match(r'^[a-zA-Z][a-zA-Z0-9+\-.]*://', target_url):
        target_url = f"https://{target_url}"

    # Block requests to Ollama local server (prevent SSRF)
    blocked = ('localhost:11434', '127.0.0.1:11434')
    if any(b in target_url for b in blocked):
        logger.warning(f"web_fetch_url blocked internal URL: {target_url}")
        return None
    
    browser_headers = {
        'User-Agent': (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/126.0.0.0 Safari/537.36'
        ),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
    }

    def _decode_response_text(resp):
        text = resp.text or ''
        if any(marker in text for marker in ('Ã', 'Â', 'á»', '�')):
            try:
                enc = resp.apparent_encoding or 'utf-8'
                text = resp.content.decode(enc, errors='replace')
            except Exception:
                pass
        return text

    def _extract_payload(resp, requested_url):
        html = _decode_response_text(resp)
        soup = BeautifulSoup(html, 'html.parser')

        for tag in soup(['script', 'style', 'noscript', 'iframe']):
            tag.decompose()

        title_tag = soup.find('title')
        title = title_tag.get_text(' ', strip=True) if title_tag else ''

        content = soup.get_text(separator='\n', strip=True)
        if len(content) > 12000:
            content = content[:12000]

        links = []
        for a in soup.find_all('a', href=True):
            href = (a.get('href') or '').strip()
            if not href:
                continue
            absolute = _requests.compat.urljoin(resp.url or requested_url, href)
            if absolute not in links:
                links.append(absolute)
            if len(links) >= 30:
                break

        return {
            'title': title,
            'content': content,
            'links': links,
        }

    try:
        resp = _requests.get(target_url, headers=browser_headers, timeout=20, verify=True, allow_redirects=True)
        resp.raise_for_status()
        payload = _extract_payload(resp, target_url)
        if payload.get('content'):
            return payload
    except Exception as e:
        logger.warning(f"web_fetch_url local https failed ({target_url}): {e}")

    if target_url.startswith('https://'):
        try:
            http_url = target_url.replace('https://', 'http://', 1)
            resp = _requests.get(http_url, headers=browser_headers, timeout=20, verify=False, allow_redirects=True)
            resp.raise_for_status()
            payload = _extract_payload(resp, http_url)
            if payload.get('content'):
                return payload
        except Exception as e:
            logger.warning(f"web_fetch_url local http failed ({target_url}): {e}")

    return None


# ---------------------------------------------------------------------------
# Dedicated scam database lookup helpers
# ---------------------------------------------------------------------------

# Known credible scam-reporting databases consulted during analysis.
SCAM_DB_URLS = {
    'scamadviser': 'https://www.scamadviser.com/check-website/',
    'scamwave':    'https://scamwave.com/scammers/',
    'trustpilot':  'https://www.trustpilot.com/review/',
    'sitejabber':  'https://www.sitejabber.com/reviews/',
    'tranco':      'https://tranco-list.eu/api/ranks/domain/',
}


def lookup_scamadviser(domain: str) -> Optional[Dict]:
    """
    Fetch the ScamAdviser report for a domain.

    Prioritizes extracting ``ratingScore`` from the SPA bootstrap payload
    embedded in ``<div id="app" data-page="...">``. This is the same source
    used by client-side rendering and is more reliable than placeholder
    progress bars in static HTML. Falls back to textual verdict extraction
    (e.g. ``Very Likely Safe``, ``Caution Recommended``) when needed.

    Args:
        domain: Domain to look up (e.g. 'example.com').  Any leading scheme
                (``http://`` / ``https://``) is stripped automatically.

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    if not clean:
        return None
    url = f"{SCAM_DB_URLS['scamadviser']}{clean}"

    trust_score_from_page = None
    verdict_from_page = None
    page_text = ""

    # 1) Direct HTML fetch to extract data-page JSON + human-readable verdict.
    try:
        import html as _html
        import requests
        from bs4 import BeautifulSoup

        response = requests.get(
            url,
            headers={
                'User-Agent': (
                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:137.0) '
                    'Gecko/20100101 Firefox/137.0'
                )
            },
            timeout=12,
        )
        if response.ok:
            html_text = response.text or ''
            page_text = BeautifulSoup(html_text, 'html.parser').get_text(' ', strip=True)[:6000]

            # Parse <div id="app" data-page="..."> then read ratingScore.
            app_data_match = re.search(
                r'<div\s+id=["\']app["\']\s+data-page=["\']([^"\']+)["\']',
                html_text,
                flags=re.IGNORECASE | re.DOTALL,
            )
            if app_data_match:
                data_page_escaped = app_data_match.group(1)
                data_page_unescaped = _html.unescape(data_page_escaped)
                try:
                    data_page_obj = json.loads(data_page_unescaped)

                    score_candidates = [
                        data_page_obj.get('props', {}).get('score', {}).get('ratingScore'),
                        data_page_obj.get('props', {}).get('ratingScore'),
                        data_page_obj.get('ratingScore'),
                    ]
                    for candidate in score_candidates:
                        if isinstance(candidate, (int, float)):
                            numeric = int(candidate)
                            if 0 <= numeric <= 100:
                                trust_score_from_page = numeric
                                break

                    verdict_candidates = [
                        data_page_obj.get('props', {}).get('score', {}).get('ratingText'),
                        data_page_obj.get('props', {}).get('score', {}).get('ratingLabel'),
                        data_page_obj.get('props', {}).get('score', {}).get('conclusion'),
                        data_page_obj.get('props', {}).get('ratingText'),
                        data_page_obj.get('ratingText'),
                    ]
                    for candidate in verdict_candidates:
                        if isinstance(candidate, str) and candidate.strip():
                            verdict_from_page = candidate.strip()
                            break
                except Exception:
                    # Fallback: extract ratingScore directly from the unescaped data blob.
                    score_match = re.search(r'"ratingScore"\s*:\s*(\d+)', data_page_unescaped)
                    if score_match:
                        trust_score_from_page = int(score_match.group(1))

            # Secondary fallback: score may still appear in the HTML snapshot.
            if trust_score_from_page is None:
                score_match = re.search(r'"ratingScore"\s*:\s*(\d+)', html_text)
                if score_match:
                    trust_score_from_page = int(score_match.group(1))

            # Known ScamAdviser-style conclusions to prioritize.
            if not verdict_from_page:
                verdict_patterns = [
                    r'Very\s+Likely\s+Safe',
                    r'Likely\s+Safe',
                    r'Caution\s+Recommended',
                    r'Suspicious',
                    r'Unsafe',
                    r'Scam',
                ]
                for pattern in verdict_patterns:
                    match = re.search(pattern, page_text, flags=re.IGNORECASE)
                    if match:
                        verdict_from_page = match.group(0)
                        break
    except Exception as exc:
        logger.debug(f"lookup_scamadviser: direct HTML parse failed ({exc})")

    # 2) Keep existing web relay extraction as primary content source.
    result = web_fetch_url(url) or {
        'title': f'ScamAdviser report for {clean}',
        'content': page_text,
        'links': [],
    }

    # 2) If not found from direct page text, attempt extraction from relay content.
    if not verdict_from_page:
        relay_content = result.get('content', '') or ''
        verdict_patterns = [
            r'Very\s+Likely\s+Safe',
            r'Likely\s+Safe',
            r'Caution\s+Recommended',
            r'Suspicious',
            r'Unsafe',
            r'Scam',
        ]
        for pattern in verdict_patterns:
            match = re.search(pattern, relay_content, flags=re.IGNORECASE)
            if match:
                verdict_from_page = match.group(0)
                break

    existing_content = result.get('content', '') or ''
    prefix_lines = []

    if trust_score_from_page is not None:
        prefix_lines.append(f"ScamAdviser Trust Score (ratingScore): {trust_score_from_page}/100")

    if verdict_from_page:
        prefix_lines.append(f"ScamAdviser Verdict: {verdict_from_page}")

    if prefix_lines:
        prefix = '\n'.join(prefix_lines)
        if prefix not in existing_content:
            result['content'] = f"{prefix}\n{existing_content}".strip()

    result['source'] = 'scamadviser'
    result['query_url'] = url
    return result


def lookup_scamwave(query: str) -> Optional[Dict]:
    """
    Search the ScamWave community scam-report database.

    Submits *query* (phone number, bank account, name, or keyword) to
    ScamWave's public search endpoint and returns the page content, which
    lists victim-reported fraud cases and blacklisted identifiers.

    Args:
        query: Search term — phone number, bank account, domain, name, or
               any fraud-related keyword.

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    import requests
    from bs4 import BeautifulSoup

    base_url = SCAM_DB_URLS['scamwave']
    clean_query = (query or '').strip()
    if not clean_query:
        return None

    headers = {
        'User-Agent': (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/126.0.0.0 Safari/537.36'
        ),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
    }

    try:
        resp = requests.get(base_url, headers=headers, timeout=20)
        resp.raise_for_status()

        html = resp.text or ''
        soup = BeautifulSoup(html, 'html.parser')
        script_tag = soup.find('script', id='tableData')
        if not script_tag or not script_tag.string:
            return {
                'source': 'scamwave',
                'query_url': base_url,
                'title': f"ScamWave lookup for {clean_query}",
                'content': 'Không tìm thấy dữ liệu bảng ScamWave (script#tableData).',
                'links': [base_url],
            }

        payload = json.loads(script_tag.string.strip())
        columns = payload.get('columns', []) or []
        data_rows = payload.get('data', []) or []

        query_l = clean_query.lower()
        query_compact = re.sub(r'[^a-z0-9]', '', query_l)

        matches = []
        for row in data_rows:
            if not isinstance(row, list):
                continue
            row_text = ' | '.join(str(cell) for cell in row if cell is not None)
            row_l = row_text.lower()
            row_compact = re.sub(r'[^a-z0-9]', '', row_l)
            if query_l in row_l or (query_compact and query_compact in row_compact):
                matches.append(row)

        if matches:
            max_preview = 25
            preview = matches[:max_preview]
            lines = [
                f"ScamWave tìm thấy {len(matches)} kết quả phù hợp cho '{clean_query}'.",
                f"Columns: {', '.join(str(c) for c in columns)}",
                "",
                "Kết quả nổi bật:",
            ]
            for row in preview:
                if columns and len(columns) == len(row):
                    cells = [f"{col}={val}" for col, val in zip(columns, row)]
                    lines.append(f"- {'; '.join(cells)}")
                else:
                    lines.append(f"- {' | '.join(str(v) for v in row)}")
            if len(matches) > max_preview:
                lines.append(f"- ... và {len(matches) - max_preview} kết quả khác")
            content = '\n'.join(lines)
        else:
            content = f"Không tìm thấy kết quả phù hợp cho '{clean_query}' trong dữ liệu ScamWave."

        return {
            'source': 'scamwave',
            'query_url': base_url,
            'title': f"ScamWave lookup for {clean_query}",
            'content': content,
            'links': [base_url],
        }
    except Exception as e:
        logger.error(f"lookup_scamwave error: {e}")
        return None


def lookup_trustpilot(domain: str) -> Optional[Dict]:
    """
    Look up a domain's reviews on Trustpilot.

    Args:
        domain: Domain name (e.g. 'example.com').

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    if not clean:
        return None
    url = f"{SCAM_DB_URLS['trustpilot']}{clean}"
    result = web_fetch_url(url)
    if result:
        result['source'] = 'trustpilot'
        result['query_url'] = url
    return result


def lookup_sitejabber(domain: str) -> Optional[Dict]:
    """
    Look up a domain's reviews on sitejabber.

    Args:
        domain: Domain name (e.g. 'example.com').

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    if not clean:
        return None
    url = f"{SCAM_DB_URLS['sitejabber']}{clean}"
    result = web_fetch_url(url)
    if result:
        result['source'] = 'sitejabber'
        result['query_url'] = url
    return result


def lookup_tranco(domain: str) -> Optional[Dict]:
    """
    Look up a domain's popularity ranking on Tranco list.

    Args:
        domain: Domain name (e.g. 'example.com').

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    import requests
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    if not clean:
        return None
    url = f"{SCAM_DB_URLS['tranco']}{clean}"
    try:
        response = requests.get(
            url,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:135.0) Gecko/20100101 Firefox/137.0"},
            timeout=10
        )
        if response.status_code == 200:
            data = response.json()
            ranks = data.get('ranks', [])
            if ranks:
                latest_rank = ranks[0].get('rank', 'N/A')
                content = f"Popularity Rank: {latest_rank}\nHistory: {json.dumps(ranks[:5])}"
            else:
                content = "No ranking data found (likely low popularity)."
            
            return {
                'source': 'tranco',
                'query_url': url,
                'title': f"Tranco Rank for {clean}",
                'content': content,
                'links': []
            }
        elif response.status_code == 404:
             return {
                'source': 'tranco',
                'query_url': url,
                'title': f"Tranco Rank for {clean}",
                'content': "Domain not found in top 1M list (Low Popularity)",
                'links': []
            }
    except Exception as e:
        logger.error(f"lookup_tranco error: {e}")
    return None



def search_agent(
    query: str,
    model: str = None,
    think=False,
    max_iterations: int = 10,
    max_context_chars: int = 8000,
) -> Optional[Dict]:
    """
    Agentic search loop that gives the model access to web_search and web_fetch
    tools, following the pattern from https://docs.ollama.com/capabilities/web-search.

    The loop continues until the model returns a non-empty final answer (no
    more tool_calls) or ``max_iterations`` is reached.

    Thinking-only turns (empty content, no tool_calls) are retried without
    appending the useless assistant message to history — preventing context
    overflow from accumulated thinking text.

    Returns:
        A dict with keys: ``answer`` (str or None) and ``searched_urls`` (list
        of URLs the agent interacted with during tool calls).
    """
    model = model or DEFAULT_MODEL
    messages: List[Dict] = [{'role': 'user', 'content': query}]
    searched_urls: List[str] = []

    agent_tools = {
        '_tool_web_search': _tool_web_search,
        '_tool_web_fetch': _tool_web_fetch,
        '_tool_lookup_scamadviser': _tool_lookup_scamadviser,
        '_tool_lookup_scamwave': _tool_lookup_scamwave,
        '_tool_lookup_trustpilot': _tool_lookup_trustpilot,
        '_tool_lookup_sitejabber': _tool_lookup_sitejabber,
        '_tool_lookup_tranco': _tool_lookup_tranco,
    }

    logger.info(f"[search_agent] START query='{query[:120]}', model={model}, "
                f"think={think}, max_iterations={max_iterations}")

    try:
        for iteration in range(max_iterations):
            logger.info(f"[search_agent iter={iteration}] calling model with "
                        f"{len(messages)} message(s)")

            chat_kwargs: Dict[str, Any] = {
                'model': model,
                'messages': messages,
                'tools': [
                    _tool_lookup_scamadviser,
                    _tool_lookup_scamwave,
                    _tool_lookup_trustpilot,
                    _tool_lookup_sitejabber,
                    _tool_lookup_tranco,
                    _tool_web_search,
                    _tool_web_fetch,
                ],
                'options': {
                    'temperature': LLM_TEMPERATURE,
                    'num_predict': 4000,  # Increased to prevent cutoff
                },
            }
            if think:
                chat_kwargs['think'] = think

            response = _chat_with_retry(**chat_kwargs)
            msg = response.message
            messages.append(message_to_dict(msg))

            content = getattr(msg, 'content', '') or ''
            tool_calls = getattr(msg, 'tool_calls', None)

            if tool_calls:
                # Append assistant message, then execute each tool
                for tool_call in tool_calls:
                    tool_name = tool_call.function.name
                    args = tool_call.function.arguments or {}
                    logger.debug(f"[search_agent iter={iteration}]   → tool '{tool_name}' "
                                f"args={json.dumps(args, ensure_ascii=False)[:200]}")

                    # Track URLs for frontend display
                    if tool_name == '_tool_web_fetch' and args.get('url'):
                        _u = args['url'].strip()
                        if _u and _u not in searched_urls:
                            searched_urls.append(_u)
                    elif tool_name == '_tool_web_search' and args.get('query'):
                        searched_urls.append(f"search:{args['query']}")
                    elif tool_name.startswith('_tool_lookup_') and (args.get('domain') or args.get('query')):
                        _lbl = tool_name.replace('_tool_lookup_', '')
                        _val = args.get('domain') or args.get('query', '')
                        searched_urls.append(f"{_lbl}:{_val}")

                    tool_func = agent_tools.get(tool_name)
                    if tool_func:
                        try:
                            result = tool_func(**args)
                            result_str = str(result)[:max_context_chars]
                        except Exception as exc:
                            result_str = f"Tool execution error: {exc}"
                            logger.error(f"[search_agent iter={iteration}]   ✗ {result_str}")
                    else:
                        result_str = f"Tool '{tool_name}' not found."
                        logger.warning(f"[search_agent iter={iteration}]   ✗ {result_str}")
                    messages.append({
                        'role': 'tool',
                        'content': result_str,
                        'tool_name': tool_name,
                    })
                # Continue loop so the model can read the tool results
                continue

            # No tool calls — check whether we actually have a final answer
            if content:
                filtered = filter_tool_call_artifacts(filter_thinking(content))
                # If the model only emitted tool-call artifacts, keep iterating.
                if not filtered:
                    continue
                _log_llm(query, filtered)
                return {'answer': filtered, 'searched_urls': searched_urls}
            else:
                # Thinking-only turn: discard (do NOT append to history) and retry
                continue

        logger.warning(f"[search_agent] Reached max_iterations={max_iterations} "
                       f"without a final answer.")
        return {'answer': None, 'searched_urls': searched_urls}

    except Exception as e:
        logger.error(f"[search_agent] error: {e}", exc_info=True)
        return {'answer': None, 'searched_urls': searched_urls}


def _extract_scam_subject(text: str) -> str:
    """
    Heuristically extract the primary subject (domain, phone, or account number)
    from a text snippet so it can be sent directly to scam databases.

    Returns the best candidate string, or the first 120 characters of *text*
    as a fallback.
    """
    # Domain / URL
    url_match = re.search(
        r'(?:https?://)?(?:www\.)?((?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,})(?:/[^\s]*)?',
        text,
    )
    if url_match:
        return url_match.group(1)

    # Vietnamese / international phone number (digits, optional +/spaces/-)
    phone_match = re.search(r'(?:\+?84|0)[\s.-]?\d[\s.-]?\d{3}[\s.-]?\d{4}', text)
    if phone_match:
        return re.sub(r'[\s.-]', '', phone_match.group(0))

    # Generic phone (7-15 digits, optional leading +)
    generic_phone = re.search(r'\+?\d[\d\s.-]{6,14}\d', text)
    if generic_phone:
        return re.sub(r'[\s.-]', '', generic_phone.group(0))

    return text[:120]


def _clean_db_snippet(db_name: str, content: str) -> str:
    """
    Clean and summarize raw content from scam databases to remove noise
    (UI artifacts, social links, generic footers) and return meaningful signals.
    """
    if not content or len(content) < 50:
        return ""

    lines = [l.strip() for l in content.splitlines() if l.strip()]
    cleaned_lines = []
    
    # Generic noise patterns to skip
    skip_patterns = [
        r'join on', r'write a review', r'latest communities', r'fighting ai scams',
        r'is this your business', r'review guidelines', r'read undefined customer reviews',
        r'smartcustomer', r'incentives or pay to remove', r'be the first to',
        r'facebook', r'reddit', r'twitter', r'instagram'
    ]
    
    for line in lines:
        if any(re.search(p, line, re.IGNORECASE) for p in skip_patterns):
            continue
        cleaned_lines.append(line)

    if not cleaned_lines:
        return ""

    # Re-join and take a meaningful snippet
    cleaned_text = "\n".join(cleaned_lines[:15])
    
    # Heuristic for "empty" database results
    if "0 reviews" in cleaned_text and "0 stars" in cleaned_text:
        return ""
    if len(cleaned_text) < 30:
        return ""
        
    return cleaned_text


def _sanitize_web_context(raw_context: str) -> str:
    """
    Normalize structured OSINT context from the agent:
    - remove malformed headings like "[Nguồn: X] cho 'domain'"
    - deduplicate repeated sources
    - keep concise, structured bullets for frontend rendering
    """
    if not raw_context:
        return ""

    text = filter_tool_call_artifacts(raw_context).strip()
    if not text:
        return ""

    if re.fullmatch(r'(?:<\s*tools\s*>\s*<\s*/\s*tools\s*>\s*)+', text, flags=re.IGNORECASE):
        return ""

    section_pattern = re.compile(r'###\s+\[(.*?)\]\s*\n([\s\S]*?)(?=\n###\s+\[|$)', re.IGNORECASE)
    sections = section_pattern.findall(text)
    if not sections:
        return text

    normalized_sections = {}
    section_order = []

    for raw_title, raw_body in sections:
        title = (raw_title or '').strip()
        body = (raw_body or '').strip()
        if not title or not body:
            continue

        title = re.sub(r'^Nguồn\s*:\s*', '', title, flags=re.IGNORECASE)
        title = re.sub(r'\]\s*cho\s*[\'\"][^\'\"]+[\'\"]\s*$', '', title, flags=re.IGNORECASE)
        title = re.sub(r'\s+cho\s*[\'\"][^\'\"]+[\'\"]\s*$', '', title, flags=re.IGNORECASE)
        title = re.sub(r'\s+', ' ', title).strip(' []')
        if not title:
            continue

        structured = ('**Đánh giá' in body or '**Đánh giá/Điểm số' in body) and ('**Chi tiết' in body)
        if not structured:
            compact = re.sub(r'\s+', ' ', body).strip()
            lower_compact = compact.lower()
            if 'no ranking data found' in lower_compact or 'domain not found in top 1m' in lower_compact:
                body = (
                    "- **Đánh giá/Điểm số**: Không có dữ liệu\n"
                    "- **Chi tiết quan trọng**: Không tìm thấy dữ liệu xếp hạng (có thể ít phổ biến)."
                )
            else:
                body = (
                    "- **Đánh giá/Điểm số**: Không có dữ liệu\n"
                    "- **Chi tiết quan trọng**: Không tìm thấy thông tin cụ thể."
                )
            structured = True

        if title not in normalized_sections:
            section_order.append(title)
            normalized_sections[title] = {'body': body, 'structured': structured}
        else:
            # Prefer structured/richer section over noisy duplicate
            existing = normalized_sections[title]
            if structured and not existing['structured']:
                normalized_sections[title] = {'body': body, 'structured': structured}

    blocks = []
    for title in section_order:
        body = normalized_sections[title]['body']
        blocks.append(f"### [{title}]\n{body}")

    return "\n\n".join(blocks).strip()


def analyze_text_for_scam(text: str, model: str = None, use_web_search: bool = False) -> Dict[str, Any]:
    """
    Analyze text for scam indicators, optionally enriching with web search.

    When ``use_web_search`` is ``True`` *and* ``OLLAMA_API_KEY`` is configured,
    the function performs two complementary enrichment steps:

    1. **Direct database lookups** — queries ScamWave, Trustpilot, Sitejabber,
       and Tranco directly for the primary subject (domain, phone, account)
       extracted from *text*.  These lookups run in parallel with the agent
       call and do not consume extra agent iterations.

    2. **Search-agent enrichment** — runs the agentic loop with an extended
       tool set that includes ``_tool_lookup_scamwave`` and other lookup tools
       so the model can proactively consult the same databases for any
       additional entities it discovers.
    """
    web_context = ""
    web_search_used = False
    agent_searched_urls: List[str] = []
    db_context_parts: List[str] = []

    if use_web_search and OLLAMA_API_KEY:
        # ------------------------------------------------------------------
        # Step 1: Direct scam-database lookups (ScamAdviser + ScamWave)
        # ------------------------------------------------------------------
        subject = _extract_scam_subject(text)
        logger.debug(f"analyze_text_for_scam: extracted subject for DB lookup: {subject!r}")

        for db_name, lookup_fn in [
            ('ScamWave',    lookup_scamwave),
            ('Trustpilot',  lookup_trustpilot),
            ('Sitejabber',  lookup_sitejabber),
            ('Tranco',      lookup_tranco),
        ]:
            try:
                db_result = lookup_fn(subject)
                if db_result and db_result.get('content'):
                    raw_content = db_result.get('content', '')
                    snippet = _clean_db_snippet(db_name, raw_content) or raw_content[:700]
                    db_context_parts.append(
                        f"### [Nguồn: {db_name}] cho '{subject}'\n{snippet}"
                    )
                    logger.debug(
                        f"analyze_text_for_scam: {db_name} returned "
                        f"{len(db_result['content'])} chars"
                    )
                else:
                    logger.debug(f"analyze_text_for_scam: {db_name} returned no content")
            except Exception as exc:
                logger.warning(f"analyze_text_for_scam: {db_name} lookup failed ({exc})")

        # Fallback if ALL databases are empty
        if not db_context_parts:
            db_context_parts.append(f"Không tìm thấy báo cáo cụ thể về '{subject}' trong các cơ sở dữ liệu lừa đảo uy tín (ScamWave, Trustpilot, Sitejabber).")

        # ------------------------------------------------------------------
        # Step 2: Agentic web-search enrichment
        # ------------------------------------------------------------------
        try:
            search_query = text[:300] + ('...' if len(text) > 300 else '')
            logger.debug(f"analyze_text_for_scam: search_agent query: {search_query!r}")

            agent_answer = search_agent(
                query=(
                    f"Kiểm tra xem trang/nội dung sau có phải là lừa đảo không: {search_query}\n"
                    "Sử dụng các công cụ theo thứ tự sau:\n"
                    f"1. _tool_lookup_scamwave    — tìm báo cáo lừa đảo về số điện thoại/tài khoản/tên trong văn bản\n"
                    f"2. _tool_lookup_trustpilot  — kiểm tra điểm số và đánh giá người dùng trên Trustpilot\n"
                    f"3. _tool_lookup_sitejabber  — kiểm tra điểm số và đánh giá người dùng trên Sitejabber\n"
                    f"4. _tool_lookup_tranco      — kiểm tra thứ hạng mức độ phổ biến (Tranco Rank). Xếp hạng càng nhỏ (vd < 1000) càng uy tín.\n"
                    "5. _tool_web_search         — tìm đánh giá trên Reddit, v.v. (dùng query 'site:reddit.com <domain>', ...)\n"
                    "QUAN TRỌNG: Tập trung vào việc kiểm tra chính trang/nội dung được cung cấp có phải là lừa đảo không.\n"
                    "KHÔNG được sao chép nguyên văn nội dung thô từ tool (menu, nút bấm, footer, danh sách link).\n"
                    "KHÔNG xuất block có dạng '### [Nguồn: ...] cho ...'. Chỉ dùng '### [Tên Nguồn]'.\n"
                    "Trình bày kết quả theo định dạng CHÍNH XÁC sau để hệ thống có thể phân tách:\n"
                    "### [Tên Nguồn]\n"
                    "- **Đánh giá/Điểm số**: ...\n"
                    "- **Chi tiết quan trọng**: Tóm tắt ngắn gọn nhất.\n\n"
                    "Ví dụ:\n"
                    "### [Trustpilot]\n"
                    "- **Đánh giá**: An toàn\n"
                    "- **Chi tiết**: ...\n\n"
                    "### [Reddit]\n"
                    "- **Đánh giá**: Nguy hiểm\n"
                    "- **Chi tiết**: ...\n\n"
                    "Nếu không có thông tin từ nguồn nào, ghi 'Không tìm thấy thông tin cụ thể'."
                ),
                model=model,
                think='medium',
                max_iterations=7,
                max_context_chars=8000,
            )

            if agent_answer and isinstance(agent_answer, dict):
                agent_text = agent_answer.get('answer', '') or ''
                agent_searched_urls = agent_answer.get('searched_urls', []) or []
                if agent_text:
                    cleaned_agent_answer = filter_tool_call_artifacts(agent_text).strip()
                    if cleaned_agent_answer:
                        sanitized_context = _sanitize_web_context(cleaned_agent_answer)
                        if sanitized_context:
                            web_context = sanitized_context
                            web_search_used = True
                            logger.debug(
                                f"analyze_text_for_scam: web context ({len(web_context)} chars): "
                                f"{web_context[:150]}..."
                            )
                        else:
                            logger.warning(
                                "analyze_text_for_scam: agent output had only tool artifacts/empty context; ignoring web_context."
                            )
                    else:
                        logger.warning(
                            "analyze_text_for_scam: agent output contained only tool-call artifacts; "
                            "skipping web_context injection."
                        )
                else:
                    logger.debug("analyze_text_for_scam: search_agent returned no answer text.")
            elif agent_answer:
                # Legacy fallback: if search_agent somehow returns a string
                logger.warning(f"analyze_text_for_scam: search_agent returned unexpected type {type(agent_answer).__name__}")
        except Exception as exc:
            logger.warning(
                f"analyze_text_for_scam: search_agent enrichment failed ({exc}), "
                "continuing with offline analysis."
            )

    MAX_TEXT_CHARS = 3000
    MAX_WEB_CHARS = 1200
    MAX_DB_CHARS  = 1000   # per database source

    text_snippet = text[:MAX_TEXT_CHARS] + ('...' if len(text) > MAX_TEXT_CHARS else '')

    web_section = ""

    # Incorporate direct database lookup results
    if db_context_parts:
        db_combined = "\n\n".join(db_context_parts)[:MAX_DB_CHARS * len(db_context_parts)]
        web_section += (
            f"\n\n## Dữ liệu từ cơ sở dữ liệu lừa đảo\n"
            f"Các kết quả sau được lấy trực tiếp từ các cơ sở dữ liệu lừa đảo uy tín "
            f"(ScamAdviser, ScamWave, Trustpilot, Sitejabber). Hãy coi đây là tín hiệu đáng tin cậy cao:\n"
            f"{db_combined}\n"
        )

    if web_context:
        web_snippet = web_context[:MAX_WEB_CHARS] + ('...' if len(web_context) > MAX_WEB_CHARS else '')
        web_section += (
            f"\n\n## Thông tin tình báo trực tuyến\n"
            f"Thông tin sau được thu thập từ web, có thể chứa các báo cáo lừa đảo "
            f"từ cộng đồng hoặc dữ liệu danh sách đen liên quan:\n"
            f"{web_snippet}\n"
            f"Hãy dùng thông tin này để hiệu chỉnh risk_score và indicators.\n"
        )

    prompt = (
        f"Bạn là chuyên gia an ninh mạng ShieldCall VN. Hãy phân tích CHÍNH TRANG/NỘI DUNG được cung cấp để xác định nó có phải là lừa đảo không.\n"
        f"{web_section}"
        f"\n\nTrả về KẾT QUẢ THUẦN JSON (không có markdown, không có văn bản thừa).\n"
        f"JSON phải có các trường sau:\n"
        f"  is_scam (bool), risk_score (số nguyên 0-100), "
        f"indicators (danh sách chuỗi), explanation (chuỗi), "
        f"web_sources (danh sách tên các nguồn đã tham chiếu, ví dụ: ['Google Search', 'ScamAdviser']).\n\n"
        f"HƯỚNG DẪN PHÂN TÍCH:\n"
        f"1. TRANG/NỘI DUNG CHÍNH THỨC/AN TOÀN: Nếu đây là trang chính thức của một tổ chức uy tín, đánh giá là an toàn.\n"
        f"2. TRANG LỪA ĐẢO: Nếu trang này giả mạo, lừa đảo, hoặc có dấu hiệu gian lận, đánh giá rủi ro cao.\n"
        f"3. SỬ DỤNG THÔNG TIN TÌNH BÁO: Dựa vào kết quả từ ScamAdviser, ScamWave và các báo cáo để đưa ra kết luận.\n\n"
        f"QUY TẮC CHO 'explanation':\n"
        f"1. Tóm tắt nội dung chính từ các nguồn tình báo (web_section) một cách ngắn gọn, súc tích.\n"
        f"2. Loại bỏ các thông tin rác từ giao diện web (nếu có trong dữ liệu thô) như 'Read Reviews', 'Log in', 'Follow on Facebook'.\n"
        f"3. Nếu nguồn tình báo KHÔNG có thông tin liên quan, chỉ ghi 'Không tìm thấy dữ liệu từ các nguồn uy tín'.\n"
        f"4. Viết bằng TIẾNG VIỆT, tập trung vào kết luận rủi ro.\n\n"
        f"Nội dung cần phân tích:\n{text_snippet}"
    )

    response = generate_response(
        prompt, model, num_ctx=8192, max_tokens=2048, tools=[], format_schema=SCAM_ANALYSIS_SCHEMA
    )

    db_sources_consulted = ['scamadviser', 'scamwave', 'trustpilot', 'sitejabber', 'tranco'] if db_context_parts else []

    # Frontend should display only structured intelligence context,
    # not raw database page dumps (which can be noisy).
    combined_web_context = (web_context or '').strip()
    if not combined_web_context and db_context_parts:
        combined_web_context = (
            "### [Tổng hợp nguồn dữ liệu]\n"
            "- **Đánh giá**: Không có dữ liệu tình báo trực tuyến rõ ràng\n"
            "- **Chi tiết**: Không tìm thấy thông tin cụ thể từ các nguồn đã kiểm tra."
        )

    retry_notice = ''
    retry_count = 0
    parsed_result = _extract_json(response) if response else None

    if response and not parsed_result:
        retry_notice = "⏳ AI cần thêm chút thời gian để chuẩn hóa kết quả, vui lòng chờ trong giây lát."
        logger.warning(
            "analyze_text_for_scam: JSON parse failed on first pass; starting repair retry. Raw response: %s",
            response[:300],
        )
        retry_count += 1
        repair_prompt = (
            "Bạn là bộ sửa JSON. Hãy chuyển nội dung sau thành JSON hợp lệ theo schema:\n"
            "{is_scam: bool, risk_score: int 0-100, indicators: [string], explanation: string, web_sources: [string]}\n"
            "Chỉ trả JSON thuần, không markdown, không giải thích.\n\n"
            "Nội dung cần sửa:\n"
            f"{response[:2500]}"
        )
        repaired = generate_response(
            repair_prompt,
            model,
            num_ctx=4096,
            max_tokens=1024,
            tools=[],
            format_schema=SCAM_ANALYSIS_SCHEMA,
        )
        parsed_result = _extract_json(repaired) if repaired else None

    if not parsed_result:
        retry_count += 1
        logger.warning("analyze_text_for_scam: Repair retry failed; rerunning primary analysis once.")
        response_retry = generate_response(
            prompt,
            model,
            num_ctx=8192,
            max_tokens=2048,
            tools=[],
            format_schema=SCAM_ANALYSIS_SCHEMA,
        )
        parsed_result = _extract_json(response_retry) if response_retry else None

    if parsed_result:
        result = parsed_result
        result['risk_score'] = int(max(0, min(100, result.get('risk_score', 0) or 0)))
        if not isinstance(result.get('indicators'), list):
            result['indicators'] = []
        if not isinstance(result.get('web_sources'), list):
            result['web_sources'] = []
        result['web_context'] = combined_web_context
        result['web_search_used'] = web_search_used
        result['searched_urls'] = agent_searched_urls
        result['db_sources_consulted'] = db_sources_consulted

        # Ensure web_sources includes db sources consulted
        existing_sources = result.get('web_sources', []) or []
        existing_sources_l = [str(s).lower() for s in existing_sources]
        if db_sources_consulted:
            for src in db_sources_consulted:
                if src not in existing_sources_l:
                    existing_sources.append(src.capitalize())
            result['web_sources'] = existing_sources

        if retry_notice and retry_count > 0:
            result['ai_retry_used'] = True
            result['ai_retry_count'] = retry_count
            result['ai_notice'] = retry_notice
            explanation = str(result.get('explanation', '') or '')
            if retry_notice not in explanation:
                result['explanation'] = f"{retry_notice} {explanation}".strip()
        else:
            result['ai_retry_used'] = False
            result['ai_retry_count'] = 0

        return result

    logger.warning(
        "analyze_text_for_scam: all JSON parsing/retry attempts failed; returning fallback payload."
    )

    return {
        "is_scam": False,
        "risk_score": 0,
        "indicators": [],
        "explanation": "⏳ AI cần thêm chút thời gian. Hệ thống đang tạm trả kết quả an toàn để tránh treo phiên phân tích.",
        "web_sources": [s.capitalize() for s in db_sources_consulted] if db_sources_consulted else ["Offline Analysis"],
        "web_context": combined_web_context,
        "web_search_used": web_search_used,
        "searched_urls": agent_searched_urls,
        "db_sources_consulted": db_sources_consulted,
        "ai_retry_used": True,
        "ai_retry_count": retry_count,
        "ai_notice": "⏳ AI cần thêm chút thời gian để chuẩn hóa kết quả, vui lòng thử lại sau vài giây.",
    }
