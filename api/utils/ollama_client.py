"""
Ollama LLM Integration using the official 'ollama' Python library.
Connects to the configured Ollama endpoint.
Settings are read from django.conf.settings:
  - OLLAMA_BASE_URL
  - LLM_MODEL
  - LLM_TEMPERATURE
  - LLM_MAX_TOKENS
"""

import json
import logging
import os
import re
from typing import Optional, Dict, Any, Generator, List

from django.conf import settings
import ollama
from ollama._types import ResponseError

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Read from Django settings with sensible defaults
OLLAMA_BASE_URL = getattr(settings, 'OLLAMA_BASE_URL', 'http://localhost:11434')
OLLAMA_API_KEY = getattr(settings, 'OLLAMA_API_KEY', None)

# Mirror OLLAMA_API_KEY into the environment for any ollama library internals
# that may read it (e.g. the search_agent client.chat call).
if OLLAMA_API_KEY:
    os.environ.setdefault('OLLAMA_API_KEY', OLLAMA_API_KEY)
DEFAULT_MODEL = getattr(settings, 'LLM_MODEL', 'neural-chat')
SMALL_MODEL = getattr(settings, 'SMALL_MODEL', DEFAULT_MODEL)
LLM_TEMPERATURE = getattr(settings, 'LLM_TEMPERATURE', 0.3)
LLM_MAX_TOKENS = getattr(settings, 'LLM_MAX_TOKENS', 1800)
DEBUG_LLM = getattr(settings, 'DEBUG_LLM', True)  # Default True for ShieldCall Debugging

# ---------------------------------------------------------------------------
# Custom tool schemas for the search agent.
# We define our OWN plain function objects so the model receives clean,
# auth-free tool specs.  When the model requests these tools we dispatch
# to web_search_query() / web_fetch_url() ourselves â€” the model never sees
# any authentication metadata and therefore never refuses to call them.
# ---------------------------------------------------------------------------

def _tool_web_search(query: str, max_results: int = 5):
    """Search the web for pages matching a query.

    Args:
        query: The search query string.
        max_results: Maximum number of results to return (default 5).

    Returns:
        A list of search results with title, url, and content fields.
    """
    return web_search_query(query, max_results=max_results)


def _tool_web_fetch(url: str):
    """Fetch the content of a web page by URL.

    Args:
        url: The URL of the web page to fetch.

    Returns:
        A dict containing title, content, and links of the fetched page.
    """
    return web_fetch_url(url)


def _tool_lookup_scamadviser(domain: str):
    """Check a domain or website against ScamAdviser's trust score database.

    Use this tool whenever you need to assess whether a website or domain is
    safe, suspicious, or a known scam site. ScamAdviser provides automated
    trust scores, registration details, hosting country, and risk indicators.

    Args:
        domain: The domain name to look up, e.g. 'example.com'. Strip any
                leading 'http://' or 'https://' scheme before passing.

    Returns:
        A dict with title and content fields containing the ScamAdviser trust
        assessment and risk indicators for the domain.
    """
    return lookup_scamadviser(domain)


def _tool_lookup_scamwave(query: str):
    """Search the ScamWave scammer database for reports matching a query.

    Use this tool to look up phone numbers, bank account numbers, names, or
    keywords in the ScamWave community-sourced scam report database. It
    surfaces victim-reported fraud cases, blacklisted numbers, and warnings.

    Args:
        query: Phone number, bank account number, full name, or keyword to
               search for in the ScamWave database.

    Returns:
        A dict with title and content fields containing matching scam reports
        and listings from the ScamWave database.
    """
    return lookup_scamwave(query)


def _tool_lookup_trustpilot(domain: str):
    """Check a domain's reputation and user reviews on Trustpilot.

    Use this tool to find user feedback, star ratings, and complaint details
    for a specific domain. Trustpilot is a major source for verifying legitimacy.

    Args:
        domain: The domain name to check (e.g. 'example.com').
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    return web_fetch_url(f"https://www.trustpilot.com/review/{clean}")


def _tool_lookup_sitejabber(domain: str):
    """Check a domain's reputation and user reviews on sitejabber.

    Use this tool to find consumer reviews and business ratings. sitejabber
    often has reviews for smaller online businesses that Trustpilot might miss.

    Args:
        domain: The domain name to check (e.g. 'example.com').
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    return web_fetch_url(f"https://www.sitejabber.com/reviews/{clean}")


def _tool_lookup_tranco(domain: str):
    """Check a domain's popularity ranking on Tranco list.

    Args:
        domain: Domain name (e.g. 'example.com').

    Returns:
        A dict with rank and history if available.
    """
    return lookup_tranco(domain)


# Map tool name -> our Python dispatcher (used in search_agent loop)
WEB_TOOLS: Dict[str, Any] = {
    '_tool_web_search': _tool_web_search,
    '_tool_web_fetch': _tool_web_fetch,
    '_tool_lookup_scamwave': _tool_lookup_scamwave,
    '_tool_lookup_trustpilot': _tool_lookup_trustpilot,
    '_tool_lookup_sitejabber': _tool_lookup_sitejabber,
    '_tool_lookup_tranco': _tool_lookup_tranco,
}

# Tool definitions for ShieldCall AI
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "scan_phone",
            "description": "Scan a phone number for scam/risk reports.",
            "parameters": {
                "type": "object",
                "properties": {
                    "phone": {"type": "string", "description": "The phone number to scan"}
                },
                "required": ["phone"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "scan_url",
            "description": "Scan a domain or URL for phishing/malware risk.",
            "parameters": {
                "type": "object",
                "properties": {
                    "url": {"type": "string", "description": "The domain or URL to scan"}
                },
                "required": ["url"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "scan_bank_account",
            "description": "Scan a bank account number for fraud reports.",
            "parameters": {
                "type": "object",
                "properties": {
                    "account_number": {"type": "string", "description": "The bank account number"},
                    "bank_name": {"type": "string", "description": "Optional bank name (e.g. MB, VCB)"}
                },
                "required": ["account_number"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "Search the internet for real-time information and news.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "The search query"},
                    "max_results": {"type": "integer", "description": "Max results (1-10)"}
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "web_fetch",
            "description": "Fetch and read the content of a specific web page.",
            "parameters": {
                "type": "object",
                "properties": {
                    "url": {"type": "string", "description": "The URL to fetch"}
                },
                "required": ["url"]
            }
        }
    }
]

# ---------------------------------------------------------------------------
# STRUCTURED OUTPUT SCHEMAS
# ---------------------------------------------------------------------------

# Schema for structured classification output
CLASSIFICATION_SCHEMA = {
    "type": "object",
    "properties": {
        "classification": {"type": "string", "enum": ["safe", "suspicious", "dangerous"]},
        "suggested_action": {"type": "string", "enum": ["NONE", "BLOCK", "REPORT"]},
        "confidence": {"type": "number"}
    },
    "required": ["classification", "suggested_action", "confidence"]
}

# Schema for structured scam analysis output
SCAM_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "is_scam": {"type": "boolean"},
        "risk_score": {"type": "integer"},
        "indicators": {"type": "array", "items": {"type": "string"}},
        "explanation": {"type": "string"},
        "web_sources": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["is_scam", "risk_score", "indicators", "explanation", "web_sources"]
}

# Initialize client
# Auto-correct bare 'ollama.com' to 'api.ollama.com' â€” the former is the
# website, the latter is the Ollama cloud API endpoint used by the Python SDK.
_effective_base_url = OLLAMA_BASE_URL
if OLLAMA_API_KEY and 'ollama.com' in OLLAMA_BASE_URL and 'api.ollama.com' not in OLLAMA_BASE_URL:
    _effective_base_url = 'https://api.ollama.com'
    logger.info(
        f"OLLAMA_BASE_URL '{OLLAMA_BASE_URL}' corrected to '{_effective_base_url}' "
        f"(cloud API endpoint required for authenticated models)."
    )

client_kwargs = {'host': _effective_base_url}
if OLLAMA_API_KEY:
    client_kwargs['headers'] = {
        'Authorization': f'Bearer {OLLAMA_API_KEY}',
    }

client = ollama.Client(**client_kwargs)

# ---------------------------------------------------------------------------
# Retry helper â€” wraps client.chat() with automatic retries on 503 / network
# errors.  Max 3 attempts with exponential backoff (2s, 4s).
# ---------------------------------------------------------------------------
import time as _time

def _chat_with_retry(max_retries: int = 4, **chat_kwargs):
    """Call client.chat() with retry on 503 / transient errors."""
    last_exc = None
    for attempt in range(1, max_retries + 1):
        try:
            return client.chat(**chat_kwargs)
        except Exception as e:
            last_exc = e
            err_str = str(e)
            # Match 503, 502, 504 (timeout), 429 (rate limit)
            is_retryable = any(tok in err_str for tok in ['503', '502', '504', '429', 'Service Unavailable',
                                                           'temporarily unavailable', 'overloaded',
                                                           'Connection', 'Timeout', 'remote end closed'])
            if is_retryable and attempt < max_retries:
                wait = 2 ** attempt  # 2s, 4s, 8s
                logger.warning(f"[Ollama] Attempt {attempt}/{max_retries} failed ({err_str[:120]}), "
                               f"retrying in {wait}s...")
                _time.sleep(wait)
                continue
            
            logger.error(f"[Ollama] Final attempt {attempt} failed: {err_str}")
            raise last_exc


def _chat_stream_with_retry(max_retries: int = 4, **chat_kwargs):
    """Call client.chat(stream=True) with retry on 503 / transient errors.
    Returns the streaming iterator."""
    chat_kwargs['stream'] = True
    last_exc = None
    for attempt in range(1, max_retries + 1):
        try:
            return client.chat(**chat_kwargs)
        except Exception as e:
            last_exc = e
            err_str = str(e)
            is_retryable = any(tok in err_str for tok in ['503', '502', '504', '429', 'Service Unavailable',
                                                           'temporarily unavailable', 'overloaded',
                                                           'Connection', 'Timeout', 'remote end closed'])
            if is_retryable and attempt < max_retries:
                wait = 2 ** attempt
                logger.warning(f"[Ollama] Stream attempt {attempt}/{max_retries} failed ({err_str[:120]}), "
                               f"retrying in {wait}s...")
                _time.sleep(wait)
                continue
            
            logger.error(f"[Ollama] Final stream attempt {attempt} failed: {err_str}")
            raise last_exc


# Specialized logger for LLM traffic
llm_logger = logging.getLogger('pkv.llm')
llm_logger.setLevel(logging.INFO)
if not llm_logger.handlers:
    fh = logging.FileHandler('llm_access.log')
    fh.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
    llm_logger.addHandler(fh)

def _log_llm(prompt: str, response: str, system_prompt: str = None):
    """Detailed logging of LLM interactions"""
    log_data = {
        'system_prompt': system_prompt,
        'prompt_length': len(prompt),
        'prompt_preview': prompt[:200] + '...' if len(prompt) > 200 else prompt,
        'response_length': len(response) if response else 0,
        'response_preview': response[:200] + '...' if response else ""
    }
    llm_logger.info(f"LLM Call: {json.dumps(log_data, ensure_ascii=False)}")
    
def filter_thinking(text: str) -> str:
    """
    Removes igid blocks from model responses.
    Used for reasoning-capable models like DeepSeek.
    """
    if not text:
        return ""
    # Remove content between igid and igid tags
    return re.sub(r'igid.*?igid', '', text, flags=re.DOTALL).strip()


def filter_hallucinations(text: str) -> str:
    """
    Filter out common AI hallucinations like unexpected CJK characters
    in a non-CJK context, especially the 'strange unicode' characters
    reported by the user (e.g., è‡ªç§°).
    """
    if not text:
        return text
    # Strip CJK Unified Ideographs if found in isolation or in strange spots
    # Range: \u4e00-\u9fff (Common Chinese)
    # We mainly target the "strange" ones like è‡ªç§° \u81ea\u79f0
    text = re.sub(r'[\u4e00-\u9fff]+', '', text)
    return text


def filter_tool_call_artifacts(text: str) -> str:
    """
    Remove accidental tool-call JSON artifacts from agent textual output.

    Some models occasionally emit raw objects like:
    {"name":"_tool_lookup_scamadviser","arguments":{...}}
    in plain content instead of structured ``tool_calls``.
    """
    if not text:
        return ""

    cleaned = text

    # Remove XML-like empty tool blocks emitted by some models.
    cleaned = re.sub(r'<\s*tools\s*>\s*<\s*/\s*tools\s*>', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'</?\s*tools\s*>', '', cleaned, flags=re.IGNORECASE)

    # Remove compact/raw tool call JSON objects (single or multiple objects).
    cleaned = re.sub(
        r'\{\s*"name"\s*:\s*"_tool_[^\"]+"\s*,\s*"arguments"\s*:\s*\{[^{}]*\}\s*\}\s*',
        '',
        cleaned,
        flags=re.DOTALL,
    )

    # Remove any line that still contains explicit tool invocations.
    cleaned = "\n".join(
        line for line in cleaned.splitlines()
        if '_tool_' not in line and '"arguments"' not in line
    )

    # Normalize excessive blank lines.
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    return cleaned.strip()

def _extract_json(text: str) -> Optional[dict]:
    """
    Extract and parse the first JSON object from an LLM response.

    Handles markdown fenced code blocks, trailing commas, and ASCII control
    characters.  Works cleanly on `message.content` which, when the official
    chat-loop pattern is used with think=True, contains only the final answer
    (no thinking prose to confuse the extractor).
    """
    if not text:
        return None

    # 1. Prefer JSON inside a markdown code fence
    fence_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
    candidate = fence_match.group(1) if fence_match else None

    if candidate is None:
        # 2. Outermost { â€¦ } block
        brace_match = re.search(r'\{.*\}', text, re.DOTALL)
        if not brace_match:
            return None
        candidate = brace_match.group(0)

    # Strip ASCII control characters
    candidate = re.sub(r'[\x00-\x1F\x7F]', ' ', candidate)

    try:
        return json.loads(candidate)
    except json.JSONDecodeError:
        # Remove trailing commas (common model mistake) and retry
        fixed = re.sub(r',\s*([}\]])', r'\1', candidate)
        try:
            return json.loads(fixed)
        except json.JSONDecodeError:
            return None


def message_to_dict(msg) -> Dict[str, Any]:
    """
    Convert an ollama ``Message`` object (or any mapping) to a plain Python
    dict suitable for JSON serialisation, logging, or re-inserting into a
    messages list.

    Only fields that are actually set are included so the output stays clean.
    Handles:
      - ``role``        (str, always present)
      - ``content``     (str, omitted when None/empty)
      - ``thinking``    (str, omitted when None/empty)
      - ``tool_name``   (str, omitted when None)
      - ``tool_calls``  (list of dicts, omitted when empty/None)
      - ``images``      (list, omitted when empty/None)

    ``ToolCall`` objects are recursively converted to::

        {"function": {"name": "...", "arguments": {...}}}
    """
    # Already a plain dict â€” normalise and return
    if isinstance(msg, dict):
        return {k: v for k, v in msg.items() if v is not None}

    d: Dict[str, Any] = {'role': msg.role}

    content = getattr(msg, 'content', None)
    if content:
        d['content'] = content

    thinking = getattr(msg, 'thinking', None)
    if thinking:
        d['thinking'] = thinking

    tool_name = getattr(msg, 'tool_name', None)
    if tool_name:
        d['tool_name'] = tool_name

    tool_calls = getattr(msg, 'tool_calls', None)
    if tool_calls:
        serialised = []
        for tc in tool_calls:
            if isinstance(tc, dict):
                serialised.append(tc)
            else:
                # ollama ToolCall object
                fn = tc.function
                serialised.append({
                    'function': {
                        'name': fn.name,
                        'arguments': dict(fn.arguments) if fn.arguments else {},
                    }
                })
        d['tool_calls'] = serialised

    images = getattr(msg, 'images', None)
    if images:
        d['images'] = list(images)

    return d


def is_ollama_available() -> bool:
    """Check if Ollama service is running."""
    try:
        client.list()
        return True
    except Exception as e:
        logger.warning(f"Ollama not available: {e}")
        return False

def get_available_models() -> list:
    """Get list of available models in Ollama."""
    try:
        models = client.list()
        return [m['name'] for m in models['models']]
    except Exception as e:
        logger.error(f"Error getting Ollama models: {e}")
    return []

def generate_response(
    prompt: str,
    model: str = None,
    system_prompt: str = None,
    num_ctx: int = None,
    max_tokens: int = None,
    tools: list = None,
    tool_dispatch: Dict[str, Any] = None,
    format_schema: dict = None,
    max_loop_iterations: int = 15,
    skip_filter: bool = False,
) -> Optional[str]:
    """
    Generate a non-streamed response from Ollama.

    When ``format_schema`` is provided, the model is guaranteed to return
    valid JSON matching the schema, simplifying the loop logic.

    Args:
        skip_filter: If True, skip filter_hallucinations on the output.
                     Use for long-form content where CJK stripping could corrupt text.
    """
    model = model or DEFAULT_MODEL
    _effective_max_tokens = max_tokens or LLM_MAX_TOKENS
    logger.info(f"[generate_response] START model={model}, max_tokens={_effective_max_tokens}, "
                f"format_schema={'yes' if format_schema else 'no'}, "
                f"tools_param={'explicit' if tools is not None else 'default'}, "
                f"skip_filter={skip_filter}, prompt_len={len(prompt)}")
    # When format_schema is set, skip tools to avoid conflict
    # (structured JSON output and tool calling are mutually exclusive)
    if format_schema:
        tools = None
        logger.info(f"[generate_response] format_schema provided â†’ tools disabled")
    elif tools is None:
        tools = TOOLS
    tool_dispatch = tool_dispatch or {}

    try:
        messages: List[Dict] = []
        if system_prompt:
            messages.append({'role': 'system', 'content': system_prompt})
        messages.append({'role': 'user', 'content': prompt})

        options: Dict[str, Any] = {
            'temperature': LLM_TEMPERATURE,
            'num_predict': _effective_max_tokens,
        }
        if num_ctx:
            options['num_ctx'] = num_ctx

        logger.info(f"[generate_response] options={options}, tools_count={len(tools) if tools else 0}")

        reply = ''
        response = None
        iteration = 0

        while iteration < max_loop_iterations:
            logger.info(f"[generate_response iter={iteration}] calling model '{model}' "
                        f"with {len(messages)} message(s), tools={'yes' if tools else 'no'}")

            chat_kwargs: Dict[str, Any] = {
                'model': model,
                'messages': messages,
                'options': options,
                'think': "low",
            }
            if tools:
                chat_kwargs['tools'] = tools
            if format_schema:
                chat_kwargs['format'] = format_schema

            response = _chat_with_retry(**chat_kwargs)
            assistant_msg = message_to_dict(response.message)
            messages.append(assistant_msg)

            part = response.message.content or ''
            tool_calls = response.message.tool_calls

            # Debug: log raw response details
            _done_reason = getattr(response, 'done_reason', 'N/A')
            _eval_count = getattr(response, 'eval_count', 'N/A')
            _prompt_eval_count = getattr(response, 'prompt_eval_count', 'N/A')
            logger.info(f"[generate_response iter={iteration}] raw response: "
                        f"content_len={len(part)}, tool_calls={len(tool_calls) if tool_calls else 0}, "
                        f"done_reason={_done_reason}, eval_count={_eval_count}, "
                        f"prompt_eval_count={_prompt_eval_count}")

            iteration += 1

            if tool_calls:
                for tool_call in tool_calls:
                    tool_name = tool_call.function.name
                    args = tool_call.function.arguments or {}
                    logger.info(f"[generate_response iter={iteration}] ðŸ”§ tool call: {tool_name}({list(args.keys())})")
                    tool_func = tool_dispatch.get(tool_name)
                    result_str = tool_func(**args) if tool_func else f"Tool '{tool_name}' not available."
                    logger.info(f"[generate_response iter={iteration}] ðŸ”§ tool result len={len(str(result_str))}")
                    messages.append({'role': 'tool', 'content': result_str, 'tool_name': tool_name})
                iteration += 1
                continue
            elif part:
                reply = part.strip()
                logger.info(f"[generate_response iter={iteration}] âœ… final answer (no tools), "
                            f"reply_len={len(reply)}, exiting loop")
                break
            else:
                logger.warning(f"[generate_response iter={iteration}] âš ï¸ empty content and no tool calls")

        if iteration >= max_loop_iterations:
            logger.warning(f"[generate_response] hit max_loop_iterations={max_loop_iterations}, returning best content so far")

        _log_llm(prompt, reply, system_prompt)

        # When format_schema is set or skip_filter requested, do NOT apply
        # filter_hallucinations as it strips CJK characters and can corrupt output.
        if format_schema or skip_filter:
            logger.info(f"[generate_response] returning reply len={len(reply)} (no filter), preview={reply[:200]!r}")
            return reply

        filtered = filter_hallucinations(reply)
        if len(filtered) != len(reply):
            logger.warning(f"[generate_response] filter_hallucinations changed len: {len(reply)} â†’ {len(filtered)} "
                           f"(diff={len(reply) - len(filtered)})")
        logger.info(f"[generate_response] returning reply len={len(filtered)}, preview={filtered[:200]!r}")
        return filtered

    except Exception as e:
        logger.error(f"Error calling Ollama: {e}", exc_info=True)
        return None

def stream_response(prompt: str, system_prompt: str = None, model: str = None, max_tokens: int = None) -> Generator[str, None, None]:
    """
    Stream a response from Ollama token-by-token.
    Properly handles thinking models (qwen3, deepseek, etc.) by using
    think="low" and separating thinking vs content chunks.

    Args:
        max_tokens: Override num_predict (default LLM_MAX_TOKENS). Use higher
                    values for long-form content generation.
    """
    model = model or DEFAULT_MODEL
    messages = []
    if system_prompt:
        messages.append({'role': 'system', 'content': system_prompt})
    messages.append({'role': 'user', 'content': prompt})

    try:
        stream = _chat_stream_with_retry(
            model=model,
            messages=messages,
            options={
                'temperature': LLM_TEMPERATURE,
                'num_predict': max_tokens or LLM_MAX_TOKENS,
            },
            think="low",
        )

        full_response = []

        for chunk in stream:
            # Handle both dict-like and object-like access (ollama sdk versions)
            msg = getattr(chunk, 'message', None)
            if msg is None and isinstance(chunk, dict):
                msg = chunk.get('message', {})

            if not msg:
                continue

            # Thinking tokens â€” skip, just signal status
            thinking = getattr(msg, 'thinking', '') or ''
            if not thinking and isinstance(msg, dict):
                thinking = msg.get('thinking', '') or ''
            if thinking:
                if not full_response:
                    yield "__STATUS__:thinking"
                continue

            # Content tokens â€” yield immediately
            content = getattr(msg, 'content', '') or ''
            if not content and isinstance(msg, dict):
                content = msg.get('content', '') or ''

            if content:
                yield content
                full_response.append(content)

        _log_llm(prompt, "".join(full_response), system_prompt)

    except Exception as e:
        logger.error(f"Error in Ollama stream: {e}", exc_info=True)
        yield f"Lá»—i káº¿t ná»‘i AI: {str(e)}"


# ---------------------------------------------------------------------------
# AI Assistant tool wrappers (parameter names match TOOLS definitions)
# ---------------------------------------------------------------------------

def _assistant_scan_phone(phone: str):
    """Scan a phone number: check ScamWave then web search for reports."""
    results = {}
    sw = _tool_lookup_scamwave(query=phone)
    if sw and sw.get('content'):
        results['scamwave'] = sw['content'][:2000]
    ws = web_search_query(f"lá»«a Ä‘áº£o sá»‘ Ä‘iá»‡n thoáº¡i {phone} scam report", max_results=3)
    if ws:
        results['web_results'] = ws
    return results or f"KhÃ´ng tÃ¬m tháº¥y bÃ¡o cÃ¡o lá»«a Ä‘áº£o vá» sá»‘ {phone}."


def _assistant_scan_url(url: str):
    """Scan a URL/domain: check ScamAdviser, Trustpilot, Tranco and web search."""
    clean = re.sub(r'^https?://', '', url).split('/')[0].strip()
    results = {}
    sa = lookup_scamadviser(clean)
    if sa and sa.get('content'):
        results['scamadviser'] = sa['content'][:2000]
    tp = _tool_lookup_trustpilot(domain=clean)
    if tp and tp.get('content'):
        results['trustpilot'] = tp['content'][:2000]
    tr = lookup_tranco(clean)
    if tr:
        results['tranco'] = tr
    ws = web_search_query(f"{clean} scam lá»«a Ä‘áº£o review", max_results=3)
    if ws:
        results['web_results'] = ws
    return results or f"KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin rá»§i ro vá» {url}."


def _assistant_scan_bank_account(account_number: str, bank_name: str = ""):
    """Scan a bank account number for fraud reports on ScamWave + web."""
    query = f"{account_number} {bank_name}".strip()
    results = {}
    sw = _tool_lookup_scamwave(query=query)
    if sw and sw.get('content'):
        results['scamwave'] = sw['content'][:2000]
    ws = web_search_query(f"lá»«a Ä‘áº£o tÃ i khoáº£n ngÃ¢n hÃ ng {query} scam", max_results=3)
    if ws:
        results['web_results'] = ws
    return results or f"KhÃ´ng tÃ¬m tháº¥y bÃ¡o cÃ¡o lá»«a Ä‘áº£o vá» tÃ i khoáº£n {account_number}."


def stream_chat_ai(messages: list, model: str = None, tool_dispatch: dict = None, debug: bool = False) -> Generator[str, None, None]:
    """
    Improved streaming chat for ShieldCall AI.
    Handles multi-turn tool loops, thinking streaming, and search result yielding.
    """
    if debug or DEBUG_LLM:
        yield f"__DEBUG_MODEL__:{model or DEFAULT_MODEL}"
        # Small summary of history
        yield f"__DEBUG_HISTORY_COUNT__:{len(messages)}"
    model = model or DEFAULT_MODEL
    tool_dispatch = tool_dispatch or {}
    # Combine with standard tools
    all_dispatch = {
        'web_search': web_search_query,
        'web_fetch': web_fetch_url,
        'scan_phone': _assistant_scan_phone,
        'scan_url': _assistant_scan_url,
        'scan_bank_account': _assistant_scan_bank_account,
    }
    all_dispatch.update(tool_dispatch)

    max_loops = 10
    iteration = 0
    done = False

    while iteration < max_loops and not done:
        max_stream_retries = 3
        stream_attempt = 0
        stream_success = False
        
        while stream_attempt < max_stream_retries and not stream_success:
            stream_attempt += 1
            try:
                chat_params = {
                    'model': model,
                    'messages': messages,
                    'options': {
                        'temperature': LLM_TEMPERATURE,
                        'num_predict': LLM_MAX_TOKENS,
                    },
                    'stream': True,
                    'think': 'low'  # Low thinking for faster responses
                }
                if TOOLS:
                    chat_params['tools'] = TOOLS

                stream = _chat_stream_with_retry(**chat_params)
                
                current_tool_calls = []
                current_content = []
                is_thinking = False

                for chunk in stream:
                    # Handle both dict-like and object-like access for chunk/message
                    msg = getattr(chunk, 'message', {})
                    if not msg and hasattr(chunk, 'get'):
                        msg = chunk.get('message', {})

                    # Handle Thinking
                    thinking = getattr(msg, 'thinking', "")
                    if not thinking and hasattr(msg, 'get'):
                        thinking = msg.get('thinking', "")
                    
                    if thinking:
                        yield f"__THINK__:{thinking}"
                    
                    # Handle Content
                    content = getattr(msg, 'content', "")
                    if not content and hasattr(msg, 'get'):
                        content = msg.get('content', "")
                    
                    if content:
                        content = filter_hallucinations(content)
                        if content: # Could be empty after filtering
                            yield content
                            current_content.append(content)

                    # Collect Tool Calls
                    tool_calls = getattr(msg, 'tool_calls', [])
                    if not tool_calls and hasattr(msg, 'get'):
                        tool_calls = msg.get('tool_calls', [])
                    
                    if tool_calls:
                        current_tool_calls.extend(tool_calls)

                # Mark stream as successfully completed
                stream_success = True

                # If no tool calls, we are done â€” exit both loops
                if not current_tool_calls:
                    _log_llm("Chat history...", "".join(current_content))
                    done = True
                    break

                # Handle Tool Calls
                messages.append({'role': 'assistant', 'content': "".join(current_content), 'tool_calls': current_tool_calls})
                
                serializable_calls = []
                for tc in current_tool_calls:
                    tool_name = tc.function.name
                    args = tc.function.arguments or {}
                    
                    # Yield marker for UI
                    yield f"__STATUS__:executing_tool:{tool_name}"
                    
                    serializable_calls.append({
                        'function': {'name': tool_name, 'arguments': args}
                    })

                    # Execute
                    tool_func = all_dispatch.get(tool_name)
                    if tool_func:
                        try:
                            result = tool_func(**args)
                            
                            # Special handling for search results to show in UI
                            if tool_name == 'web_search' and isinstance(result, list):
                                yield f"__SEARCH_RESULTS__:{json.dumps(result)}"
                            
                            if result is None:
                                result_str = "KhÃ´ng cÃ³ káº¿t quáº£."
                            elif isinstance(result, (dict, list)):
                                result_str = json.dumps(result, ensure_ascii=False)
                                if len(result_str) > 8000:
                                    result_str = result_str[:8000] + "..."
                            else:
                                result_str = str(result)[:8000]
                        except Exception as e:
                            logger.error(f"Tool {tool_name} execution error: {e}")
                            result_str = f"Lá»—i khi thá»±c thi cÃ´ng cá»¥: {str(e)}"
                    else:
                        result_str = f"Tool '{tool_name}' not found."

                    messages.append({
                        'role': 'tool',
                        'content': result_str,
                        'tool_name': tool_name
                    })

                yield f"__TOOL_CALLS__:{json.dumps(serializable_calls)}"
                iteration += 1
                continue

            except ResponseError as e:
                # Handle 503 Service Unavailable errors with retry
                if '503' in str(e) or 'Service Unavailable' in str(e) or 'Service Temporarily Unavailable' in str(e):
                    if stream_attempt < max_stream_retries:
                        wait_time = 2 ** stream_attempt
                        logger.warning(f"[Ollama] Stream iteration attempt {stream_attempt}/{max_stream_retries} "
                                     f"failed (503 Service Unavailable), retrying in {wait_time}s...")
                        import time as _time
                        _time.sleep(wait_time)
                        continue  # Retry the stream
                    else:
                        # All retries exhausted
                        logger.error(f"[Ollama] All stream retries exhausted: {e}")
                        yield "Xin lá»—i, há»‡ thá»‘ng AI táº¡m thá»i quÃ¡ táº£i. Vui lÃ²ng thá»­ láº¡i sau."
                        return
                else:
                    # Non-503 ResponseError, treat as normal error
                    logger.error(f"Error in stream_chat_ai: {e}", exc_info=True)
                    yield f"ÄÃ£ xáº£y ra lá»—i khi xá»­ lÃ½. Vui lÃ²ng thá»­ láº¡i."
                    return

            except Exception as e:
                # Other exceptions - check if retryable
                err_str = str(e)
                is_retryable = any(tok in err_str for tok in ['503', '502', '504', 'Service Unavailable',
                                                               'temporarily unavailable', 'overloaded'])
                if is_retryable and stream_attempt < max_stream_retries:
                    wait_time = 2 ** stream_attempt
                    logger.warning(f"[Ollama] Stream attempt {stream_attempt}/{max_stream_retries} "
                                 f"failed ({err_str[:100]}), retrying in {wait_time}s...")
                    import time as _time
                    _time.sleep(wait_time)
                    continue  # Retry the stream
                else:
                    # Non-retryable error or retries exhausted
                    logger.error(f"Error in stream_chat_ai: {e}", exc_info=True)
                    if debug or DEBUG_LLM:
                        yield f"__DEBUG_ERROR__:{str(e)}"
                    # Don't expose raw error to user
                    yield "ÄÃ£ xáº£y ra lá»—i. Vui lÃ²ng thá»­ láº¡i sau."
                    return

def classify_message(message: str, model: str = None) -> Dict[str, Any]:
    """
    Classify a message and suggest actions.
    """
    prompt = f"""Báº¡n lÃ  chuyÃªn gia an ninh máº¡ng. HÃ£y phÃ¢n loáº¡i tin nháº¯n sau vÃ  Ä‘á» xuáº¥t hÃ nh Ä‘á»™ng xá»­ lÃ½.
Tráº£ vá» Káº¾T QUáº¢ THUáº¦N JSON (khÃ´ng cÃ³ markdown, khÃ´ng cÃ³ vÄƒn báº£n thá»«a).
JSON pháº£i cÃ³ cÃ¡c trÆ°á»ng: classification (safe/suspicious/dangerous), suggested_action (NONE/BLOCK/REPORT), confidence (sá»‘ thá»±c 0-1).
Tin nháº¯n cáº§n phÃ¢n loáº¡i: {message}"""

    response = generate_response(prompt, model, tools=[], format_schema=CLASSIFICATION_SCHEMA)

    if response:
        parsed = _extract_json(response)
        if parsed:
            return parsed

    return {
        "classification": "unknown",
        "suggested_action": "NONE",
        "confidence": 0.5,
    }

# ---------------------------------------------------------------------------
# Web search / fetch helpers
# These call https://ollama.com/api/web_search and /web_fetch directly via
# requests so we control the Authorization header ourselves rather than
# relying on the ollama library's global client (which is initialised at
# import time and does not pick up our OLLAMA_API_KEY reliably).
# ---------------------------------------------------------------------------

_OLLAMA_WEB_API_BASE = "https://ollama.com/api"


def _web_api_headers() -> Dict[str, str]:
    """Build auth headers for the Ollama web API."""
    key = OLLAMA_API_KEY or os.environ.get('OLLAMA_API_KEY', '')
    return {
        'Authorization': f'Bearer {key}',
        'Content-Type': 'application/json',
    }


def web_search_query(query: str, max_results: int = 5) -> Optional[List[Dict]]:
    """
    Perform a web search via Ollama's web search REST API.
    (https://ollama.com/api/web_search)
    Requires OLLAMA_API_KEY in Django settings.

    Returns a list of result dicts with keys: title, url, content.
    """
    import requests as _requests
    try:
        resp = _requests.post(
            f"{_OLLAMA_WEB_API_BASE}/web_search",
            headers=_web_api_headers(),
            json={"query": query, "max_results": max_results},
            timeout=15,
        )
        resp.raise_for_status()
        data = resp.json()
        results = data.get('results', [])
        return results
    except Exception as e:
        logger.error(f"web_search_query error: {e}")
        return None


def web_fetch_url(url: str) -> Optional[Dict]:
    """
    Fetch a web page. Primary: Ollama web_fetch API. Fallback: local requests + BS4.

    Returns a dict with keys: title, content, links.
    """
    import requests as _requests
    from bs4 import BeautifulSoup

    if not url:
        return None

    target_url = url.strip()
    if not re.match(r'^[a-zA-Z][a-zA-Z0-9+\-.]*://', target_url):
        target_url = f"https://{target_url}"

    # Block requests to Ollama local server (prevent SSRF)
    blocked = ('localhost:11434', '127.0.0.1:11434')
    if any(b in target_url for b in blocked):
        logger.warning(f"web_fetch_url blocked internal URL: {target_url}")
        return None
    
    browser_headers = {
        'User-Agent': (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/126.0.0.0 Safari/537.36'
        ),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
    }

    def _decode_response_text(resp):
        text = resp.text or ''
        if any(marker in text for marker in ('Ãƒ', 'Ã‚', 'Ã¡Â»', 'ï¿½')):
            try:
                enc = resp.apparent_encoding or 'utf-8'
                text = resp.content.decode(enc, errors='replace')
            except Exception:
                pass
        return text

    def _extract_payload(resp, requested_url):
        html = _decode_response_text(resp)
        soup = BeautifulSoup(html, 'html.parser')

        for tag in soup(['script', 'style', 'noscript', 'iframe']):
            tag.decompose()

        title_tag = soup.find('title')
        title = title_tag.get_text(' ', strip=True) if title_tag else ''

        content = soup.get_text(separator='\n', strip=True)
        if len(content) > 12000:
            content = content[:12000]

        links = []
        for a in soup.find_all('a', href=True):
            href = (a.get('href') or '').strip()
            if not href:
                continue
            absolute = _requests.compat.urljoin(resp.url or requested_url, href)
            if absolute not in links:
                links.append(absolute)
            if len(links) >= 30:
                break

        return {
            'title': title,
            'content': content,
            'links': links,
        }

    try:
        resp = _requests.get(target_url, headers=browser_headers, timeout=20, verify=True, allow_redirects=True)
        resp.raise_for_status()
        payload = _extract_payload(resp, target_url)
        if payload.get('content'):
            return payload
    except Exception as e:
        logger.warning(f"web_fetch_url local https failed ({target_url}): {e}")

    if target_url.startswith('https://'):
        try:
            http_url = target_url.replace('https://', 'http://', 1)
            resp = _requests.get(http_url, headers=browser_headers, timeout=20, verify=False, allow_redirects=True)
            resp.raise_for_status()
            payload = _extract_payload(resp, http_url)
            if payload.get('content'):
                return payload
        except Exception as e:
            logger.warning(f"web_fetch_url local http failed ({target_url}): {e}")

    return None


# ---------------------------------------------------------------------------
# Dedicated scam database lookup helpers
# ---------------------------------------------------------------------------

# Known credible scam-reporting databases consulted during analysis.
SCAM_DB_URLS = {
    'scamadviser': 'https://www.scamadviser.com/check-website/',
    'scamwave':    'https://scamwave.com/scammers/',
    'trustpilot':  'https://www.trustpilot.com/review/',
    'sitejabber':  'https://www.sitejabber.com/reviews/',
    'tranco':      'https://tranco-list.eu/api/ranks/domain/',
}


def lookup_scamadviser(domain: str) -> Optional[Dict]:
    """
    Fetch the ScamAdviser report for a domain.

    Prioritizes extracting ``ratingScore`` from the SPA bootstrap payload
    embedded in ``<div id="app" data-page="...">``. This is the same source
    used by client-side rendering and is more reliable than placeholder
    progress bars in static HTML. Falls back to textual verdict extraction
    (e.g. ``Very Likely Safe``, ``Caution Recommended``) when needed.

    Args:
        domain: Domain to look up (e.g. 'example.com').  Any leading scheme
                (``http://`` / ``https://``) is stripped automatically.

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    if not clean:
        return None
    url = f"{SCAM_DB_URLS['scamadviser']}{clean}"

    trust_score_from_page = None
    verdict_from_page = None
    page_text = ""

    # 1) Direct HTML fetch to extract data-page JSON + human-readable verdict.
    try:
        import html as _html
        import requests
        from bs4 import BeautifulSoup

        response = requests.get(
            url,
            headers={
                'User-Agent': (
                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:137.0) '
                    'Gecko/20100101 Firefox/137.0'
                )
            },
            timeout=12,
        )
        if response.ok:
            html_text = response.text or ''
            page_text = BeautifulSoup(html_text, 'html.parser').get_text(' ', strip=True)[:6000]

            # Parse <div id="app" data-page="..."> then read ratingScore.
            app_data_match = re.search(
                r'<div\s+id=["\']app["\']\s+data-page=["\']([^"\']+)["\']',
                html_text,
                flags=re.IGNORECASE | re.DOTALL,
            )
            if app_data_match:
                data_page_escaped = app_data_match.group(1)
                data_page_unescaped = _html.unescape(data_page_escaped)
                try:
                    data_page_obj = json.loads(data_page_unescaped)

                    score_candidates = [
                        data_page_obj.get('props', {}).get('score', {}).get('ratingScore'),
                        data_page_obj.get('props', {}).get('ratingScore'),
                        data_page_obj.get('ratingScore'),
                    ]
                    for candidate in score_candidates:
                        if isinstance(candidate, (int, float)):
                            numeric = int(candidate)
                            if 0 <= numeric <= 100:
                                trust_score_from_page = numeric
                                break

                    verdict_candidates = [
                        data_page_obj.get('props', {}).get('score', {}).get('ratingText'),
                        data_page_obj.get('props', {}).get('score', {}).get('ratingLabel'),
                        data_page_obj.get('props', {}).get('score', {}).get('conclusion'),
                        data_page_obj.get('props', {}).get('ratingText'),
                        data_page_obj.get('ratingText'),
                    ]
                    for candidate in verdict_candidates:
                        if isinstance(candidate, str) and candidate.strip():
                            verdict_from_page = candidate.strip()
                            break
                except Exception:
                    # Fallback: extract ratingScore directly from the unescaped data blob.
                    score_match = re.search(r'"ratingScore"\s*:\s*(\d+)', data_page_unescaped)
                    if score_match:
                        trust_score_from_page = int(score_match.group(1))

            # Secondary fallback: score may still appear in the HTML snapshot.
            if trust_score_from_page is None:
                score_match = re.search(r'"ratingScore"\s*:\s*(\d+)', html_text)
                if score_match:
                    trust_score_from_page = int(score_match.group(1))

            # Known ScamAdviser-style conclusions to prioritize.
            if not verdict_from_page:
                verdict_patterns = [
                    r'Very\s+Likely\s+Safe',
                    r'Likely\s+Safe',
                    r'Caution\s+Recommended',
                    r'Suspicious',
                    r'Unsafe',
                    r'Scam',
                ]
                for pattern in verdict_patterns:
                    match = re.search(pattern, page_text, flags=re.IGNORECASE)
                    if match:
                        verdict_from_page = match.group(0)
                        break
    except Exception as exc:
        logger.debug(f"lookup_scamadviser: direct HTML parse failed ({exc})")

    # 2) Keep existing web relay extraction as primary content source.
    result = web_fetch_url(url) or {
        'title': f'ScamAdviser report for {clean}',
        'content': page_text,
        'links': [],
    }

    # 2) If not found from direct page text, attempt extraction from relay content.
    if not verdict_from_page:
        relay_content = result.get('content', '') or ''
        verdict_patterns = [
            r'Very\s+Likely\s+Safe',
            r'Likely\s+Safe',
            r'Caution\s+Recommended',
            r'Suspicious',
            r'Unsafe',
            r'Scam',
        ]
        for pattern in verdict_patterns:
            match = re.search(pattern, relay_content, flags=re.IGNORECASE)
            if match:
                verdict_from_page = match.group(0)
                break

    existing_content = result.get('content', '') or ''
    prefix_lines = []

    if trust_score_from_page is not None:
        prefix_lines.append(f"ScamAdviser Trust Score (ratingScore): {trust_score_from_page}/100")

    if verdict_from_page:
        prefix_lines.append(f"ScamAdviser Verdict: {verdict_from_page}")

    if prefix_lines:
        prefix = '\n'.join(prefix_lines)
        if prefix not in existing_content:
            result['content'] = f"{prefix}\n{existing_content}".strip()

    result['source'] = 'scamadviser'
    result['query_url'] = url
    return result


def lookup_scamwave(query: str) -> Optional[Dict]:
    """
    Search the ScamWave community scam-report database.

    Submits *query* (phone number, bank account, name, or keyword) to
    ScamWave's public search endpoint and returns the page content, which
    lists victim-reported fraud cases and blacklisted identifiers.

    Args:
        query: Search term â€” phone number, bank account, domain, name, or
               any fraud-related keyword.

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    import requests
    from bs4 import BeautifulSoup

    base_url = SCAM_DB_URLS['scamwave']
    clean_query = (query or '').strip()
    if not clean_query:
        return None

    headers = {
        'User-Agent': (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/126.0.0.0 Safari/537.36'
        ),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
    }

    try:
        resp = requests.get(base_url, headers=headers, timeout=20)
        resp.raise_for_status()

        html = resp.text or ''
        soup = BeautifulSoup(html, 'html.parser')
        script_tag = soup.find('script', id='tableData')
        if not script_tag or not script_tag.string:
            return {
                'source': 'scamwave',
                'query_url': base_url,
                'title': f"ScamWave lookup for {clean_query}",
                'content': 'KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u báº£ng ScamWave (script#tableData).',
                'links': [base_url],
            }

        payload = json.loads(script_tag.string.strip())
        columns = payload.get('columns', []) or []
        data_rows = payload.get('data', []) or []

        query_l = clean_query.lower()
        query_compact = re.sub(r'[^a-z0-9]', '', query_l)

        matches = []
        for row in data_rows:
            if not isinstance(row, list):
                continue
            row_text = ' | '.join(str(cell) for cell in row if cell is not None)
            row_l = row_text.lower()
            row_compact = re.sub(r'[^a-z0-9]', '', row_l)
            if query_l in row_l or (query_compact and query_compact in row_compact):
                matches.append(row)

        if matches:
            max_preview = 25
            preview = matches[:max_preview]
            lines = [
                f"ScamWave tÃ¬m tháº¥y {len(matches)} káº¿t quáº£ phÃ¹ há»£p cho '{clean_query}'.",
                f"Columns: {', '.join(str(c) for c in columns)}",
                "",
                "Káº¿t quáº£ ná»•i báº­t:",
            ]
            for row in preview:
                if columns and len(columns) == len(row):
                    cells = [f"{col}={val}" for col, val in zip(columns, row)]
                    lines.append(f"- {'; '.join(cells)}")
                else:
                    lines.append(f"- {' | '.join(str(v) for v in row)}")
            if len(matches) > max_preview:
                lines.append(f"- ... vÃ  {len(matches) - max_preview} káº¿t quáº£ khÃ¡c")
            content = '\n'.join(lines)
        else:
            content = f"KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p cho '{clean_query}' trong dá»¯ liá»‡u ScamWave."

        return {
            'source': 'scamwave',
            'query_url': base_url,
            'title': f"ScamWave lookup for {clean_query}",
            'content': content,
            'links': [base_url],
        }
    except Exception as e:
        logger.error(f"lookup_scamwave error: {e}")
        return None


def lookup_trustpilot(domain: str) -> Optional[Dict]:
    """
    Look up a domain's reviews on Trustpilot.

    Args:
        domain: Domain name (e.g. 'example.com').

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    if not clean:
        return None
    url = f"{SCAM_DB_URLS['trustpilot']}{clean}"
    result = web_fetch_url(url)
    if result:
        result['source'] = 'trustpilot'
        result['query_url'] = url
    return result


def lookup_sitejabber(domain: str) -> Optional[Dict]:
    """
    Look up a domain's reviews on sitejabber.

    Args:
        domain: Domain name (e.g. 'example.com').

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    if not clean:
        return None
    url = f"{SCAM_DB_URLS['sitejabber']}{clean}"
    result = web_fetch_url(url)
    if result:
        result['source'] = 'sitejabber'
        result['query_url'] = url
    return result


def lookup_tranco(domain: str) -> Optional[Dict]:
    """
    Look up a domain's popularity ranking on Tranco list.

    Args:
        domain: Domain name (e.g. 'example.com').

    Returns:
        Dict with keys ``source``, ``query_url``, ``title``, ``content``, and
        ``links``; or ``None`` on failure.
    """
    import requests
    clean = re.sub(r'^https?://', '', domain).split('/')[0].strip()
    if not clean:
        return None
    url = f"{SCAM_DB_URLS['tranco']}{clean}"
    try:
        response = requests.get(
            url,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:135.0) Gecko/20100101 Firefox/137.0"},
            timeout=10
        )
        if response.status_code == 200:
            data = response.json()
            ranks = data.get('ranks', [])
            if ranks:
                latest_rank = ranks[0].get('rank', 'N/A')
                content = f"Popularity Rank: {latest_rank}\nHistory: {json.dumps(ranks[:5])}"
            else:
                content = "No ranking data found (likely low popularity)."
            
            return {
                'source': 'tranco',
                'query_url': url,
                'title': f"Tranco Rank for {clean}",
                'content': content,
                'links': []
            }
        elif response.status_code == 404:
             return {
                'source': 'tranco',
                'query_url': url,
                'title': f"Tranco Rank for {clean}",
                'content': "Domain not found in top 1M list (Low Popularity)",
                'links': []
            }
    except Exception as e:
        logger.error(f"lookup_tranco error: {e}")
    return None



def search_agent(
    query: str,
    model: str = None,
    think=False,
    max_iterations: int = 10,
    max_context_chars: int = 8000,
) -> Optional[Dict]:
    """
    Agentic search loop that gives the model access to web_search and web_fetch
    tools, following the pattern from https://docs.ollama.com/capabilities/web-search.

    The loop continues until the model returns a non-empty final answer (no
    more tool_calls) or ``max_iterations`` is reached.

    Thinking-only turns (empty content, no tool_calls) are retried without
    appending the useless assistant message to history â€” preventing context
    overflow from accumulated thinking text.

    Returns:
        A dict with keys: ``answer`` (str or None) and ``searched_urls`` (list
        of URLs the agent interacted with during tool calls).
    """
    model = model or DEFAULT_MODEL
    messages: List[Dict] = [{'role': 'user', 'content': query}]
    searched_urls: List[str] = []

    agent_tools = {
        '_tool_web_search': _tool_web_search,
        '_tool_web_fetch': _tool_web_fetch,
        '_tool_lookup_scamadviser': _tool_lookup_scamadviser,
        '_tool_lookup_scamwave': _tool_lookup_scamwave,
        '_tool_lookup_trustpilot': _tool_lookup_trustpilot,
        '_tool_lookup_sitejabber': _tool_lookup_sitejabber,
        '_tool_lookup_tranco': _tool_lookup_tranco,
    }

    logger.info(f"[search_agent] START query='{query[:120]}', model={model}, "
                f"think={think}, max_iterations={max_iterations}")

    try:
        for iteration in range(max_iterations):
            logger.info(f"[search_agent iter={iteration}] calling model with "
                        f"{len(messages)} message(s)")

            chat_kwargs: Dict[str, Any] = {
                'model': model,
                'messages': messages,
                'tools': [
                    _tool_lookup_scamadviser,
                    _tool_lookup_scamwave,
                    _tool_lookup_trustpilot,
                    _tool_lookup_sitejabber,
                    _tool_lookup_tranco,
                    _tool_web_search,
                    _tool_web_fetch,
                ],
                'options': {
                    'temperature': LLM_TEMPERATURE,
                    'num_predict': 4000,  # Increased to prevent cutoff
                },
            }
            if think:
                chat_kwargs['think'] = think

            response = _chat_with_retry(**chat_kwargs)
            msg = response.message
            messages.append(message_to_dict(msg))

            content = getattr(msg, 'content', '') or ''
            tool_calls = getattr(msg, 'tool_calls', None)

            if tool_calls:
                # Append assistant message, then execute each tool
                for tool_call in tool_calls:
                    tool_name = tool_call.function.name
                    args = tool_call.function.arguments or {}
                    logger.debug(f"[search_agent iter={iteration}]   â†’ tool '{tool_name}' "
                                f"args={json.dumps(args, ensure_ascii=False)[:200]}")

                    # Track URLs for frontend display
                    if tool_name == '_tool_web_fetch' and args.get('url'):
                        _u = args['url'].strip()
                        if _u and _u not in searched_urls:
                            searched_urls.append(_u)
                    elif tool_name == '_tool_web_search' and args.get('query'):
                        searched_urls.append(f"search:{args['query']}")
                    elif tool_name.startswith('_tool_lookup_') and (args.get('domain') or args.get('query')):
                        _lbl = tool_name.replace('_tool_lookup_', '')
                        _val = args.get('domain') or args.get('query', '')
                        searched_urls.append(f"{_lbl}:{_val}")

                    tool_func = agent_tools.get(tool_name)
                    if tool_func:
                        try:
                            result = tool_func(**args)
                            result_str = str(result)[:max_context_chars]
                        except Exception as exc:
                            result_str = f"Tool execution error: {exc}"
                            logger.error(f"[search_agent iter={iteration}]   âœ— {result_str}")
                    else:
                        result_str = f"Tool '{tool_name}' not found."
                        logger.warning(f"[search_agent iter={iteration}]   âœ— {result_str}")
                    messages.append({
                        'role': 'tool',
                        'content': result_str,
                        'tool_name': tool_name,
                    })
                # Continue loop so the model can read the tool results
                continue

            # No tool calls â€” check whether we actually have a final answer
            if content:
                filtered = filter_tool_call_artifacts(filter_thinking(content))
                # If the model only emitted tool-call artifacts, keep iterating.
                if not filtered:
                    continue
                _log_llm(query, filtered)
                return {'answer': filtered, 'searched_urls': searched_urls}
            else:
                # Thinking-only turn: discard (do NOT append to history) and retry
                continue

        logger.warning(f"[search_agent] Reached max_iterations={max_iterations} "
                       f"without a final answer.")
        return {'answer': None, 'searched_urls': searched_urls}

    except Exception as e:
        logger.error(f"[search_agent] error: {e}", exc_info=True)
        return {'answer': None, 'searched_urls': searched_urls}


def _extract_scam_subject(text: str) -> str:
    """
    Heuristically extract the primary subject (domain, phone, or account number)
    from a text snippet so it can be sent directly to scam databases.

    Returns the best candidate string, or the first 120 characters of *text*
    as a fallback.
    """
    # Domain / URL
    url_match = re.search(
        r'(?:https?://)?(?:www\.)?((?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,})(?:/[^\s]*)?',
        text,
    )
    if url_match:
        return url_match.group(1)

    # Vietnamese / international phone number (digits, optional +/spaces/-)
    phone_match = re.search(r'(?:\+?84|0)[\s.-]?\d[\s.-]?\d{3}[\s.-]?\d{4}', text)
    if phone_match:
        return re.sub(r'[\s.-]', '', phone_match.group(0))

    # Generic phone (7-15 digits, optional leading +)
    generic_phone = re.search(r'\+?\d[\d\s.-]{6,14}\d', text)
    if generic_phone:
        return re.sub(r'[\s.-]', '', generic_phone.group(0))

    return text[:120]


def _clean_db_snippet(db_name: str, content: str) -> str:
    """
    Clean and summarize raw content from scam databases to remove noise
    (UI artifacts, social links, generic footers) and return meaningful signals.
    """
    if not content or len(content) < 50:
        return ""

    lines = [l.strip() for l in content.splitlines() if l.strip()]
    cleaned_lines = []
    
    # Generic noise patterns to skip
    skip_patterns = [
        r'join on', r'write a review', r'latest communities', r'fighting ai scams',
        r'is this your business', r'review guidelines', r'read undefined customer reviews',
        r'smartcustomer', r'incentives or pay to remove', r'be the first to',
        r'facebook', r'reddit', r'twitter', r'instagram'
    ]
    
    for line in lines:
        if any(re.search(p, line, re.IGNORECASE) for p in skip_patterns):
            continue
        cleaned_lines.append(line)

    if not cleaned_lines:
        return ""

    # Re-join and take a meaningful snippet
    cleaned_text = "\n".join(cleaned_lines[:15])
    
    # Heuristic for "empty" database results
    if "0 reviews" in cleaned_text and "0 stars" in cleaned_text:
        return ""
    if len(cleaned_text) < 30:
        return ""
        
    return cleaned_text


def _sanitize_web_context(raw_context: str) -> str:
    """
    Normalize structured OSINT context from the agent:
    - remove malformed headings like "[Nguá»“n: X] cho 'domain'"
    - deduplicate repeated sources
    - keep concise, structured bullets for frontend rendering
    """
    if not raw_context:
        return ""

    text = filter_tool_call_artifacts(raw_context).strip()
    if not text:
        return ""

    if re.fullmatch(r'(?:<\s*tools\s*>\s*<\s*/\s*tools\s*>\s*)+', text, flags=re.IGNORECASE):
        return ""

    section_pattern = re.compile(r'###\s+\[(.*?)\]\s*\n([\s\S]*?)(?=\n###\s+\[|$)', re.IGNORECASE)
    sections = section_pattern.findall(text)
    if not sections:
        return text

    normalized_sections = {}
    section_order = []

    for raw_title, raw_body in sections:
        title = (raw_title or '').strip()
        body = (raw_body or '').strip()
        if not title or not body:
            continue

        title = re.sub(r'^Nguá»“n\s*:\s*', '', title, flags=re.IGNORECASE)
        title = re.sub(r'\]\s*cho\s*[\'\"][^\'\"]+[\'\"]\s*$', '', title, flags=re.IGNORECASE)
        title = re.sub(r'\s+cho\s*[\'\"][^\'\"]+[\'\"]\s*$', '', title, flags=re.IGNORECASE)
        title = re.sub(r'\s+', ' ', title).strip(' []')
        if not title:
            continue

        structured = ('**ÄÃ¡nh giÃ¡' in body or '**ÄÃ¡nh giÃ¡/Äiá»ƒm sá»‘' in body) and ('**Chi tiáº¿t' in body)
        if not structured:
            compact = re.sub(r'\s+', ' ', body).strip()
            lower_compact = compact.lower()
            if 'no ranking data found' in lower_compact or 'domain not found in top 1m' in lower_compact:
                body = (
                    "- **ÄÃ¡nh giÃ¡/Äiá»ƒm sá»‘**: KhÃ´ng cÃ³ dá»¯ liá»‡u\n"
                    "- **Chi tiáº¿t quan trá»ng**: KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u xáº¿p háº¡ng (cÃ³ thá»ƒ Ã­t phá»• biáº¿n)."
                )
            else:
                body = (
                    "- **ÄÃ¡nh giÃ¡/Äiá»ƒm sá»‘**: KhÃ´ng cÃ³ dá»¯ liá»‡u\n"
                    "- **Chi tiáº¿t quan trá»ng**: KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin cá»¥ thá»ƒ."
                )
            structured = True

        if title not in normalized_sections:
            section_order.append(title)
            normalized_sections[title] = {'body': body, 'structured': structured}
        else:
            # Prefer structured/richer section over noisy duplicate
            existing = normalized_sections[title]
            if structured and not existing['structured']:
                normalized_sections[title] = {'body': body, 'structured': structured}

    blocks = []
    for title in section_order:
        body = normalized_sections[title]['body']
        blocks.append(f"### [{title}]\n{body}")

    return "\n\n".join(blocks).strip()


def analyze_text_for_scam(text: str, model: str = None, use_web_search: bool = False) -> Dict[str, Any]:
    """
    Analyze text for scam indicators, optionally enriching with web search.

    When ``use_web_search`` is ``True`` *and* ``OLLAMA_API_KEY`` is configured,
    the function performs two complementary enrichment steps:

    1. **Direct database lookups** â€” queries ScamWave, Trustpilot, Sitejabber,
       and Tranco directly for the primary subject (domain, phone, account)
       extracted from *text*.  These lookups run in parallel with the agent
       call and do not consume extra agent iterations.

    2. **Search-agent enrichment** â€” runs the agentic loop with an extended
       tool set that includes ``_tool_lookup_scamwave`` and other lookup tools
       so the model can proactively consult the same databases for any
       additional entities it discovers.
    """
    web_context = ""
    web_search_used = False
    agent_searched_urls: List[str] = []
    db_context_parts: List[str] = []

    if use_web_search and OLLAMA_API_KEY:
        # ------------------------------------------------------------------
        # Step 1: Direct scam-database lookups (ScamAdviser + ScamWave)
        # ------------------------------------------------------------------
        subject = _extract_scam_subject(text)
        logger.debug(f"analyze_text_for_scam: extracted subject for DB lookup: {subject!r}")

        for db_name, lookup_fn in [
            ('ScamWave',    lookup_scamwave),
            ('Trustpilot',  lookup_trustpilot),
            ('Sitejabber',  lookup_sitejabber),
            ('Tranco',      lookup_tranco),
        ]:
            try:
                db_result = lookup_fn(subject)
                if db_result and db_result.get('content'):
                    raw_content = db_result.get('content', '')
                    snippet = _clean_db_snippet(db_name, raw_content) or raw_content[:700]
                    db_context_parts.append(
                        f"### [Nguá»“n: {db_name}] cho '{subject}'\n{snippet}"
                    )
                    logger.debug(
                        f"analyze_text_for_scam: {db_name} returned "
                        f"{len(db_result['content'])} chars"
                    )
                else:
                    logger.debug(f"analyze_text_for_scam: {db_name} returned no content")
            except Exception as exc:
                logger.warning(f"analyze_text_for_scam: {db_name} lookup failed ({exc})")

        # Fallback if ALL databases are empty
        if not db_context_parts:
            db_context_parts.append(f"KhÃ´ng tÃ¬m tháº¥y bÃ¡o cÃ¡o cá»¥ thá»ƒ vá» '{subject}' trong cÃ¡c cÆ¡ sá»Ÿ dá»¯ liá»‡u lá»«a Ä‘áº£o uy tÃ­n (ScamWave, Trustpilot, Sitejabber).")

        # ------------------------------------------------------------------
        # Step 2: Agentic web-search enrichment
        # ------------------------------------------------------------------
        try:
            search_query = text[:300] + ('...' if len(text) > 300 else '')
            logger.debug(f"analyze_text_for_scam: search_agent query: {search_query!r}")

            agent_answer = search_agent(
                query=(
                    f"Kiá»ƒm tra xem trang/ná»™i dung sau cÃ³ pháº£i lÃ  lá»«a Ä‘áº£o khÃ´ng: {search_query}\n"
                    "Sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ theo thá»© tá»± sau:\n"
                    f"1. _tool_lookup_scamwave    â€” tÃ¬m bÃ¡o cÃ¡o lá»«a Ä‘áº£o vá» sá»‘ Ä‘iá»‡n thoáº¡i/tÃ i khoáº£n/tÃªn trong vÄƒn báº£n\n"
                    f"2. _tool_lookup_trustpilot  â€” kiá»ƒm tra Ä‘iá»ƒm sá»‘ vÃ  Ä‘Ã¡nh giÃ¡ ngÆ°á»i dÃ¹ng trÃªn Trustpilot\n"
                    f"3. _tool_lookup_sitejabber  â€” kiá»ƒm tra Ä‘iá»ƒm sá»‘ vÃ  Ä‘Ã¡nh giÃ¡ ngÆ°á»i dÃ¹ng trÃªn Sitejabber\n"
                    f"4. _tool_lookup_tranco      â€” kiá»ƒm tra thá»© háº¡ng má»©c Ä‘á»™ phá»• biáº¿n (Tranco Rank). Xáº¿p háº¡ng cÃ ng nhá» (vd < 1000) cÃ ng uy tÃ­n.\n"
                    "5. _tool_web_search         â€” tÃ¬m Ä‘Ã¡nh giÃ¡ trÃªn Reddit, v.v. (dÃ¹ng query 'site:reddit.com <domain>', ...)\n"
                    "QUAN TRá»ŒNG: Táº­p trung vÃ o viá»‡c kiá»ƒm tra chÃ­nh trang/ná»™i dung Ä‘Æ°á»£c cung cáº¥p cÃ³ pháº£i lÃ  lá»«a Ä‘áº£o khÃ´ng.\n"
                    "KHÃ”NG Ä‘Æ°á»£c sao chÃ©p nguyÃªn vÄƒn ná»™i dung thÃ´ tá»« tool (menu, nÃºt báº¥m, footer, danh sÃ¡ch link).\n"
                    "KHÃ”NG xuáº¥t block cÃ³ dáº¡ng '### [Nguá»“n: ...] cho ...'. Chá»‰ dÃ¹ng '### [TÃªn Nguá»“n]'.\n"
                    "TrÃ¬nh bÃ y káº¿t quáº£ theo Ä‘á»‹nh dáº¡ng CHÃNH XÃC sau Ä‘á»ƒ há»‡ thá»‘ng cÃ³ thá»ƒ phÃ¢n tÃ¡ch:\n"
                    "### [TÃªn Nguá»“n]\n"
                    "- **ÄÃ¡nh giÃ¡/Äiá»ƒm sá»‘**: ...\n"
                    "- **Chi tiáº¿t quan trá»ng**: TÃ³m táº¯t ngáº¯n gá»n nháº¥t.\n\n"
                    "VÃ­ dá»¥:\n"
                    "### [Trustpilot]\n"
                    "- **ÄÃ¡nh giÃ¡**: An toÃ n\n"
                    "- **Chi tiáº¿t**: ...\n\n"
                    "### [Reddit]\n"
                    "- **ÄÃ¡nh giÃ¡**: Nguy hiá»ƒm\n"
                    "- **Chi tiáº¿t**: ...\n\n"
                    "Náº¿u khÃ´ng cÃ³ thÃ´ng tin tá»« nguá»“n nÃ o, ghi 'KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin cá»¥ thá»ƒ'."
                ),
                model=model,
                think='medium',
                max_iterations=7,
                max_context_chars=8000,
            )

            if agent_answer and isinstance(agent_answer, dict):
                agent_text = agent_answer.get('answer', '') or ''
                agent_searched_urls = agent_answer.get('searched_urls', []) or []
                if agent_text:
                    cleaned_agent_answer = filter_tool_call_artifacts(agent_text).strip()
                    if cleaned_agent_answer:
                        sanitized_context = _sanitize_web_context(cleaned_agent_answer)
                        if sanitized_context:
                            web_context = sanitized_context
                            web_search_used = True
                            logger.debug(
                                f"analyze_text_for_scam: web context ({len(web_context)} chars): "
                                f"{web_context[:150]}..."
                            )
                        else:
                            logger.warning(
                                "analyze_text_for_scam: agent output had only tool artifacts/empty context; ignoring web_context."
                            )
                    else:
                        logger.warning(
                            "analyze_text_for_scam: agent output contained only tool-call artifacts; "
                            "skipping web_context injection."
                        )
                else:
                    logger.debug("analyze_text_for_scam: search_agent returned no answer text.")
            elif agent_answer:
                # Legacy fallback: if search_agent somehow returns a string
                logger.warning(f"analyze_text_for_scam: search_agent returned unexpected type {type(agent_answer).__name__}")
        except Exception as exc:
            logger.warning(
                f"analyze_text_for_scam: search_agent enrichment failed ({exc}), "
                "continuing with offline analysis."
            )

    MAX_TEXT_CHARS = 3000
    MAX_WEB_CHARS = 1200
    MAX_DB_CHARS  = 1000   # per database source

    text_snippet = text[:MAX_TEXT_CHARS] + ('...' if len(text) > MAX_TEXT_CHARS else '')

    web_section = ""

    # Incorporate direct database lookup results
    if db_context_parts:
        db_combined = "\n\n".join(db_context_parts)[:MAX_DB_CHARS * len(db_context_parts)]
        web_section += (
            f"\n\n## Dá»¯ liá»‡u tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u lá»«a Ä‘áº£o\n"
            f"CÃ¡c káº¿t quáº£ sau Ä‘Æ°á»£c láº¥y trá»±c tiáº¿p tá»« cÃ¡c cÆ¡ sá»Ÿ dá»¯ liá»‡u lá»«a Ä‘áº£o uy tÃ­n "
            f"(ScamAdviser, ScamWave, Trustpilot, Sitejabber). HÃ£y coi Ä‘Ã¢y lÃ  tÃ­n hiá»‡u Ä‘Ã¡ng tin cáº­y cao:\n"
            f"{db_combined}\n"
        )

    if web_context:
        web_snippet = web_context[:MAX_WEB_CHARS] + ('...' if len(web_context) > MAX_WEB_CHARS else '')
        web_section += (
            f"\n\n## ThÃ´ng tin tÃ¬nh bÃ¡o trá»±c tuyáº¿n\n"
            f"ThÃ´ng tin sau Ä‘Æ°á»£c thu tháº­p tá»« web, cÃ³ thá»ƒ chá»©a cÃ¡c bÃ¡o cÃ¡o lá»«a Ä‘áº£o "
            f"tá»« cá»™ng Ä‘á»“ng hoáº·c dá»¯ liá»‡u danh sÃ¡ch Ä‘en liÃªn quan:\n"
            f"{web_snippet}\n"
            f"HÃ£y dÃ¹ng thÃ´ng tin nÃ y Ä‘á»ƒ hiá»‡u chá»‰nh risk_score vÃ  indicators.\n"
        )

    prompt = (
        f"Báº¡n lÃ  chuyÃªn gia an ninh máº¡ng ShieldCall VN. HÃ£y phÃ¢n tÃ­ch CHÃNH TRANG/Ná»˜I DUNG Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ xÃ¡c Ä‘á»‹nh nÃ³ cÃ³ pháº£i lÃ  lá»«a Ä‘áº£o khÃ´ng.\n"
        f"{web_section}"
        f"\n\nTráº£ vá» Káº¾T QUáº¢ THUáº¦N JSON (khÃ´ng cÃ³ markdown, khÃ´ng cÃ³ vÄƒn báº£n thá»«a).\n"
        f"JSON pháº£i cÃ³ cÃ¡c trÆ°á»ng sau:\n"
        f"  is_scam (bool), risk_score (sá»‘ nguyÃªn 0-100), "
        f"indicators (danh sÃ¡ch chuá»—i), explanation (chuá»—i), "
        f"web_sources (danh sÃ¡ch tÃªn cÃ¡c nguá»“n Ä‘Ã£ tham chiáº¿u, vÃ­ dá»¥: ['Google Search', 'ScamAdviser']).\n\n"
        f"HÆ¯á»šNG DáºªN PHÃ‚N TÃCH:\n"
        f"1. TRANG/Ná»˜I DUNG CHÃNH THá»¨C/AN TOÃ€N: Náº¿u Ä‘Ã¢y lÃ  trang chÃ­nh thá»©c cá»§a má»™t tá»• chá»©c uy tÃ­n, Ä‘Ã¡nh giÃ¡ lÃ  an toÃ n.\n"
        f"2. TRANG Lá»ªA Äáº¢O: Náº¿u trang nÃ y giáº£ máº¡o, lá»«a Ä‘áº£o, hoáº·c cÃ³ dáº¥u hiá»‡u gian láº­n, Ä‘Ã¡nh giÃ¡ rá»§i ro cao.\n"
        f"3. Sá»¬ Dá»¤NG THÃ”NG TIN TÃŒNH BÃO: Dá»±a vÃ o káº¿t quáº£ tá»« ScamAdviser, ScamWave vÃ  cÃ¡c bÃ¡o cÃ¡o Ä‘á»ƒ Ä‘Æ°a ra káº¿t luáº­n.\n\n"
        f"QUY Táº®C CHO 'explanation':\n"
        f"1. TÃ³m táº¯t ná»™i dung chÃ­nh tá»« cÃ¡c nguá»“n tÃ¬nh bÃ¡o (web_section) má»™t cÃ¡ch ngáº¯n gá»n, sÃºc tÃ­ch.\n"
        f"2. Loáº¡i bá» cÃ¡c thÃ´ng tin rÃ¡c tá»« giao diá»‡n web (náº¿u cÃ³ trong dá»¯ liá»‡u thÃ´) nhÆ° 'Read Reviews', 'Log in', 'Follow on Facebook'.\n"
        f"3. Náº¿u nguá»“n tÃ¬nh bÃ¡o KHÃ”NG cÃ³ thÃ´ng tin liÃªn quan, chá»‰ ghi 'KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u tá»« cÃ¡c nguá»“n uy tÃ­n'.\n"
        f"4. Viáº¿t báº±ng TIáº¾NG VIá»†T, táº­p trung vÃ o káº¿t luáº­n rá»§i ro.\n\n"
        f"Ná»™i dung cáº§n phÃ¢n tÃ­ch:\n{text_snippet}"
    )

    response = generate_response(
        prompt, model, num_ctx=8192, max_tokens=2048, tools=[], format_schema=SCAM_ANALYSIS_SCHEMA
    )

    db_sources_consulted = ['scamadviser', 'scamwave', 'trustpilot', 'sitejabber', 'tranco'] if db_context_parts else []

    # Frontend should display only structured intelligence context,
    # not raw database page dumps (which can be noisy).
    combined_web_context = (web_context or '').strip()
    if not combined_web_context and db_context_parts:
        combined_web_context = (
            "### [Tá»•ng há»£p nguá»“n dá»¯ liá»‡u]\n"
            "- **ÄÃ¡nh giÃ¡**: KhÃ´ng cÃ³ dá»¯ liá»‡u tÃ¬nh bÃ¡o trá»±c tuyáº¿n rÃµ rÃ ng\n"
            "- **Chi tiáº¿t**: KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin cá»¥ thá»ƒ tá»« cÃ¡c nguá»“n Ä‘Ã£ kiá»ƒm tra."
        )

    retry_notice = ''
    retry_count = 0
    parsed_result = _extract_json(response) if response else None

    if response and not parsed_result:
        retry_notice = "â³ AI cáº§n thÃªm chÃºt thá»i gian Ä‘á»ƒ chuáº©n hÃ³a káº¿t quáº£, vui lÃ²ng chá» trong giÃ¢y lÃ¡t."
        logger.warning(
            "analyze_text_for_scam: JSON parse failed on first pass; starting repair retry. Raw response: %s",
            response[:300],
        )
        retry_count += 1
        repair_prompt = (
            "Báº¡n lÃ  bá»™ sá»­a JSON. HÃ£y chuyá»ƒn ná»™i dung sau thÃ nh JSON há»£p lá»‡ theo schema:\n"
            "{is_scam: bool, risk_score: int 0-100, indicators: [string], explanation: string, web_sources: [string]}\n"
            "Chá»‰ tráº£ JSON thuáº§n, khÃ´ng markdown, khÃ´ng giáº£i thÃ­ch.\n\n"
            "Ná»™i dung cáº§n sá»­a:\n"
            f"{response[:2500]}"
        )
        repaired = generate_response(
            repair_prompt,
            model,
            num_ctx=4096,
            max_tokens=1024,
            tools=[],
            format_schema=SCAM_ANALYSIS_SCHEMA,
        )
        parsed_result = _extract_json(repaired) if repaired else None

    if not parsed_result:
        retry_count += 1
        logger.warning("analyze_text_for_scam: Repair retry failed; rerunning primary analysis once.")
        response_retry = generate_response(
            prompt,
            model,
            num_ctx=8192,
            max_tokens=2048,
            tools=[],
            format_schema=SCAM_ANALYSIS_SCHEMA,
        )
        parsed_result = _extract_json(response_retry) if response_retry else None

    if parsed_result:
        result = parsed_result
        result['risk_score'] = int(max(0, min(100, result.get('risk_score', 0) or 0)))
        if not isinstance(result.get('indicators'), list):
            result['indicators'] = []
        if not isinstance(result.get('web_sources'), list):
            result['web_sources'] = []
        result['web_context'] = combined_web_context
        result['web_search_used'] = web_search_used
        result['searched_urls'] = agent_searched_urls
        result['db_sources_consulted'] = db_sources_consulted

        # Ensure web_sources includes db sources consulted
        existing_sources = result.get('web_sources', []) or []
        existing_sources_l = [str(s).lower() for s in existing_sources]
        if db_sources_consulted:
            for src in db_sources_consulted:
                if src not in existing_sources_l:
                    existing_sources.append(src.capitalize())
            result['web_sources'] = existing_sources

        if retry_notice and retry_count > 0:
            result['ai_retry_used'] = True
            result['ai_retry_count'] = retry_count
            result['ai_notice'] = retry_notice
            explanation = str(result.get('explanation', '') or '')
            if retry_notice not in explanation:
                result['explanation'] = f"{retry_notice} {explanation}".strip()
        else:
            result['ai_retry_used'] = False
            result['ai_retry_count'] = 0

        return result

    logger.warning(
        "analyze_text_for_scam: all JSON parsing/retry attempts failed; returning fallback payload."
    )

    return {
        "is_scam": False,
        "risk_score": 0,
        "indicators": [],
        "explanation": "â³ AI cáº§n thÃªm chÃºt thá»i gian. Há»‡ thá»‘ng Ä‘ang táº¡m tráº£ káº¿t quáº£ an toÃ n Ä‘á»ƒ trÃ¡nh treo phiÃªn phÃ¢n tÃ­ch.",
        "web_sources": [s.capitalize() for s in db_sources_consulted] if db_sources_consulted else ["Offline Analysis"],
        "web_context": combined_web_context,
        "web_search_used": web_search_used,
        "searched_urls": agent_searched_urls,
        "db_sources_consulted": db_sources_consulted,
        "ai_retry_used": True,
        "ai_retry_count": retry_count,
        "ai_notice": "â³ AI cáº§n thÃªm chÃºt thá»i gian Ä‘á»ƒ chuáº©n hÃ³a káº¿t quáº£, vui lÃ²ng thá»­ láº¡i sau vÃ i giÃ¢y.",
    }
